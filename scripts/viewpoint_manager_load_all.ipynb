{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "import os\n",
    "import sys   \n",
    "import copy\n",
    "from PIL import Image\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import cv2\n",
    "from rotations import quat_to_rot\n",
    "import time\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def get_rot_vec(R):\n",
    "    x = R[:, 2, 1] - R[:, 1, 2]\n",
    "    y = R[:, 0, 2] - R[:, 2, 0]\n",
    "    z = R[:, 1, 0] - R[:, 0, 1]\n",
    "\n",
    "    r = torch.norm(torch.stack([x, y, z], dim=1))\n",
    "    t = R[:, 0, 0] + R[:, 1, 1] + R[:, 2, 2]\n",
    "    phi = torch.atan2(r, t - 1)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def angle_gen(mat, n_mat):\n",
    "    \"\"\"\n",
    "    mat target dim: 3X3\n",
    "    n_mat dim: Nx3x3\n",
    "\n",
    "    returns distance betweem the rotation matrixes dim: N\n",
    "    \"\"\"\n",
    "    dif = []\n",
    "    for i in range(n_mat.shape[0]):\n",
    "        r, _ = cv2.Rodrigues(mat.dot(n_mat[i, :, :].T))\n",
    "        dif.append(np.linalg.norm(r))\n",
    "\n",
    "    return np.array(dif)\n",
    "\n",
    "\n",
    "def angle_batch_torch_full(mat, n_mat):\n",
    "    \"\"\"\n",
    "    mat target dim: BSx3X3\n",
    "    n_mat dim: BSxNx3x3\n",
    "\n",
    "    return BSXN\n",
    "    \"\"\"\n",
    "    bs = mat.shape[0]\n",
    "    rep = n_mat.shape[1]\n",
    "    mat = mat.unsqueeze(1).repeat((1, rep, 1, 1))\n",
    "    mat = mat.view((-1, 3, 3))\n",
    "    n_mat = n_mat.view((-1, 3, 3))\n",
    "    out = torch.bmm(mat, torch.transpose(n_mat, 1, 2)).view(-1, 3, 3)\n",
    "\n",
    "    vectors = get_rot_vec(out).view(bs, -1, 1)\n",
    "    vectors = torch.abs(vectors)\n",
    "    idx_argmin = torch.argmin(vectors, dim=1)\n",
    "    return idx_argmin\n",
    "\n",
    "\n",
    "class ViewpointManager():\n",
    "\n",
    "    def __init__(self, store, name_to_idx, device='cuda:0',load_images = False):\n",
    "        self.store = store\n",
    "        self.device = device\n",
    "        self.name_to_idx = name_to_idx\n",
    "        self.idx_to_name = {}\n",
    "        self.load_images = load_images\n",
    "\n",
    "        for key, value in self.name_to_idx.items():\n",
    "            self.idx_to_name[value] = key\n",
    "\n",
    "        self._load()\n",
    "        if self.load_images:\n",
    "            self._load_images()\n",
    "            \n",
    "    def _load(self):\n",
    "        self.img_dict = {}\n",
    "        self.pose_dict = {}\n",
    "        self.cam_dict = {}\n",
    "        self.depth_dict = {}\n",
    "        self.sim_dict = {}\n",
    "\n",
    "        for obj in self.name_to_idx.keys():\n",
    "            idx = self.name_to_idx[obj]\n",
    "            self.pose_dict[idx] = torch.tensor(pkl.load(\n",
    "                open(f'{self.store}/{obj}/pose.pkl', \"rb\"))).type(torch.float32).cuda()\n",
    "            self.cam_dict[idx] = torch.tensor(\n",
    "                pkl.load(open(f'{self.store}/{obj}/cam.pkl', \"rb\"))).type(torch.float32).cuda()\n",
    "    \n",
    "    def _load_images(self):\n",
    "        \n",
    "        ls = glob.glob(f'{self.store}/*/*.png')\n",
    "        self.images = {}\n",
    "        for i,f in enumerate(ls):\n",
    "            self.images[f] = Image.open( f ) \n",
    "        print( 'Loaded all Images for ViewpointManger')\n",
    "        \n",
    "    def get_closest_image(self, idx, mat):\n",
    "        \"\"\"\n",
    "        idx: start at 1 and goes to num_obj!\n",
    "        \"\"\"\n",
    "        st = time.time()\n",
    "        dif = angle_gen(mat, self.pose_dict[idx][:, :3, :3].cpu().numpy())\n",
    "        idx_argmin = np.argmin(np.abs(dif))\n",
    "\n",
    "        print('single image idx', idx_argmin, 'value', dif[idx_argmin])\n",
    "        st = time.time()\n",
    "        obj = self.idx_to_name[idx]\n",
    "\n",
    "        st = time.time()\n",
    "        if self.load_images:\n",
    "            img = self.images[f'{self.store}/{obj}/{idx_argmin}-color.png']\n",
    "            depth = self.images[f'{self.store}/{obj}/{idx_argmin}-depth.png']\n",
    "        else:\n",
    "            img = Image.open(f'{self.store}/{obj}/{idx_argmin}-color.png')\n",
    "            depth = Image.open(f'{self.store}/{obj}/{idx_argmin}-depth.png')\n",
    "            \n",
    "        target = self.pose_dict[idx][idx_argmin, :3, :3]\n",
    "        return self.pose_dict[idx][idx_argmin],\\\n",
    "            self.cam_dict[idx][idx_argmin],\\\n",
    "            img,\\\n",
    "            depth, target, idx_argmin\n",
    "\n",
    "    def get_closest_image_single(self, idx, mat):\n",
    "        idx = idx.unsqueeze(0).unsqueeze(0)\n",
    "        mat = mat.unsqueeze(0)\n",
    "        return self.get_closest_image_batch(idx, mat)\n",
    "\n",
    "    def get_closest_image_batch(self, i, rot, conv='wxyz'):\n",
    "        \"\"\"\n",
    "        mat: BSx3x3\n",
    "        idx: BSx1 0-num_obj-1\n",
    "        \"\"\"\n",
    "        # adapt index notation to 1-num_obj\n",
    "        idx = copy.copy(i) + 1\n",
    "\n",
    "        if rot.shape[-1] == 3:\n",
    "          # rotation matrix input\n",
    "            pass\n",
    "        elif rot.shape[-1] == 4:\n",
    "            rot = quat_to_rot(rot=rot, conv=conv, device=self.device)\n",
    "        else:\n",
    "            raise Exception('invalidae shape received for rot', rot.shape)\n",
    "\n",
    "        sr = self.pose_dict[int(idx[0])].shape  # shape reference size sr\n",
    "        bs = idx.shape[0]\n",
    "\n",
    "        # tensor created during runtime to handle flexible batch size\n",
    "        n_mat = torch.empty((idx.shape[0], sr[0], 3, 3), device=self.device)\n",
    "\n",
    "        for i in range(0, idx.shape[0]):\n",
    "            n_mat[i] = self.pose_dict[int(idx[i])][:, :3, :3]\n",
    "\n",
    "        best_match_idx = angle_batch_torch_full(rot, n_mat)\n",
    "\n",
    "        img = []\n",
    "        depth = []\n",
    "        target = []\n",
    "\n",
    "        imgls = torch.empty((idx.shape[0], 480, 640, 3), device=self.device)\n",
    "        depls = torch.empty((idx.shape[0], 480, 640), device=self.device)\n",
    "        tarls = torch.empty((idx.shape[0], 4, 4), device=self.device)\n",
    "\n",
    "        st = time.time()\n",
    "        for j, i in enumerate(idx.tolist()):\n",
    "            best_match = int(best_match_idx[j])\n",
    "            obj = self.idx_to_name[i[0]]\n",
    "\n",
    "            if self.load_images:\n",
    "                imgls[j, :, :, :] = torch.from_numpy(np.array(self.images[ f'{self.store}/{obj}/{best_match}-color.png'] ).astype(np.float32)).to(self.device).unsqueeze(0)\n",
    "                depls[j, :, :] = torch.from_numpy(np.array(self.images[ f'{self.store}/{obj}/{best_match}-depth.png']).astype(np.float32)).to(self.device).unsqueeze(0)\n",
    "\n",
    "            else:\n",
    "                imgls[j, :, :, :] = torch.from_numpy(np.array(Image.open(\n",
    "                    f'{self.store}/{obj}/{best_match}-color.png')).astype(np.float32)).to(self.device).unsqueeze(0)\n",
    "                depls[j, :, :] = torch.from_numpy(np.array(Image.open(\n",
    "                    f'{self.store}/{obj}/{best_match}-depth.png')).astype(np.float32)).to(self.device).unsqueeze(0)\n",
    "\n",
    "            \n",
    "            tarls[j, :, :] = copy.deepcopy(\n",
    "                self.pose_dict[i[0]][best_match, :4, :4])\n",
    "\n",
    "        print(f'loading time is {time.time()-st}')\n",
    "        return imgls, depls, tarls\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test rotation vector\n",
    "    from scipy.stats import special_ortho_group\n",
    "\n",
    "    mat = np.array(special_ortho_group.rvs(dim=3, size=10))\n",
    "    Rin = torch.from_numpy(mat).type(torch.float32).cuda()\n",
    "    q = get_rot_vec(Rin)\n",
    "\n",
    "    # load dataset\n",
    "    # import os\n",
    "    import os\n",
    "    import sys\n",
    "    os.chdir('/home/jonfrey/PLR')\n",
    "    sys.path.append('src')\n",
    "    sys.path.append('src/dense_fusion')\n",
    "\n",
    "    import scipy.io as scio\n",
    "    from loaders_v2 import Backend, ConfigLoader, GenericDataset\n",
    "    import time\n",
    "    exp_cfg = ConfigLoader().from_file(\n",
    "        '/home/jonfrey/PLR2/src/loaders_v2/test/dataset_cfgs.yml')\n",
    "    env_cfg = ConfigLoader().from_file(\n",
    "        '/home/jonfrey/PLR2/src/loaders_v2/test/env_ws.yml')\n",
    "\n",
    "    generic = GenericDataset(\n",
    "        cfg_d=exp_cfg['d_ycb'],\n",
    "        cfg_env=env_cfg)\n",
    "\n",
    "    # load data from dataloader\n",
    "    model = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/models'\n",
    "    base = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/data/0003'\n",
    "    desig = '000550'\n",
    "    store = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/viewpoints_renderings'\n",
    "    img = Image.open('{0}/{1}-color.png'.format(base, desig))\n",
    "    obj = '025_mug'\n",
    "\n",
    "    vm = ViewpointManager(store, generic._backend._name_to_idx)\n",
    "\n",
    "    # apply the same to verify it with an image\n",
    "\n",
    "    obj_idx = generic._backend._name_to_idx[obj]\n",
    "\n",
    "    meta = scio.loadmat('{0}/{1}-meta.mat'.format(base, desig))\n",
    "    obj_tmp = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "    obj_idx_in_list = int(np.argwhere(obj_tmp == obj_idx))\n",
    "    target_r = np.array(meta['poses'][:, :, obj_idx_in_list][:, 0:3])\n",
    "    target_t = np.array(\n",
    "        [meta['poses'][:, :, obj_idx_in_list][:, 3:4].flatten()])[0, :]\n",
    "\n",
    "    start = time.time()\n",
    "    # test numpy implementation\n",
    "    p, c, img, depth, target, idx_argmin = vm.get_closest_image(\n",
    "        idx=obj_idx, mat=target_r)\n",
    "    print(\"Time get single image numpy cv2 backbone: \", time.time() - start)\n",
    "\n",
    "    t_target_r = torch.tensor(target_r, dtype=torch.float32).cuda()\n",
    "    t_obj_idx = torch.tensor(obj_idx, dtype=torch.int64).cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    # test pytorch implementation\n",
    "    img, depth, target = vm.get_closest_image_single(\n",
    "        idx=t_obj_idx, mat=t_target_r)\n",
    "    print(\"Time get single image pytorch: \", time.time() - start)\n",
    "\n",
    "    bs = 10\n",
    "    start = time.time()\n",
    "    img, depth, target = vm.get_closest_image_batch(\n",
    "        idx=t_obj_idx.view((-1, 1)).repeat((bs, 1)), mat=t_target_r.view(-1, 3, 3).repeat((bs, 1, 1)))\n",
    "    print(f'Time get {bs} images pytorch: ', time.time() - start)\n",
    "\n",
    "    # This looks good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "store = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/viewpoints_renderings'\n",
    "import glob\n",
    "from PIL import Image\n",
    "ls = glob.glob(f'{store}/*/*.png')\n",
    "print(ls[:10])\n",
    "images = {}\n",
    "for i,f in enumerate(ls):\n",
    "    images[f] = Image.open( f ) \n",
    "    if i%100==0:\n",
    "        print(f'{i}/{len(ls)}')\n",
    "    #).astype(np.uint8)).to(self.device).unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "d = 1999\n",
    "for i in range(0,d):\n",
    "    torch.from_numpy(np.array(images[ls[i]]).astype(np.float32)).to('cuda:0').unsqueeze(0)\n",
    "print( (time.time()-st) / d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track",
   "language": "python",
   "name": "track"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
