{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonfrey/PLR3/src/loaders_v2/dataset_ycb.py:426: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  init_trans = torch.normal(mean=torch.tensor(gt_trans), std=nt)\n",
      "/home/jonfrey/PLR3/src/helper/bounding_box.py:203: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  masked_idx = (d != 0).nonzero()\n",
      "/home/jonfrey/miniconda3/envs/track_latest/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "os.chdir('/home/jonfrey/PLR3')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd() + '/src'))\n",
    "sys.path.append(os.path.join(os.getcwd() + '/lib'))\n",
    "\n",
    "import loaders_v2\n",
    "from loaders_v2 import GenericDataset\n",
    "from rotations import * \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from visu import plot_pcd, Visualizer\n",
    "import copy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from helper import re_quat\n",
    "from PIL import Image, ImageDraw\n",
    "from deep_im import LossAddS\n",
    "import copy\n",
    "#from deep_im import flow_to_trafo\n",
    "from visu import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import k3d\n",
    "#exp_cfg_path = '/home/jonfrey/PLR3/yaml/exp/exp_ws_deepim_debug_natrix.yml'\n",
    "\n",
    "env_cfg_path = '/home/jonfrey/PLR3/yaml/env/env_natrix_jonas.yml'\n",
    "exp_cfg_path = '/home/jonfrey/PLR3/yaml/exp/exp_evaluate_pose_estimation.yml'\n",
    "h = 480\n",
    "w = 640\n",
    "import k3d\n",
    "\n",
    "def load_from_file(p):\n",
    "    if os.path.isfile(p):\n",
    "        with open(p, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return data\n",
    "\n",
    "exp = load_from_file(exp_cfg_path)\n",
    "env = load_from_file(env_cfg_path)\n",
    "\n",
    "dataset_train = GenericDataset(\n",
    "    cfg_d=exp['d_train'],\n",
    "    cfg_env=env)\n",
    "\n",
    "batch = dataset_train[13450][0] #bann 10450\n",
    "points, choose, img, target, model_points, idx = batch[0:6]\n",
    "depth_img, label_img, img_orig, cam = batch[6:10]\n",
    "gt_rot_wxyz, gt_trans, unique_desig = batch[10:13]\n",
    "\n",
    "real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "pred_rot_wxyz, pred_trans, pred_points, h_render,h_real, render_img_original = batch[18:24]\n",
    "u_map, v_map, flow_mask,  bb = batch[24:]\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Helper Functions (not needed)\n",
    "class Drawer():\n",
    "    def __init__(self):\n",
    "        self.im_in_plot = 0\n",
    "        self.data = []\n",
    "        \n",
    "    def disp_img_1d(self,img,hold=False, save=False, nr=0 , ret=False):\n",
    "        self.data.append(img)\n",
    "        p = '/home/jonfrey/Debug/Midterm2/'\n",
    "        \n",
    "        if not hold:\n",
    "            fig = plt.figure(figsize=(6*2*len(self.data),7))\n",
    "            ax = []\n",
    "            for j,a in enumerate(self.data):\n",
    "                ax.append( fig.add_subplot(1,len(self.data), j+1)  )\n",
    "                \n",
    "                ax[-1].get_xaxis().set_visible(False)\n",
    "                ax[-1].get_yaxis().set_visible(False)\n",
    "                pos = ax[-1].imshow( a, cmap='Reds' )\n",
    "                \n",
    "                fig.colorbar(pos, ax=ax[-1])\n",
    "            plt.show()\n",
    "            if save:\n",
    "                fig.savefig(p+str(nr)+'.png', dpi=300)\n",
    "                \n",
    "            if ret: \n",
    "                if isinstance( self.data[0], torch.Tensor):\n",
    "                    self.data[0] = self.data[0].numpy()\n",
    "                    print('CONV')\n",
    "                    \n",
    "                print(self.data[0].shape)\n",
    "                a = np.max(self.data[0])\n",
    "                b = np.min(self.data[0])\n",
    "                \n",
    "                d = (self.data[0]-float(b))\n",
    "                d = (d / ((float(a)-float(b))) )*255 \n",
    "                d = np.uint8(d)\n",
    "                img = Image.fromarray( d )\n",
    "                return d\n",
    "            self.data = []\n",
    "            self.ax = []\n",
    "            \n",
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "def disp_alignment(depth, label, real):\n",
    "    data = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    t = real\n",
    "    data[:,:,:3] = t.numpy() # red patch in upper left\n",
    "    data_depth[:,:,:3] = t.numpy()\n",
    "    data[:,:,3] = 70\n",
    "    data[:,:,3][label==8] = 255\n",
    "    \n",
    "    min_val = torch.min( depth[depth!=0] )\n",
    "    max_val = torch.max( depth[depth!=0] )\n",
    "    val = torch.clamp( ((depth-min_val) // (max_val-min_val))*255, 0, 255)\n",
    "    \n",
    "    img = Image.fromarray(data, 'RGBA')\n",
    "    display(img)\n",
    "\n",
    "def plot_mask(mask):\n",
    "    min_val = torch.min( mask )\n",
    "    max_val = float( max(1,torch.max( mask )) )\n",
    "    mask = torch.clamp( (mask-min_val) / (max_val-min_val)*255 ,0,255)\n",
    "    \n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth[:,:,3] = 255\n",
    "    for i in range(480):\n",
    "        for j in range(640):\n",
    "            data_depth[i,j,:4] = np.array( cmaplist[ int(mask[i,j])] )*255\n",
    "    data_depth[:,:,3] = 255\n",
    "    data_depth[:,:,3][label==2] = 255\n",
    "    img_depth = Image.fromarray(data_depth, 'RGBA')\n",
    "    display(img_depth)\n",
    "\n",
    "def plot_two_pcd_line(x, y, point_size=0.005, c1='g', c2='r'):\n",
    "    if c1 == 'b':\n",
    "        k = 245\n",
    "    elif c1 == 'g':\n",
    "        k = 25811000\n",
    "    elif c1 == 'r':\n",
    "        k = 11801000\n",
    "    elif c1 == 'black':\n",
    "        k = 2580\n",
    "    else:\n",
    "        k = 2580\n",
    "\n",
    "    if c2 == 'b':\n",
    "        k2 = 245\n",
    "    elif c2 == 'g':\n",
    "        k2 = 25811000\n",
    "    elif c2 == 'r':\n",
    "        k2 = 11801000\n",
    "    elif c2 == 'black':\n",
    "        k2 = 2580\n",
    "    else:\n",
    "        k2 = 2580\n",
    "\n",
    "    col1 = np.ones(x.shape[0]) * k\n",
    "    col2 = np.ones(y.shape[0]) * k2\n",
    "    plot = k3d.plot(name='points')\n",
    "    plt_points = k3d.points(x, col1.astype(np.uint32), point_size=point_size)\n",
    "    plot += plt_points\n",
    "    plt_points = k3d.points(y, col2.astype(np.uint32), point_size=point_size)\n",
    "    plot += plt_points\n",
    "    for i in range(min(100,x.shape[0]) ):\n",
    "        plot += k3d.line([x[i],y[i]],shader='mesh', width=0.0005, color=0xff0000)\n",
    "    \n",
    "    plt_points.shader = '3d'\n",
    "    plot.display()\n",
    "\n",
    "def plot_hist(x,n_bins = 20):\n",
    "    fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "    colors = ['lime']\n",
    "    axs.hist(x, bins=n_bins, color=colors, label=colors)\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "def get_scale_for_erosion(ero_in):\n",
    "    res = torch.sum ( ero_in, dim = (2,3))\n",
    "    res[res < 5000] = 2\n",
    "    res[res < 5000] = 5\n",
    "    res[res < 10000] = 10\n",
    "    res[res < 30000] = 20\n",
    "    res[res < 40000] = 25\n",
    "    res[res < 50000] = 30\n",
    "    res[res >= 50000] = 40\n",
    "    return res\n",
    "\n",
    "def eroision(t,size=3):\n",
    "    \"t: tensor shape BS, C, H,W\"\n",
    "    out_c = t.shape[1]\n",
    "    kernel_tensor = torch.ones( (out_c,1,size,size) )\n",
    "    print(size, kernel_tensor, t.shape)\n",
    "    return torch.nn.functional.conv2d(t, kernel_tensor, padding=(int((size)/2), int((size)/2))) == (size*size)\n",
    "\n",
    "def eroision_batch(t,t_size):\n",
    "    \"t: tensor shape BS, C, H,W\"\n",
    "    \"t_size: tensor shape BS\"\n",
    "    out_c = t.shape[1]\n",
    "    for b in range( t.shape[0] ):\n",
    "        size = int( t_size[b] )\n",
    "        kernel_tensor = torch.ones( (out_c,1,size,size) )\n",
    "        t[b] = (torch.nn.functional.conv2d(t[b][None], kernel_tensor, padding=(int((size)/2), int((size)/2))) == (size*size))[0,:,:t.shape[2], :t.shape[3]]\n",
    "    return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a specific datapoint form the dataset (Not Needed)\n",
    "\n",
    "# [('data/0003/001742',), tensor([8], dtype=torch.int32)]\n",
    "\n",
    "desig = unique_desig[0]\n",
    "desig = 'data/0003/001742'\n",
    "\n",
    "_p_ycb = \"/media/scratch1/jonfrey/datasets/YCB_Video_Dataset\"\n",
    "depth = np.array(Image.open(\n",
    "    '{0}/{1}-depth.png'.format(_p_ycb, desig)))\n",
    "depth.shape\n",
    "\n",
    "label = np.array(Image.open(\n",
    "    '{0}/{1}-label.png'.format(_p_ycb, desig)))\n",
    "img = np.array(Image.open(\n",
    "    '{0}/{1}-color.png'.format(_p_ycb, desig)))\n",
    "batch = dataset_train._backend.getElement( desig, 8)\n",
    "batch = batch #bann 10450   \n",
    "model_points = batch[4]\n",
    "idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "real_img_original = batch[8]\n",
    "cam = batch[9]\n",
    "real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "u_map, v_map, flow_mask, bb = batch[24:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loader to BS 1\n",
    "exp['loader']['batch_size'] = 1\n",
    "exp['loader']['pin_memory'] = False\n",
    "exp['loader']['shuffle'] = False\n",
    "\n",
    "\n",
    "exp['d_train'][\"output_cfg\"]['overfitting_nr_idx'] = -1\n",
    "dataset_train = GenericDataset(\n",
    "    cfg_d=exp['d_train'],\n",
    "    cfg_env=env)\n",
    "\n",
    "\n",
    "# get test and train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                                       **exp['loader'])\n",
    "exp['d_test'][\"output_cfg\"]['overfitting_nr_idx'] = -1\n",
    "exp['d_test'][\"flow_cfg\"]['sub'] = 1\n",
    "exp['d_test'][\"flow_cfg\"]['min_matches'] = 50\n",
    "exp['d_test'][\"flow_cfg\"]['max_matches'] = 3000\n",
    "exp['d_test'][\"flow_cfg\"]['max_iterations'] = 20000\n",
    "exp['d_test'][\"flow_cfg\"]['max_iterations'] = 20000\n",
    "\n",
    "        \n",
    "dataset_test = GenericDataset(\n",
    "    cfg_d=exp['d_test'],\n",
    "    cfg_env=env)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test,\n",
    "                                                       **exp['loader'])\n",
    "\n",
    "# get Loss function\n",
    "criterion_adds = LossAddS(sym_list=exp['d_train']['obj_list_sym'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy ICP implementation from https://github.com/ClayFlannigan/icp/blob/master/icp.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def best_fit_transform(A, B):\n",
    "    '''\n",
    "    Calculates the least-squares best-fit transform that maps corresponding points A to B in m spatial dimensions\n",
    "    Input:\n",
    "      A: Nxm numpy array of corresponding points\n",
    "      B: Nxm numpy array of corresponding points\n",
    "    Returns:\n",
    "      T: (m+1)x(m+1) homogeneous transformation matrix that maps A on to B\n",
    "      R: mxm rotation matrix\n",
    "      t: mx1 translation vector\n",
    "    '''\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    # get number of dimensions\n",
    "    m = A.shape[1]\n",
    "\n",
    "    # translate points to their centroids\n",
    "    centroid_A = np.mean(A, axis=0)\n",
    "    centroid_B = np.mean(B, axis=0)\n",
    "    AA = A - centroid_A\n",
    "    BB = B - centroid_B\n",
    "\n",
    "    # rotation matrix\n",
    "    H = np.dot(AA.T, BB)\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = np.dot(Vt.T, U.T)\n",
    "\n",
    "    # special reflection case\n",
    "    if np.linalg.det(R) < 0:\n",
    "       Vt[m-1,:] *= -1\n",
    "       R = np.dot(Vt.T, U.T)\n",
    "\n",
    "    # translation\n",
    "    t = centroid_B.T - np.dot(R,centroid_A.T)\n",
    "\n",
    "    # homogeneous transformation\n",
    "    T = np.identity(m+1)\n",
    "    T[:m, :m] = R\n",
    "    T[:m, m] = t\n",
    "\n",
    "    return T, R, t\n",
    "\n",
    "\n",
    "def nearest_neighbor(src, dst):\n",
    "    '''\n",
    "    Find the nearest (Euclidean) neighbor in dst for each point in src\n",
    "    Input:\n",
    "        src: Nxm array of points\n",
    "        dst: Nxm array of points\n",
    "    Output:\n",
    "        distances: Euclidean distances of the nearest neighbor\n",
    "        indices: dst indices of the nearest neighbor\n",
    "    '''\n",
    "\n",
    "    assert src.shape == dst.shape\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=1)\n",
    "    neigh.fit(dst)\n",
    "    distances, indices = neigh.kneighbors(src, return_distance=True)\n",
    "    return distances.ravel(), indices.ravel()\n",
    "\n",
    "\n",
    "def icp(A, B, init_pose=None, max_iterations=20, tolerance=0.001):\n",
    "    '''\n",
    "    The Iterative Closest Point method: finds best-fit transform that maps points A on to points B\n",
    "    Input:\n",
    "        A: Nxm numpy array of source mD points\n",
    "        B: Nxm numpy array of destination mD point\n",
    "        init_pose: (m+1)x(m+1) homogeneous transformation\n",
    "        max_iterations: exit algorithm after max_iterations\n",
    "        tolerance: convergence criteria\n",
    "    Output:\n",
    "        T: final homogeneous transformation that maps A on to B\n",
    "        distances: Euclidean distances (errors) of the nearest neighbor\n",
    "        i: number of iterations to converge\n",
    "    '''\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    # get number of dimensions\n",
    "    m = A.shape[1]\n",
    "\n",
    "    # make points homogeneous, copy them to maintain the originals\n",
    "    src = np.ones((m+1,A.shape[0]))\n",
    "    dst = np.ones((m+1,B.shape[0]))\n",
    "    src[:m,:] = np.copy(A.T)\n",
    "    dst[:m,:] = np.copy(B.T)\n",
    "\n",
    "    # apply the initial pose estimation\n",
    "    if init_pose is not None:\n",
    "        src = np.dot(init_pose, src)\n",
    "\n",
    "    prev_error = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # find the nearest neighbors between the current source and destination points\n",
    "        distances, indices = nearest_neighbor(src[:m,:].T, dst[:m,:].T)\n",
    "\n",
    "        # compute the transformation between the current source and nearest destination points\n",
    "        T,_,_ = best_fit_transform(src[:m,:].T, dst[:m,indices].T)\n",
    "\n",
    "        # update the current source\n",
    "        src = np.dot(T, src)\n",
    "\n",
    "        # check error\n",
    "        mean_error = np.mean(distances)\n",
    "        if np.abs(prev_error - mean_error) < tolerance:\n",
    "            break\n",
    "        prev_error = mean_error\n",
    "\n",
    "    # calculate final transformation\n",
    "    T,_,_ = best_fit_transform(A, src[:m,:].T)\n",
    "\n",
    "    return T, distances, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax = -1 \n",
    "def get_H(pcd):\n",
    "    pcd_ret = torch.ones( (pcd.shape[0],pcd.shape[1]+1),device=pcd.device, dtype=pcd.dtype )\n",
    "    pcd_ret[:,:3] = pcd\n",
    "    return pcd_ret\n",
    "\n",
    "def eval_T(P_real_in_center, P_ren_in_center, T_res):\n",
    "        \"\"\"\n",
    "        NR,3\n",
    "        NR,3 \n",
    "        4,4\n",
    "        \"\"\"\n",
    "        P_ren_H = get_H( P_ren_in_center )\n",
    "        P_ren_trafo =  (P_ren_H @ T_res.T)[:,:3]\n",
    "        L2_dis_post = torch.mean( torch.norm( P_real_in_center-P_ren_trafo, dim=1 ) )\n",
    "        L2_dis_pre = torch.mean( torch.norm( P_real_in_center-P_ren_in_center, dim=1 ) )\n",
    "        return L2_dis_post, L2_dis_pre  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from helper import anal_tensor\n",
    "\n",
    "def solve_transform(keypoints, gt_keypoints):\n",
    "    \"\"\"\n",
    "    keypoints: N x K x 3\n",
    "    gt_keypoints: K x 3\n",
    "    return: N x 4 x 4 transformation matrix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keypoints = keypoints.clone()\n",
    "        gt_keypoints = gt_keypoints.clone()\n",
    "        N, K, _ = keypoints.shape\n",
    "        center = keypoints.mean(dim=1)\n",
    "        gt_center = gt_keypoints.mean(dim=0)\n",
    "        keypoints -= center[:, None, :]\n",
    "        gt_keypoints -= gt_center[None]\n",
    "        matrix = keypoints.transpose(2, 1) @ gt_keypoints[None]\n",
    "        U, S, V = torch.svd(matrix)\n",
    "        \n",
    "        Vt = V.transpose(2, 1)\n",
    "        Ut = U.transpose(2, 1)\n",
    "\n",
    "        d = (V @ Ut).det()\n",
    "        I = torch.eye(3, 3, dtype=gt_center.dtype, device= keypoints.device)[None].repeat(N, 1, 1)\n",
    "        I[:, 2, 2] = d.clone()\n",
    "\n",
    "        R = U @ I @ Vt\n",
    "        T = torch.zeros(N, 4, 4, dtype=gt_center.dtype, device= keypoints.device)\n",
    "        T[:, 0:3, 0:3] = R\n",
    "        T[:, 0:3, 3] = center[None] - (R @ gt_center[None :, None])[:, :, 0]\n",
    "        T[:, 3, 3] = 1.0\n",
    "\n",
    "        return T\n",
    "    except RuntimeError as error:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        print(\"Something went wrong\")\n",
    "\n",
    "# costume implementation \n",
    "def solve_transform2(A,B):\n",
    "    if A.shape[0] > B.shape[0]:\n",
    "        x=torch.arange(A.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:B.shape[0]]\n",
    "        A = torch.index_select(A, 0, out)\n",
    "    if A.shape[0] < B.shape[0]:\n",
    "        x=torch.arange(B.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:A.shape[0]]\n",
    "        B = torch.index_select(B, 0, out)\n",
    "\n",
    "    #A = torch.choice(A,B.shape[0])\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    m = A.shape[1]\n",
    "    centroid_A = torch.mean(A, dim=0)\n",
    "    centroid_B = torch.mean(B, dim=0)\n",
    "\n",
    "    AA = (A - centroid_A)\n",
    "    BB = (B - centroid_B)\n",
    "    H = AA.transpose(0,1) @ BB\n",
    "    U, S, Vt = torch.svd(H)\n",
    "    R = Vt @ U.transpose(0,1)\n",
    "    if torch.det(R) < 0:\n",
    "        Vt[m-1,:] *= -1\n",
    "        R = Vt.transpose(0,1) @ U.transpose(0,1)\n",
    "\n",
    "    # translation\n",
    "    t = centroid_B - (R @ centroid_A)\n",
    "    # homogeneous transformation\n",
    "    T = torch.eye(m+1, device= A.device)\n",
    "    T[:m, :m] = R\n",
    "    T[:m, m] = t\n",
    "    return T\n",
    "\n",
    "\n",
    "# NR = 1000\n",
    "# DIM = 3\n",
    "# A = torch.ones( (NR,DIM), dtype= torch.float32)\n",
    "# B = A*1.7223\n",
    "# T = solve_transform2(A,B)\n",
    "# print(T)\n",
    "\n",
    "\n",
    "# A_hom = get_H(A)\n",
    "# A_hom2 = A_hom @ T.T\n",
    "# print(A_hom2[:,:3])\n",
    "\n",
    "def filter_pcd_given_depthmap(pcd, depth, scal= 10000):\n",
    "    \"\"\"\n",
    "    pcd = Nx3 troch.float32\n",
    "    depth = N torch.float32\n",
    "\n",
    "    return N torch.bool\n",
    "    \"\"\"\n",
    "    m1 = (depth/scal) > 0.2\n",
    "    #print( \"Thorwn away values\", (depth/scal) < 0.2 )\n",
    "    return m1\n",
    "\n",
    "    val_d = depth[ m1 ]\n",
    "    mean = torch.mean(val_d)\n",
    "    new_d = depth - mean\n",
    "    tol = 0.5\n",
    "    m2 = torch.abs( new_d/scal ) < tol \n",
    "    return m1 * m2\n",
    "    \n",
    "def filter_pcd( pcd, tol = 0.6):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        pcd : Nx3 torch.float32\n",
    "    returns:\n",
    "        mask : NX3 torch.bool \n",
    "    \"\"\"\n",
    "    m = torch.mean(pcd, dim = 0)\n",
    "    comp = m[None,:].repeat(pcd.shape[0],1) + tol\n",
    "    mean_free = pcd-m[None,:].repeat(comp.shape[0],1)\n",
    "    mask = torch.norm( mean_free,  dim= 1) > tol\n",
    "    #print(f\"filter_pcd PRE: {pcd.shape}, POST: {float(torch.sum(mask[:,None].repeat(1,3) == False ))/3.0}\")\n",
    "    return mask[:,None].repeat(1,3) == False\n",
    "\n",
    "def filter_pcd_cor(pcd1, pcd2, max_mean_deviation=0.2):\n",
    "    \n",
    "    dif = torch.norm( pcd1-pcd2 , dim= 1)\n",
    "    mean = torch.mean(dif, dim = 0)\n",
    "    mean_free = torch.abs(dif-mean)\n",
    "    #print(f\"filter_pcd_cor PRE: {pcd1.shape[0]}, POST: {torch.sum(mean_free < max_mean_deviation)}\")\n",
    "    return mean_free < max_mean_deviation\n",
    "    \n",
    "def flow_to_trafo(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      real_br: torch.tensor torch.Size([2])\n",
    "      real_tl: torch.tensor torch.Size([2])\n",
    "      ren_br: torch.tensor torch.Size([2])\n",
    "      ren_tl: torch.tensor torch.Size([2])\n",
    "      flow_mask: torch.Size([480, 640])\n",
    "      u_map: torch.Size([480, 640])\n",
    "      v_map: torch.Size([480, 640])\n",
    "      K_real: torch.Size([3, 3])\n",
    "      K_ren: torch.Size([3, 3])\n",
    "      real_d: torch.Size([480, 640]) \n",
    "      render_d: torch.Size([480, 640])\n",
    "      h_real: torch.Size([4, 4])\n",
    "      h_render: torch.Size([4, 4])\n",
    "    output:\n",
    "      P_real_in_center: torch.Size([N, 3])\n",
    "      P_ren_in_center: torch.Size([N, 3]) \n",
    "      P_real_trafo: torch.Size([N, 3])\n",
    "      T_res: torch.Size([4, 4])\n",
    "      \n",
    "      The output rotation T_res is defined in the Camera coordinate frame. \n",
    "      Therfore premultiply the T_Res with h_render to get the new h_real_new !!!\n",
    "    \"\"\"\n",
    "    for k in kwargs.keys():\n",
    "        pass\n",
    "        #print(f\"Variable: {k}, Type {type(kwargs[k])}, Dtype{kwargs[k].dtype}, Shape{kwargs[k].shape}\")\n",
    "    real_br = kwargs['real_br']\n",
    "    real_tl = kwargs['real_tl']\n",
    "    ren_br = kwargs['ren_br']\n",
    "    ren_tl = kwargs['ren_tl']\n",
    "    flow_mask = kwargs['flow_mask']\n",
    "    u_map = kwargs['u_map']\n",
    "    v_map = kwargs['v_map']\n",
    "    K_real = kwargs['K_real']\n",
    "    K_ren = kwargs['K_ren']\n",
    "    real_d = kwargs['real_d']\n",
    "    render_d = kwargs['render_d']\n",
    "    h_real = kwargs['h_real']\n",
    "    h_render = kwargs['h_render']\n",
    "    plot_pcd = kwargs.get('plot_pcd',False)\n",
    "  \n",
    "    # Grid for upsampled real\n",
    "    a = float(real_br[0]-real_tl[0])/480*1.0000001\n",
    "    b = float(real_br[1]-real_tl[1])/640*1.0000001\n",
    "    grid_real_h = torch.arange(int(real_tl[0]) ,int(real_br[0]) , a, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_real_w = torch.arange(int(real_tl[1]) ,int(real_br[1]) , b, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "\n",
    "    # Grid for upsampled ren\n",
    "    a = float(ren_br[0]-ren_tl[0])/480*1.0000001\n",
    "    b = float(ren_br[1]-ren_tl[1])/640*1.0000001\n",
    "    c = 0\n",
    "    \n",
    "    grid_ren_h = torch.arange(int(ren_tl[0]) ,int(ren_br[0]) , a, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_ren_w = torch.arange(int(ren_tl[1]) ,int(ren_br[1]) , b, device=u_map.device)[None,:].repeat(480,1)\n",
    "    # Calculate valid depth map for rendered image\n",
    "    render_d_ind_h = torch.arange(0 ,480 , 1, device=u_map.device)[:,None].repeat(1,640)\n",
    "    render_d_ind_w= torch.arange(0 ,640 , 1, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "    render_d_ind_h = torch.clamp(torch.round((render_d_ind_h - u_map).type(torch.float32)) ,0,479).type( torch.long )[flow_mask]\n",
    "    render_d_ind_w = torch.clamp(torch.round((render_d_ind_w - v_map).type(torch.float32)),0,639).type( torch.long )[flow_mask] \n",
    "    index = render_d_ind_h*640 + render_d_ind_w # hacky indexing along two dimensions\n",
    "    ren_d_masked  = render_d.flatten()[index]\n",
    "    \n",
    "    # Project depth map to the pointcloud real\n",
    "    cam_scale = 10000\n",
    "\n",
    "    real_pixels = torch.stack( [grid_real_w[flow_mask], grid_real_h[flow_mask], torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)[flow_mask]], dim=1 ).type(u_map.dtype)\n",
    "    K_inv = torch.inverse(K_real.type(torch.float32)).type(u_map.dtype)\n",
    "    P_real = K_inv @ real_pixels.T\n",
    "    P_real = P_real * real_d[flow_mask] / cam_scale\n",
    "    P_real = P_real.T\n",
    "    \n",
    "    # Project depth map to the pointcloud render\n",
    "    K_ren_inv = torch.inverse(K_ren.type(torch.float32)).type(u_map.dtype)\n",
    "    ren_pixels = torch.stack( [grid_ren_w[flow_mask] - v_map[flow_mask], \n",
    "                            grid_ren_h[flow_mask] - u_map[flow_mask],\n",
    "                            torch.ones(grid_ren_h.shape, device = u_map.device,  dtype= u_map.dtype )[flow_mask]], \n",
    "                            dim=1 ).type(u_map.dtype)\n",
    "    P_ren = K_ren_inv @ ren_pixels.T\n",
    "    P_ren = P_ren * ren_d_masked / cam_scale\n",
    "    P_ren = P_ren.T\n",
    "\n",
    "    # Filter the pointclouds given the depthmap\n",
    "    m_ren_depth = filter_pcd_given_depthmap(P_ren, render_d[flow_mask])\n",
    "    m_real_depth = filter_pcd_given_depthmap(P_real, real_d[flow_mask])\n",
    "    m_total =  m_ren_depth * m_real_depth\n",
    "    \n",
    "    min_points = 20\n",
    "    if torch.sum(m_total) < min_points:\n",
    "        print(f'Violation filter pcd_given_depthmap: P_in: {P_ren.shape[0]} P_out: {torch.sum(m_total)}')\n",
    "        return P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_ren = P_ren[m_total] \n",
    "    P_real = P_real[m_total]\n",
    "    # anal_tensor(  P_ren, 'P_ren m_total masked')\n",
    "\n",
    "    # Do not transfrom to center coordinate system\n",
    "    P_real_in_center = P_real                      \n",
    "    P_ren_in_center = P_ren \n",
    "    \n",
    "    m_real = filter_pcd( P_real_in_center )\n",
    "    m_ren = filter_pcd( P_ren_in_center )\n",
    "    m_tot = m_real * m_ren\n",
    "    if torch.sum(m_tot) < min_points:\n",
    "        print(f'Violation filter_pcd: P_in: { P_ren_in_center.shape[0]} P_out: {torch.sum(m_tot)}')\n",
    "        return P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[m_tot[:,0]]\n",
    "    P_ren_in_center = P_ren_in_center[m_tot[:,0]]\n",
    "  \n",
    "    # Max mean deviation\n",
    "    m_new = filter_pcd_cor(P_real_in_center, P_ren_in_center)\n",
    "    \n",
    "    if torch.sum(m_new) < min_points:\n",
    "        print(f'Violation filter_pcd_cor: P_in: { P_ren_in_center.shape[0]} P_out: {torch.sum(m_mew)}')\n",
    "        return P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[m_new]\n",
    "    P_ren_in_center = P_ren_in_center[m_new]\n",
    "\n",
    "    # random shuffel\n",
    "    pts_trafo = min( P_real_in_center.shape[0], 1000 )\n",
    "    idx = torch.randperm( P_real_in_center.shape[0] )[0:pts_trafo]\n",
    "    P_real_in_center = P_real_in_center[idx]\n",
    "    P_ren_in_center = P_ren_in_center[idx]\n",
    "\n",
    "    T_res = solve_transform( P_real_in_center[None].type(torch.float64 ) , P_ren_in_center.type(torch.float64 ) ).type(u_map.dtype )\n",
    "    \n",
    "    # Transform the real points according to calculated transformation\n",
    "    P_hr = torch.ones( (P_real_in_center.shape[0],4 ) , device=u_map.device, dtype= u_map.dtype)\n",
    "    P_hr[:,:3] = P_real_in_center\n",
    "    P_real_trafo = (torch.inverse( T_res[0].type(torch.float32) ).type(u_map.dtype ) @ copy.deepcopy(P_hr).T).T [:,:3]\n",
    "\n",
    "    return P_real_in_center, P_ren_in_center, P_real_trafo, T_res[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Processed 0/10\n",
      "Suc Iteration 0 < 0.02 ADD-S, GT 0.0075017730705440044 ERRODED 0.007872780784964561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonfrey/miniconda3/envs/track_latest/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Iteration 1 > 0.02 ADD-S, GT 0.025795092806220055 ERRODED 0.02517268806695938\n",
      "    shape of use points:  torch.Size([1000, 3])\n",
      "    number of point with a distance greater 0.2 0 \n",
      "    number of point with a distance greater 0.02 9 \n",
      "    number of point with a distance smaller 0.02 991 \n",
      "Failed Iteration 2 > 0.02 ADD-S, GT 0.05843918398022652 ERRODED 0.06180953234434128\n",
      "    shape of use points:  torch.Size([1000, 3])\n",
      "    number of point with a distance greater 0.2 0 \n",
      "    number of point with a distance greater 0.02 69 \n",
      "    number of point with a distance smaller 0.02 931 \n",
      "Suc Iteration 3 < 0.02 ADD-S, GT 0.0021543321199715137 ERRODED 0.001782842562533915\n",
      "Suc Iteration 4 < 0.02 ADD-S, GT 0.00803694874048233 ERRODED 0.008439507335424423\n",
      "Suc Iteration 5 < 0.02 ADD-S, GT 0.005636031273752451 ERRODED 0.005265112966299057\n",
      "Suc Iteration 6 < 0.02 ADD-S, GT 0.014894338324666023 ERRODED 0.014557445421814919\n",
      "Suc Iteration 7 < 0.02 ADD-S, GT 0.015868935734033585 ERRODED 0.017895806580781937\n",
      "Suc Iteration 8 < 0.02 ADD-S, GT 0.001922949100844562 ERRODED 0.002855265513062477\n",
      "Suc Iteration 9 < 0.02 ADD-S, GT 0.013330873101949692 ERRODED 0.012738050892949104\n"
     ]
    }
   ],
   "source": [
    "from visu import plot_two_pcd\n",
    "import itertools\n",
    "# idxmax can be set to != -1 to load specific sample\n",
    "\n",
    "idxmax = -1 \n",
    "device = 'cpu'\n",
    "desig_ls= []\n",
    "adds_gt_ls = []\n",
    "adds_init_ls = []\n",
    "adds_erode_ls = []\n",
    "adds_noise1_ls = []\n",
    "adds_noise2_ls = []\n",
    "visualizer = Visualizer('/home/jonfrey/Debug', None)\n",
    "print('START')\n",
    "\n",
    "K_ren = torch.tensor( dataset_train._backend.get_camera('data_syn/0019', K=True), device=device ) \n",
    "max_iter = 10\n",
    "\n",
    "if idxmax != -1:\n",
    "    print(idxmax, \"is not equal to -1 load batch\")\n",
    "    batch = next(itertools.islice(dataloader_train, idxmax, None))[0]\n",
    "\n",
    "for j,ba in enumerate(dataloader_test):\n",
    "    if j > max_iter-1:\n",
    "        break\n",
    "    if j % 10 == 0:\n",
    "        print(f\"Processed {j}/{max_iter}\")\n",
    "    if idxmax == -1:  \n",
    "        batch = ba[0]\n",
    "    # use first sample in batch \n",
    "    b = 0\n",
    "    \n",
    "    model_points = batch[4]\n",
    "    idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "    label = batch[7]\n",
    "    real_img_original = batch[8]\n",
    "    cam = batch[9]\n",
    "    gt_rot_wxyz, gt_trans, unique_desig = batch[10:13] # unique_desig[1] contains the idx starting at 1 for the first object \n",
    "    bs = model_points.shape[0]\n",
    "    if batch[13] is False:\n",
    "        print('Continue')\n",
    "        continue\n",
    "    real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "    pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "    u_map, v_map, flow_mask, bb = batch[24:]\n",
    "    real_tl, real_br, ren_tl, ren_br = bb \n",
    "    \n",
    "    data = torch.cat([real_img, render_img], dim=1)\n",
    "    uv_gt = torch.stack( [u_map, v_map], dim=3 ).permute(0,3,1,2)\n",
    "    \n",
    "    \n",
    "    ero_in = (gt_label_cropped ==  unique_desig[1])[:,None,:,:].type(torch.float32) # BS,C,H,W\n",
    "    t_size = get_scale_for_erosion(ero_in)\n",
    "    ero_out = eroision_batch(ero_in,t_size)\n",
    "    \n",
    "    # get camera\n",
    "    K_real = torch.tensor( [[cam[b,2],0,cam[b,0]],[b,cam[b,3],cam[b,1]],[0,0,1]], device=device )\n",
    "    \n",
    "    #get inital estimate of the poistion given by the dataloader\n",
    "    h_real_est = torch.eye(4,device=device)\n",
    "    h_real_est[:3,:3] = quat_to_rot(pred_rot_wxyz[b][None,:], conv='wxyz', device=device)\n",
    "    h_real_est[:3,3] = torch.tensor( pred_trans[b].clone().detach() ,device=device )\n",
    "\n",
    "    \n",
    "    ### GT Estimate ###\n",
    "    typ = u_map.dtype\n",
    "    fmt = flow_mask.dtype\n",
    "    \n",
    "     ### GT Label ###\n",
    "    P_real_in_center, P_ren_in_center, P_real_trafo, gt_T_res = flow_to_trafo(\n",
    "        real_br = real_br[b].clone(),\n",
    "        real_tl = (real_tl[b]).clone(), \n",
    "        ren_br = (ren_br[b]).clone(), \n",
    "        ren_tl = (ren_tl[b]).clone(),\n",
    "        flow_mask = (flow_mask[b]).clone(), \n",
    "        u_map = (u_map[b].type( typ )).clone(),\n",
    "        v_map = (v_map[b].type( typ )).clone(), \n",
    "        K_real = (K_real.type( typ )).clone(),\n",
    "        K_ren = (K_ren.type( typ )).clone(),\n",
    "        real_d = (real_d[b].type( typ )).clone(),\n",
    "        render_d = (render_d[b].type( typ )).clone(),\n",
    "        h_real = (h_real_est.type( typ )).clone(), \n",
    "        h_render = (h_render[b].type( typ )).clone())\n",
    "    gt_h_est =  gt_T_res @ h_render[0].type(typ)\n",
    "\n",
    "    ### Erode Label ###\n",
    "    flow_mask_eroded  = (flow_mask * ero_out.type(torch.float32)).type(fmt)[:,0]\n",
    "    _,_,_, ero_T_res = flow_to_trafo(\n",
    "        real_br = (real_br[b]).clone(),\n",
    "        real_tl = (real_tl[b]).clone(), \n",
    "        ren_br = (ren_br[b]).clone(), \n",
    "        ren_tl = (ren_tl[b]).clone(),\n",
    "        flow_mask = (flow_mask_eroded[b]).clone(), \n",
    "        u_map = ((u_map[b]).type( typ )).clone(), \n",
    "        v_map = ((v_map[b]).type( typ )).clone(), \n",
    "        K_real =(K_real.type( typ )).clone(),\n",
    "        K_ren = (K_ren.type( typ )).clone(),\n",
    "        real_d = (real_d[b].type( typ )).clone(),\n",
    "        render_d = (render_d[b].type( typ )).clone(),\n",
    "        h_real = (h_real_est.type( typ )).clone(), \n",
    "        h_render = (h_render[b].type( typ )).clone() )\n",
    "    ero_h_est =  ero_T_res @ h_render[0].type(typ) # set rotation\n",
    "\n",
    "    mask = (flow_mask == True)\n",
    "\n",
    "    # Target Model-points Transformed\n",
    "    p = model_points.shape[1]\n",
    "    target = torch.bmm( model_points, torch.transpose(h_real[:,:3,:3], 1,2 ) ) + h_real[:,:3,3][:,None,:].repeat(1,p,1)\n",
    "\n",
    "    # Compute ADD-S\n",
    "    adds_res_gt_flow = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = gt_h_est[None].type( target.dtype) )\n",
    "    adds_res_gt_flow_eroded = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = ero_h_est[None].type( target.dtype) )\n",
    "    adds_init = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = h_real_est[None].type( target.dtype))\n",
    "    \n",
    "    # Append results to list\n",
    "    desig_ls.append(unique_desig)\n",
    "    adds_init_ls.append(float(adds_init.detach()))\n",
    "    adds_gt_ls.append(float(adds_res_gt_flow.detach()))\n",
    "    adds_erode_ls.append(float(adds_res_gt_flow_eroded.detach()))\n",
    "\n",
    "    \n",
    "    if adds_gt_ls[-1] > 0.02 :\n",
    "        print(f'Failed Iteration {j} > 0.02 ADD-S, GT', adds_gt_ls[-1] , 'ERRODED', adds_erode_ls[-1]  )\n",
    "        \n",
    "        no = torch.norm(P_real_trafo- P_ren_in_center, dim=1)\n",
    "        print( f'    shape of use points: ',P_real_trafo.shape )\n",
    "        print( f'    number of point with a distance greater 0.2 {torch.sum( no>0.2 )} ' )\n",
    "        print( f'    number of point with a distance greater 0.02 {torch.sum( no>0.02 )} ' )\n",
    "        print( f'    number of point with a distance smaller 0.02 {torch.sum( no<0.02 )} ' )\n",
    "        \n",
    "        \n",
    "        plot1 = True\n",
    "    else:\n",
    "        print(f'Suc Iteration {j} < 0.02 ADD-S, GT', adds_gt_ls[-1] , 'ERRODED', adds_erode_ls[-1]  )\n",
    "        plot1 = False\n",
    "    if idxmax != -1:\n",
    "        plot1 = True\n",
    "\n",
    "    if plot1:\n",
    "        \n",
    "                                             \n",
    "        pass\n",
    "#         print(\"Real depth map cropped\")\n",
    "#         real_depth_img = Drawer().disp_img_1d(real_d[b].numpy(),ret=True)\n",
    "#         real_depth_img = np.repeat( real_depth_img[:,:,None],3, axis=2)\n",
    "#         print(\"Render depth map cropped\")\n",
    "#         render_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "#         render_depth_img = np.repeat( render_depth_img[:,:,None],3, axis=2)\n",
    "#         Drawer().disp_img_1d(flow_mask[0],ret=True)\n",
    "\n",
    "#         sub = max(1,int( P_real_in_center.shape[0]/50 ) )\n",
    "#         plot_two_pcd_line(P_real_in_center[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "#         plot_two_pcd_line(P_real_trafo[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "\n",
    "#         print(f\"Real Image, Estimated Points given GT Flow {P_real_in_center.shape}\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_img_original[b].cpu().numpy(),\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "#         print(\"Real Image Cropped, Estimated Points given GT Flow\")\n",
    "#         visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_depth_img,\n",
    "#                         tl = real_tl[0],\n",
    "#                         br = real_br[0],\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "\n",
    "#         fil = label == unique_desig[1]    \n",
    "#         real_img_original[b][ fil[0][:,:,None].repeat(1,1,3) ] = 255\n",
    "#         print(\"Real Image, Estimated Points given GT Flow with label is white\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_img_original[b].cpu().numpy(),\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "\n",
    "#         print(\"Render Image, Rendered Points not transformed reprojected\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                             epoch = 1,\n",
    "#                             img= render_img_original[b].cpu().numpy(),\n",
    "#                             points = copy.deepcopy(P_ren_in_center.cpu().numpy()),\n",
    "#                             store = False,\n",
    "#                             jupyter=True,\n",
    "#                             K = K_ren.cpu().numpy(),\n",
    "#                             H = np.eye(4),\n",
    "#                             method='def')\n",
    "#         print(\"Render Image Cropped, Rendered Points not transformed reprojected\")\n",
    "#         visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "#                             epoch = 1,\n",
    "#                             img= render_depth_img,\n",
    "#                             points = copy.deepcopy(P_ren_in_center.cpu().numpy()),\n",
    "#                             tl = ren_tl[0],\n",
    "#                             br = ren_br[0],\n",
    "#                             store = False,\n",
    "#                             jupyter=True,\n",
    "#                             K = K_ren.cpu().numpy(),\n",
    "#                             H = np.eye(4),\n",
    "#                             method='def')\n",
    "#         print(\"Corrospondence\")\n",
    "#         visualizer.plot_corrospondence(tag=f'_',\n",
    "#                                            epoch=0,\n",
    "#                                             u_map=u_map[0], \n",
    "#                                             v_map=v_map[0], \n",
    "#                                             flow_mask=flow_mask[0], \n",
    "#                                             real_img=real_img[0], \n",
    "#                                             render_img=render_img[0],\n",
    "#                                             store=False,\n",
    "#                                             jupyter=True,\n",
    "#                                             coloful=True,\n",
    "#                                             method='def',\n",
    "#                                             res_h=1,\n",
    "#                                             res_w=1)\n",
    "\n",
    "        \n",
    "# print(f'Result, GT Avg', sum(adds_gt_ls)/len(adds_gt_ls), 'Errode Avg', sum(adds_erode_ls)/len(adds_erode_ls) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADD-S GT</th>\n",
       "      <th>ADD-S INITAL</th>\n",
       "      <th>ADD-S Eroded Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007502</td>\n",
       "      <td>0.054498</td>\n",
       "      <td>0.007873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025795</td>\n",
       "      <td>0.025013</td>\n",
       "      <td>0.025173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.058439</td>\n",
       "      <td>0.053979</td>\n",
       "      <td>0.061810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.009931</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.024726</td>\n",
       "      <td>0.008440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005636</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.005265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.035180</td>\n",
       "      <td>0.014557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.015869</td>\n",
       "      <td>0.044131</td>\n",
       "      <td>0.017896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.015694</td>\n",
       "      <td>0.002855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.013331</td>\n",
       "      <td>0.052049</td>\n",
       "      <td>0.012738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ADD-S GT  ADD-S INITAL  ADD-S Eroded Mask\n",
       "0  0.007502      0.054498           0.007873\n",
       "1  0.025795      0.025013           0.025173\n",
       "2  0.058439      0.053979           0.061810\n",
       "3  0.002154      0.009931           0.001783\n",
       "4  0.008037      0.024726           0.008440\n",
       "5  0.005636      0.020880           0.005265\n",
       "6  0.014894      0.035180           0.014557\n",
       "7  0.015869      0.044131           0.017896\n",
       "8  0.001923      0.015694           0.002855\n",
       "9  0.013331      0.052049           0.012738"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "data = np.array( [adds_gt_ls,adds_init_ls,adds_erode_ls] ).T\n",
    "df = pandas.DataFrame( data, columns = ['ADD-S GT','ADD-S INITAL','ADD-S Eroded Mask'] )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN:\n",
      " ADD-S GT             0.015358\n",
      "ADD-S INITAL         0.033608\n",
      "ADD-S Eroded Mask    0.015839\n",
      "dtype: float64\n",
      "\n",
      " \n",
      "STD:\n",
      " ADD-S GT             0.016780\n",
      "ADD-S INITAL         0.016670\n",
      "ADD-S Eroded Mask    0.017676\n",
      "dtype: float64\n",
      "\n",
      " \n",
      "MAX:\n",
      " ADD-S GT             0.058439\n",
      "ADD-S INITAL         0.054498\n",
      "ADD-S Eroded Mask    0.061810\n",
      "dtype: float64\n",
      "\n",
      " \n",
      "IDXMAX:\n",
      " ADD-S GT             2\n",
      "ADD-S INITAL         0\n",
      "ADD-S Eroded Mask    2\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "Worst Sample Index is 2\n"
     ]
    }
   ],
   "source": [
    "print(\"MEAN:\\n\", df.mean() )\n",
    "print(\"\\n \\nSTD:\\n\", df.std() )\n",
    "print(\"\\n \\nMAX:\\n\", df.max())\n",
    "print(\"\\n \\nIDXMAX:\\n\", df.idxmax())\n",
    "\n",
    "idxmax = df.idxmax()[0]\n",
    "print(f'\\n \\nWorst Sample Index is {idxmax}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = adds_erode_ls\n",
    "n_bins = 100\n",
    "failures_filter = 1\n",
    "fig, axs = plt.subplots(3, 1, sharey=True, tight_layout=True)\n",
    "axs[0].hist(adds_init_ls, bins=n_bins)\n",
    "\n",
    "adds_gt_f = [a for a in adds_gt_ls if a < failures_filter]\n",
    "adds_erode_f = [a for a in adds_erode_ls if a < failures_filter]\n",
    "\n",
    "axs[1].hist(adds_gt_f, bins=n_bins)\n",
    "axs[2].hist(adds_erode_f, bins=n_bins)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = adds_erode_ls\n",
    "n_bins = 100\n",
    "failures_filter = 0.1\n",
    "fig, axs = plt.subplots(3, 1, sharey=True, tight_layout=True)\n",
    "axs[0].hist(adds_init_ls, bins=n_bins)\n",
    "\n",
    "adds_gt_f = [a for a in adds_gt_ls if a < failures_filter]\n",
    "adds_erode_f = [a for a in adds_erode_ls if a < failures_filter]\n",
    "\n",
    "axs[1].hist(adds_gt_f, bins=n_bins)\n",
    "axs[2].hist(adds_erode_f, bins=n_bins)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_latest",
   "language": "python",
   "name": "track_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
