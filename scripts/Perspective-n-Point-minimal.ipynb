{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonfrey/PLR3/src/loaders_v2/dataset_ycb.py:431: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  init_trans = torch.normal(mean=torch.tensor(gt_trans), std=nt)\n",
      "/home/jonfrey/PLR3/src/helper/bounding_box.py:203: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  masked_idx = (d != 0).nonzero()\n",
      "/home/jonfrey/miniconda3/envs/track_latest/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "os.chdir('/home/jonfrey/PLR3')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd() + '/src'))\n",
    "sys.path.append(os.path.join(os.getcwd() + '/lib'))\n",
    "\n",
    "import loaders_v2\n",
    "from loaders_v2 import GenericDataset\n",
    "from rotations import * \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from visu import plot_pcd, Visualizer\n",
    "import copy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from helper import re_quat\n",
    "from PIL import Image, ImageDraw\n",
    "from deep_im import LossAddS\n",
    "import copy\n",
    "#from deep_im import flow_to_trafo\n",
    "from visu import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import k3d\n",
    "#exp_cfg_path = '/home/jonfrey/PLR3/yaml/exp/exp_ws_deepim_debug_natrix.yml'\n",
    "\n",
    "env_cfg_path = '/home/jonfrey/PLR3/yaml/env/env_natrix_jonas.yml'\n",
    "exp_cfg_path = '/home/jonfrey/PLR3/yaml/exp/exp_evaluate_pose_estimation.yml'\n",
    "h = 480\n",
    "w = 640\n",
    "import k3d\n",
    "\n",
    "def load_from_file(p):\n",
    "    if os.path.isfile(p):\n",
    "        with open(p, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return data\n",
    "\n",
    "exp = load_from_file(exp_cfg_path)\n",
    "env = load_from_file(env_cfg_path)\n",
    "\n",
    "dataset_train = GenericDataset(\n",
    "    cfg_d=exp['d_train'],\n",
    "    cfg_env=env)\n",
    "\n",
    "batch = dataset_train[13450][0] #bann 10450\n",
    "points, choose, img, target, model_points, idx = batch[0:6]\n",
    "depth_img, label_img, img_orig, cam = batch[6:10]\n",
    "gt_rot_wxyz, gt_trans, unique_desig = batch[10:13]\n",
    "\n",
    "real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "pred_rot_wxyz, pred_trans, pred_points, h_render,h_real, render_img_original = batch[18:24]\n",
    "u_map, v_map, flow_mask,  bb , depth_render_original= batch[24:]\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ray Tracering \n",
    "# DepthMap -> Reproject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Helper Functions (not needed)\n",
    "class Drawer():\n",
    "    def __init__(self):\n",
    "        self.im_in_plot = 0\n",
    "        self.data = []\n",
    "        \n",
    "    def disp_img_1d(self,img,hold=False, save=False, nr=0 , ret=False):\n",
    "        self.data.append(img)\n",
    "        p = '/home/jonfrey/Debug/Midterm2/'\n",
    "        \n",
    "        if not hold:\n",
    "            fig = plt.figure(figsize=(6*2*len(self.data),7))\n",
    "            ax = []\n",
    "            for j,a in enumerate(self.data):\n",
    "                ax.append( fig.add_subplot(1,len(self.data), j+1)  )\n",
    "                \n",
    "                ax[-1].get_xaxis().set_visible(False)\n",
    "                ax[-1].get_yaxis().set_visible(False)\n",
    "                pos = ax[-1].imshow( a, cmap='Reds' )\n",
    "                try:\n",
    "                    if a.shape[2] != 3:\n",
    "                        fig.colorbar(pos, ax=ax[-1])\n",
    "                except:\n",
    "                    pass\n",
    "            plt.show()\n",
    "            if save:\n",
    "                fig.savefig(p+str(nr)+'.png', dpi=300)\n",
    "                \n",
    "            if ret: \n",
    "                if isinstance( self.data[0], torch.Tensor):\n",
    "                    self.data[0] = self.data[0].numpy()\n",
    "                    print('CONV')\n",
    "                    \n",
    "                print(self.data[0].shape)\n",
    "                a = np.max(self.data[0])\n",
    "                b = np.min(self.data[0])\n",
    "                \n",
    "                d = (self.data[0]-float(b))\n",
    "                d = (d / ((float(a)-float(b))) )*255 \n",
    "                d = np.uint8(d)\n",
    "                img = Image.fromarray( d )\n",
    "                return d\n",
    "            self.data = []\n",
    "            self.ax = []\n",
    "            \n",
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "def disp_alignment(depth, label, real):\n",
    "    data = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    t = real\n",
    "    data[:,:,:3] = t.numpy() # red patch in upper left\n",
    "    data_depth[:,:,:3] = t.numpy()\n",
    "    data[:,:,3] = 70\n",
    "    data[:,:,3][label==8] = 255\n",
    "    \n",
    "    min_val = torch.min( depth[depth!=0] )\n",
    "    max_val = torch.max( depth[depth!=0] )\n",
    "    val = torch.clamp( ((depth-min_val) // (max_val-min_val))*255, 0, 255)\n",
    "    \n",
    "    img = Image.fromarray(data, 'RGBA')\n",
    "    display(img)\n",
    "\n",
    "def plot_mask(mask):\n",
    "    min_val = torch.min( mask )\n",
    "    max_val = float( max(1,torch.max( mask )) )\n",
    "    mask = torch.clamp( (mask-min_val) / (max_val-min_val)*255 ,0,255)\n",
    "    \n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth[:,:,3] = 255\n",
    "    for i in range(480):\n",
    "        for j in range(640):\n",
    "            data_depth[i,j,:4] = np.array( cmaplist[ int(mask[i,j])] )*255\n",
    "    data_depth[:,:,3] = 255\n",
    "    data_depth[:,:,3][label==2] = 255\n",
    "    img_depth = Image.fromarray(data_depth, 'RGBA')\n",
    "    display(img_depth)\n",
    "\n",
    "def plot_two_pcd_line(x, y, point_size=0.005, c1='g', c2='r'):\n",
    "    if c1 == 'b':\n",
    "        k = 245\n",
    "    elif c1 == 'g':\n",
    "        k = 25811000\n",
    "    elif c1 == 'r':\n",
    "        k = 11801000\n",
    "    elif c1 == 'black':\n",
    "        k = 2580\n",
    "    else:\n",
    "        k = 2580\n",
    "\n",
    "    if c2 == 'b':\n",
    "        k2 = 245\n",
    "    elif c2 == 'g':\n",
    "        k2 = 25811000\n",
    "    elif c2 == 'r':\n",
    "        k2 = 11801000\n",
    "    elif c2 == 'black':\n",
    "        k2 = 2580\n",
    "    else:\n",
    "        k2 = 2580\n",
    "\n",
    "    col1 = np.ones(x.shape[0]) * k\n",
    "    col2 = np.ones(y.shape[0]) * k2\n",
    "    plot = k3d.plot(name='points')\n",
    "    plt_points = k3d.points(x, col1.astype(np.uint32), point_size=point_size)\n",
    "    plot += plt_points\n",
    "    plt_points = k3d.points(y, col2.astype(np.uint32), point_size=point_size)\n",
    "    plot += plt_points\n",
    "    for i in range(min(100,x.shape[0]) ):\n",
    "        plot += k3d.line([x[i],y[i]],shader='mesh', width=0.0005, color=0xff0000)\n",
    "    \n",
    "    plt_points.shader = '3d'\n",
    "    plot.display()\n",
    "\n",
    "def plot_hist(x,n_bins = 20):\n",
    "    fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "    colors = ['lime']\n",
    "    axs.hist(x, bins=n_bins, color=colors, label=colors)\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "def get_scale_for_erosion(ero_in):\n",
    "    res = torch.sum ( ero_in, dim = (2,3))\n",
    "    res[res < 5000] = 5\n",
    "    res[res < 10000] = 10\n",
    "    res[res < 30000] = 20\n",
    "    res[res < 40000] = 25\n",
    "    res[res < 50000] = 30\n",
    "    res[res >= 50000] = 40\n",
    "    return res\n",
    "\n",
    "def eroision(t,size=3):\n",
    "    \"t: tensor shape BS, C, H,W\"\n",
    "    out_c = t.shape[1]\n",
    "    kernel_tensor = torch.ones( (out_c,1,size,size) )\n",
    "    print(size, kernel_tensor, t.shape)\n",
    "    return torch.nn.functional.conv2d(t, kernel_tensor, padding=(int((size)/2), int((size)/2))) == (size*size)\n",
    "\n",
    "def eroision_batch(t,t_size):\n",
    "    \"t: tensor shape BS, C, H,W\"\n",
    "    \"t_size: tensor shape BS\"\n",
    "    out_c = t.shape[1]\n",
    "    for b in range( t.shape[0] ):\n",
    "        size = int( t_size[b] )\n",
    "        kernel_tensor = torch.ones( (out_c,1,size,size) )\n",
    "        t[b] = (torch.nn.functional.conv2d(t[b][None], kernel_tensor, padding=(int((size)/2), int((size)/2))) == (size*size))[0,:,:t.shape[2], :t.shape[3]]\n",
    "    return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a specific datapoint form the dataset (Not Needed)\n",
    "\n",
    "# [('data/0003/001742',), tensor([8], dtype=torch.int32)]\n",
    "\n",
    "desig = unique_desig[0]\n",
    "desig = 'data/0003/001742'\n",
    "\n",
    "_p_ycb = \"/media/scratch1/jonfrey/datasets/YCB_Video_Dataset\"\n",
    "depth = np.array(Image.open(\n",
    "    '{0}/{1}-depth.png'.format(_p_ycb, desig)))\n",
    "depth.shape\n",
    "\n",
    "label = np.array(Image.open(\n",
    "    '{0}/{1}-label.png'.format(_p_ycb, desig)))\n",
    "img = np.array(Image.open(\n",
    "    '{0}/{1}-color.png'.format(_p_ycb, desig)))\n",
    "batch = dataset_train._backend.getElement( desig, 8)\n",
    "batch = batch #bann 10450   \n",
    "model_points = batch[4]\n",
    "idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "real_img_original = batch[8]\n",
    "cam = batch[9]\n",
    "real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "u_map, v_map, flow_mask, bb, render_orig = batch[24:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loader to BS 1\n",
    "exp['loader']['batch_size'] = 1\n",
    "exp['loader']['pin_memory'] = False\n",
    "exp['loader']['shuffle'] = True\n",
    "exp['loader']['num_workers'] = 1\n",
    "\n",
    "exp['d_train'][\"output_cfg\"]['overfitting_nr_idx'] = -1\n",
    "exp['d_train'][\"flow_cfg\"]['sub'] = 1\n",
    "exp['d_train'][\"flow_cfg\"]['min_matches'] = 50\n",
    "exp['d_train'][\"flow_cfg\"]['max_matches'] = 5000\n",
    "exp['d_train'][\"flow_cfg\"]['max_iterations'] = 20000\n",
    "exp['d_train'][\"flow_cfg\"]['dil_kernel_size'] = 3\n",
    "\n",
    "dataset_train = GenericDataset(\n",
    "    cfg_d=exp['d_train'],\n",
    "    cfg_env=env)\n",
    "\n",
    "\n",
    "# get test and train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                                       **exp['loader'])\n",
    "exp['d_test'][\"output_cfg\"]['overfitting_nr_idx'] = -1\n",
    "exp['d_test'][\"output_cfg\"]['noise_translation'] = 0.025\n",
    "exp['d_test'][\"output_cfg\"]['noise_rotation'] = 25\n",
    "\n",
    "exp['d_test'][\"flow_cfg\"]['sub'] = 1\n",
    "exp['d_test'][\"flow_cfg\"]['min_matches'] = 300\n",
    "exp['d_test'][\"flow_cfg\"]['max_matches'] = 5000\n",
    "exp['d_test'][\"flow_cfg\"]['max_iterations'] = 20000\n",
    "exp['d_test'][\"flow_cfg\"]['dil_kernel_size'] = 1\n",
    "dataset_test = GenericDataset(\n",
    "    cfg_d=exp['d_test'],\n",
    "    cfg_env=env)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test,\n",
    "                                                       **exp['loader'])\n",
    "\n",
    "# get Loss function\n",
    "criterion_adds = LossAddS(sym_list=exp['d_train']['obj_list_sym'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax = -1 \n",
    "def get_H(pcd):\n",
    "    pcd_ret = torch.ones( (pcd.shape[0],pcd.shape[1]+1),device=pcd.device, dtype=pcd.dtype )\n",
    "    pcd_ret[:,:3] = pcd\n",
    "    return pcd_ret\n",
    "\n",
    "def eval_T(P_real_in_center, P_ren_in_center, T_res):\n",
    "        \"\"\"\n",
    "        NR,3\n",
    "        NR,3 \n",
    "        4,4\n",
    "        \"\"\"\n",
    "        P_ren_H = get_H( P_ren_in_center )\n",
    "        P_ren_trafo =  (P_ren_H @ T_res.T)[:,:3]\n",
    "        L2_dis_post = torch.mean( torch.norm( P_real_in_center-P_ren_trafo, dim=1 ) )\n",
    "        L2_dis_pre = torch.mean( torch.norm( P_real_in_center-P_ren_in_center, dim=1 ) )\n",
    "        return L2_dis_post, L2_dis_pre  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from helper import anal_tensor\n",
    "\n",
    "def solve_transform(keypoints, gt_keypoints):\n",
    "    \"\"\"\n",
    "    keypoints: N x K x 3\n",
    "    gt_keypoints: K x 3\n",
    "    return: N x 4 x 4 transformation matrix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keypoints = keypoints.clone()\n",
    "        gt_keypoints = gt_keypoints.clone()\n",
    "        N, K, _ = keypoints.shape\n",
    "        center = keypoints.mean(dim=1)\n",
    "        gt_center = gt_keypoints.mean(dim=0)\n",
    "        keypoints -= center[:, None, :]\n",
    "        gt_keypoints -= gt_center[None]\n",
    "        matrix = keypoints.transpose(2, 1) @ gt_keypoints[None]\n",
    "        U, S, V = torch.svd(matrix)\n",
    "        \n",
    "        Vt = V.transpose(2, 1)\n",
    "        Ut = U.transpose(2, 1)\n",
    "\n",
    "        d = (V @ Ut).det()\n",
    "        I = torch.eye(3, 3, dtype=gt_center.dtype, device= keypoints.device)[None].repeat(N, 1, 1)\n",
    "        I[:, 2, 2] = d.clone()\n",
    "\n",
    "        R = U @ I @ Vt\n",
    "        T = torch.zeros(N, 4, 4, dtype=gt_center.dtype, device= keypoints.device)\n",
    "        T[:, 0:3, 0:3] = R\n",
    "        T[:, 0:3, 3] = center[None] - (R @ gt_center[None :, None])[:, :, 0]\n",
    "        T[:, 3, 3] = 1.0\n",
    "\n",
    "        return T\n",
    "    except RuntimeError as error:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        print(\"Something went wrong\")\n",
    "\n",
    "# costume implementation \n",
    "def solve_transform2(A,B):\n",
    "    if A.shape[0] > B.shape[0]:\n",
    "        x=torch.arange(A.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:B.shape[0]]\n",
    "        A = torch.index_select(A, 0, out)\n",
    "    if A.shape[0] < B.shape[0]:\n",
    "        x=torch.arange(B.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:A.shape[0]]\n",
    "        B = torch.index_select(B, 0, out)\n",
    "\n",
    "    #A = torch.choice(A,B.shape[0])\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    m = A.shape[1]\n",
    "    centroid_A = torch.mean(A, dim=0)\n",
    "    centroid_B = torch.mean(B, dim=0)\n",
    "\n",
    "    AA = (A - centroid_A)\n",
    "    BB = (B - centroid_B)\n",
    "    H = AA.transpose(0,1) @ BB\n",
    "    U, S, Vt = torch.svd(H)\n",
    "    R = Vt @ U.transpose(0,1)\n",
    "    if torch.det(R) < 0:\n",
    "        Vt[m-1,:] *= -1\n",
    "        R = Vt.transpose(0,1) @ U.transpose(0,1)\n",
    "\n",
    "    # translation\n",
    "    t = centroid_B - (R @ centroid_A)\n",
    "    # homogeneous transformation\n",
    "    T = torch.eye(m+1, device= A.device)\n",
    "    T[:m, :m] = R\n",
    "    T[:m, m] = t\n",
    "    return T\n",
    "\n",
    "\n",
    "# NR = 1000\n",
    "# DIM = 3\n",
    "# A = torch.ones( (NR,DIM), dtype= torch.float32)\n",
    "# B = A*1.7223\n",
    "# T = solve_transform2(A,B)\n",
    "# print(T)\n",
    "\n",
    "\n",
    "# A_hom = get_H(A)\n",
    "# A_hom2 = A_hom @ T.T\n",
    "# print(A_hom2[:,:3])\n",
    "\n",
    "def filter_pcd_given_depthmap(pcd, depth, scal= 10000):\n",
    "    \"\"\"\n",
    "    pcd = Nx3 troch.float32\n",
    "    depth = N torch.float32\n",
    "\n",
    "    return N torch.bool\n",
    "    \"\"\"\n",
    "    m1 = (depth/scal) > 0.01\n",
    "    #print( \"Thorwn away values\", (depth/scal) < 0.2 )\n",
    "    return m1\n",
    "\n",
    "    val_d = depth[ m1 ]\n",
    "    mean = torch.mean(val_d)\n",
    "    new_d = depth - mean\n",
    "    tol = 0.5\n",
    "    m2 = torch.abs( new_d/scal ) < tol \n",
    "    return m1 * m2\n",
    "    \n",
    "def filter_pcd( pcd, tol = 0.6):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        pcd : Nx3 torch.float32\n",
    "    returns:\n",
    "        mask : NX3 torch.bool \n",
    "    \"\"\"\n",
    "    return pcd[:,2] > 0.05\n",
    "    \n",
    "    m = torch.mean(pcd, dim = 0)\n",
    "    comp = m[None,:].repeat(pcd.shape[0],1) + tol\n",
    "    mean_free = pcd-m[None,:].repeat(comp.shape[0],1)\n",
    "    mask = torch.norm( mean_free,  dim= 1) > tol\n",
    "    #print(f\"filter_pcd PRE: {pcd.shape}, POST: {float(torch.sum(mask[:,None].repeat(1,3) == False ))/3.0}\")\n",
    "    return mask == False\n",
    "\n",
    "def filter_pcd_cor(pcd1, pcd2, max_mean_deviation=0.2):\n",
    "    \n",
    "    dif = torch.norm( pcd1-pcd2 , dim= 1)\n",
    "    mean = torch.mean(dif, dim = 0)\n",
    "    mean_free = torch.abs(dif-mean)\n",
    "    #print(f\"filter_pcd_cor PRE: {pcd1.shape[0]}, POST: {torch.sum(mean_free < max_mean_deviation)}\")\n",
    "    return mean_free < max_mean_deviation\n",
    "    \n",
    "def flow_to_trafo(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      real_br: torch.tensor torch.Size([2])\n",
    "      real_tl: torch.tensor torch.Size([2])\n",
    "      ren_br: torch.tensor torch.Size([2])\n",
    "      ren_tl: torch.tensor torch.Size([2])\n",
    "      flow_mask: torch.Size([480, 640])\n",
    "      u_map: torch.Size([480, 640])\n",
    "      v_map: torch.Size([480, 640])\n",
    "      K_real: torch.Size([3, 3])\n",
    "      K_ren: torch.Size([3, 3])\n",
    "      real_d: torch.Size([480, 640]) \n",
    "      render_d: torch.Size([480, 640])\n",
    "      h_real: torch.Size([4, 4])\n",
    "      h_render: torch.Size([4, 4])\n",
    "    output:\n",
    "      P_real_in_center: torch.Size([N, 3])\n",
    "      P_ren_in_center: torch.Size([N, 3]) \n",
    "      P_real_trafo: torch.Size([N, 3])\n",
    "      T_res: torch.Size([4, 4])\n",
    "      \n",
    "      The output rotation T_res is defined in the Camera coordinate frame. \n",
    "      Therfore premultiply the T_Res with h_render to get the new h_real_new !!!\n",
    "    \"\"\"\n",
    "    for k in kwargs.keys():\n",
    "        pass\n",
    "        #print(f\"Variable: {k}, Type {type(kwargs[k])}, Dtype{kwargs[k].dtype}, Shape{kwargs[k].shape}\")\n",
    "    real_br = kwargs['real_br']\n",
    "    real_tl = kwargs['real_tl']\n",
    "    ren_br = kwargs['ren_br']\n",
    "    ren_tl = kwargs['ren_tl']\n",
    "    flow_mask = kwargs['flow_mask']\n",
    "    u_map = kwargs['u_map']\n",
    "    v_map = kwargs['v_map']\n",
    "    K_real = kwargs['K_real']\n",
    "    K_ren = kwargs['K_ren']\n",
    "    real_d = kwargs['real_d']\n",
    "    render_d = kwargs['render_d']\n",
    "    h_real = kwargs['h_real']\n",
    "    h_render = kwargs['h_render']\n",
    "    plot_pcd = kwargs.get('plot_pcd',False)\n",
    "  \n",
    "    # Grid for upsampled real\n",
    "    grid_real_h = torch.linspace(int(real_tl[0]) ,int(real_br[0]) , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_real_w = torch.linspace(int(real_tl[1]) ,int(real_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "\n",
    "    # Grid for upsampled ren\n",
    "    c = 0\n",
    "    \n",
    "    grid_ren_h = torch.linspace(int(ren_tl[0]) ,int(ren_br[0]) , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_ren_w = torch.linspace(int(ren_tl[1]) ,int(ren_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    # Calculate valid depth map for rendered image\n",
    "    render_d_ind_h = torch.linspace(0 ,479 , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    render_d_ind_w= torch.linspace(0 ,639 , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "    render_d_ind_h = torch.clamp(torch.round((render_d_ind_h - u_map).type(torch.float32)) ,0,479).type( torch.long )[flow_mask]\n",
    "    render_d_ind_w = torch.clamp(torch.round((render_d_ind_w - v_map).type(torch.float32)),0,639).type( torch.long )[flow_mask] \n",
    "    index = render_d_ind_h*640 + render_d_ind_w # hacky indexing along two dimensions\n",
    "    ren_d_masked  = render_d.flatten()[index]\n",
    "    \n",
    "    # Project depth map to the pointcloud real\n",
    "    cam_scale = 10000\n",
    "\n",
    "    real_pixels = torch.stack( [grid_real_w[flow_mask], grid_real_h[flow_mask], torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)[flow_mask]], dim=1 ).type(u_map.dtype)\n",
    "    K_inv = torch.inverse(K_real.type(torch.float32)).type(u_map.dtype)\n",
    "    P_real = K_inv @ real_pixels.T\n",
    "    P_real = P_real * real_d[flow_mask] / cam_scale\n",
    "    P_real = P_real.T\n",
    "    \n",
    "    # Project depth map to the pointcloud render\n",
    "    K_ren_inv = torch.inverse(K_ren.type(torch.float32)).type(u_map.dtype)\n",
    "    ren_pixels = torch.stack( [grid_ren_w[flow_mask] - v_map[flow_mask], \n",
    "                            grid_ren_h[flow_mask] - u_map[flow_mask],\n",
    "                            torch.ones(grid_ren_h.shape, device = u_map.device,  dtype= u_map.dtype )[flow_mask]], \n",
    "                            dim=1 ).type(u_map.dtype)\n",
    "    P_ren = K_ren_inv @ ren_pixels.T\n",
    "    P_ren = P_ren * ren_d_masked / cam_scale\n",
    "    P_ren = P_ren.T\n",
    "\n",
    "    # Filter the pointclouds given the depthmap\n",
    "#     m_ren_depth = filter_pcd_given_depthmap(P_ren, ren_d_masked)\n",
    "#     m_real_depth = filter_pcd_given_depthmap(P_real, real_d[flow_mask])\n",
    "#     m_total =  m_ren_depth * m_real_depth\n",
    "    \n",
    "    min_points = 20\n",
    "#     if torch.sum(m_total) < min_points:\n",
    "#         print(f'Violation filter pcd_given_depthmap: P_in: {P_ren.shape[0]} P_out: {torch.sum(m_total)}')\n",
    "#         return False, P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "#     P_ren = P_ren[m_total] \n",
    "#     P_real = P_real[m_total]\n",
    "    # anal_tensor(  P_ren, 'P_ren m_total masked')\n",
    "\n",
    "    # Do not transfrom to center coordinate system\n",
    "    P_real_in_center = P_real                      \n",
    "    P_ren_in_center = P_ren \n",
    "    \n",
    "    m_real = filter_pcd( P_real_in_center )\n",
    "    m_ren = filter_pcd( P_ren_in_center )\n",
    "    m_tot = m_real * m_ren\n",
    "    if torch.sum(m_tot) < min_points:\n",
    "        print(f'Violation filter_pcd: P_in: { P_ren_in_center.shape[0]} P_out: {torch.sum(m_tot)}')\n",
    "        return False, P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[m_tot]\n",
    "    P_ren_in_center = P_ren_in_center[m_tot]\n",
    "  \n",
    "    # Max mean deviation\n",
    "    m_new = filter_pcd_cor(P_real_in_center, P_ren_in_center)\n",
    "    \n",
    "    if torch.sum(m_new) < min_points:\n",
    "        print(f'Violation filter_pcd_cor: P_in: { P_ren_in_center.shape[0]} P_out: {torch.sum(m_new)}')\n",
    "        return False, P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[m_new]\n",
    "    P_ren_in_center = P_ren_in_center[m_new]\n",
    "\n",
    "    # random shuffel\n",
    "    pts_trafo = min( P_real_in_center.shape[0], 1000 )\n",
    "    idx = torch.randperm( P_real_in_center.shape[0] )[0:pts_trafo]\n",
    "    P_real_in_center = P_real_in_center[idx]\n",
    "    P_ren_in_center = P_ren_in_center[idx]\n",
    "\n",
    "    T_res = solve_transform( P_real_in_center[None].type(torch.float64 ) , P_ren_in_center.type(torch.float64 ) ).type(u_map.dtype )\n",
    "    \n",
    "    # Transform the real points according to calculated transformation\n",
    "    P_hr = torch.ones( (P_real_in_center.shape[0],4 ) , device=u_map.device, dtype= u_map.dtype)\n",
    "    P_hr[:,:3] = P_real_in_center\n",
    "    P_real_trafo = (torch.inverse( T_res[0].type(torch.float32) ).type(u_map.dtype ) @ copy.deepcopy(P_hr).T).T [:,:3]\n",
    "\n",
    "    return True, P_real_in_center, P_ren_in_center, P_real_trafo, T_res[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "cv2.__version__\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import k3d \n",
    "def rvec_tvec_to_H(r_vec,t_vec):\n",
    "    # get homogenous output transformation\n",
    "    rot = R.from_rotvec(r_vec)\n",
    "    h = np.eye(4)\n",
    "    h[:3,:3] = rot.as_matrix()\n",
    "    h[:3,3] = t_vec.T\n",
    "#     print(h)\n",
    "    return h\n",
    "class SingleObjectADDLoss:\n",
    "    def asymmetric(self, gt_T, T_hat, model_points):\n",
    "        R_hat = T_hat[:3, :3]\n",
    "        t_hat = T_hat[:3, 3, None]\n",
    "        model_points = model_points[:, :, None]\n",
    "        predicted = ((R_hat @ model_points) + t_hat)[:, :, 0]\n",
    "        R_gt = gt_T[:3, :3]\n",
    "        t_gt = gt_T[:3, 3, None]\n",
    "        ground_truth = ((R_gt @ model_points) + t_gt)[:, :, 0]\n",
    "        return (ground_truth - predicted).norm(dim=1).mean()\n",
    "\n",
    "    def symmetric(self, gt_T, T_hat, model_points):\n",
    "        ones = torch.ones(\n",
    "            model_points.shape[0], 1, dtype=model_points.dtype).to(gt_T.device)\n",
    "        points = torch.cat([model_points, ones], dim=1)[:, :, None]\n",
    "        ground_truth = (gt_T @ points)[:, :3, 0]\n",
    "        predicted = (T_hat @ points)[:, :3, 0]\n",
    "        dima = (ground_truth[None] - predicted[:, None]).norm(dim=2)\n",
    "        min_values, _ = dima.min(dim=1)\n",
    "        return min_values.mean(dim=0)\n",
    "    \n",
    "    def pcd(self, ground_truth, predicted):\n",
    "        dima = (ground_truth[None] - predicted[:, None]).norm(dim=2)\n",
    "        min_values, _ = dima.min(dim=1)\n",
    "        return min_values.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def flow_to_trafo_PnP(*args, **kwargs):\n",
    "    real_br = kwargs['real_br']\n",
    "    real_tl = kwargs['real_tl']\n",
    "    ren_br = kwargs['ren_br']\n",
    "    ren_tl = kwargs['ren_tl']\n",
    "    flow_mask = kwargs['flow_mask']\n",
    "    u_map = kwargs['u_map']\n",
    "    v_map = kwargs['v_map']\n",
    "    K_real = kwargs['K_real']\n",
    "    K_ren = kwargs['K_ren']\n",
    "    real_d = kwargs['real_d']\n",
    "    render_d = kwargs['render_d']\n",
    "    h_real = kwargs['h_real']\n",
    "    h_render = kwargs['h_render']\n",
    "    h_real_est = kwargs['h_real_est']\n",
    "    mp = kwargs['mp']\n",
    "    img = kwargs['img']\n",
    "    mode = kwargs['refine']\n",
    "    \n",
    "    typ = u_map.dtype\n",
    "\n",
    "    # Grid for upsampled real\n",
    "    grid_real_h = torch.linspace(int(real_tl[0]) ,int(real_br[0]) , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_real_w = torch.linspace(int(real_tl[1]) ,int(real_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    # Project depth map to the pointcloud real\n",
    "    cam_scale = 10000\n",
    "    real_pixels = torch.stack( [grid_real_w[flow_mask], grid_real_h[flow_mask], torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)[flow_mask]], dim=1 ).type(typ)\n",
    "    K_inv = torch.inverse(K_real.type(torch.float64)).type(typ)\n",
    "    P_real = K_inv @ real_pixels.T\n",
    "    P_real = P_real.type(torch.float64) * real_d[flow_mask] / cam_scale\n",
    "    P_real = P_real.T\n",
    "\n",
    "    \n",
    "    grid_ren_h = torch.linspace(int(ren_tl[0]) ,int(ren_br[0]), 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_ren_w = torch.linspace(int(ren_tl[1]) ,int(ren_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    crop_d_pixels = torch.stack( [grid_ren_w.flatten(), grid_ren_h.flatten(), torch.ones(grid_ren_w.shape, device = u_map.device,  dtype= torch.float64).flatten()], dim=1 ).type(typ)\n",
    "    K_inv = torch.inverse(K_ren.type(torch.float64)).type(typ)\n",
    "    P_crop_d = K_inv @ crop_d_pixels.T.type(typ)\n",
    "    P_crop_d = P_crop_d.type(torch.float64) * render_d.flatten() / cam_scale\n",
    "    P_crop_d = P_crop_d.T\n",
    "\n",
    "\n",
    "    render_d_ind_h = torch.linspace(0 ,479 , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    render_d_ind_w= torch.linspace(0 ,639 , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    render_d_ind_h = torch.clamp((render_d_ind_h - u_map).type(torch.float64) ,0,479).type( torch.long )[flow_mask]\n",
    "    render_d_ind_w = torch.clamp((render_d_ind_w - v_map).type(torch.float32),0,639).type( torch.long )[flow_mask] \n",
    "    index = render_d_ind_h*640 + render_d_ind_w # hacky indexing along two dimensions\n",
    "\n",
    "    P_crop_d  = P_crop_d[index] \n",
    "    crop_d_pixels = crop_d_pixels[index] \n",
    "\n",
    "\n",
    "    m = filter_pcd( P_crop_d)\n",
    "    P_crop_d  = P_crop_d[ m ]\n",
    "    P_real = P_real[m]\n",
    "    real_pixels = real_pixels[m]\n",
    "    P_ren = P_crop_d\n",
    "\n",
    "    # random shuffel\n",
    "    pts_trafo = min(P_real.shape[0], 50000)\n",
    "    idx = torch.randperm( P_real.shape[0] )[0:pts_trafo]\n",
    "    P_real = P_real[idx]\n",
    "    P_ren = P_ren[idx]\n",
    "    real_pixels = real_pixels[idx]\n",
    "\n",
    "    nr = 10000\n",
    "    mp_render = get_H(mp)\n",
    "    mp_render = (mp_render.type(typ)@ h_render.type(typ).T)[::5,:3]\n",
    "    mp_real = get_H(mp)\n",
    "    mp_real = (mp_real.type(typ) @ h_real.type(typ).T)[::5,:3]\n",
    "\n",
    "#    \n",
    "\n",
    "    P_ren_in_origin =  (get_H( P_ren ).type(typ) @ torch.inverse( h_render.type(torch.float64) ).type(typ).T) [:,:3]\n",
    "\n",
    "    # PNP estimation\n",
    "    objectPoints = P_ren_in_origin.clone().numpy()    \n",
    "    imagePoints = real_pixels[:,:2].numpy()\n",
    "    dist = np.array( [[0.0,0.0,0.0,0.0]] )\n",
    "\n",
    "    if objectPoints.shape[0] < 8:\n",
    "#         visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "#             epoch = 1,\n",
    "#             img= render_img[0].numpy()  ,\n",
    "#             tl = ren_tl,\n",
    "#             br = ren_br,\n",
    "#             points = P_ren[:nr,:3],\n",
    "#             store = False,\n",
    "#             jupyter= True,\n",
    "#             K = K_ren.cpu().numpy(),\n",
    "#             H = np.eye(4),\n",
    "#             method='def')\n",
    "\n",
    "#         plot_two_pcd(mp_render.numpy(), P_ren[:nr,:3].numpy())\n",
    "\n",
    "#         #inp = render_img[0].clone()\n",
    "#         inp = render_img_original[0].clone()\n",
    "#         inp = real_img_original[0].clone()\n",
    "#         for j in range(0,nr):\n",
    "#             try:\n",
    "\n",
    "#                 v = int(real_pixels[j,0])\n",
    "#                 u = int(real_pixels[j,1])\n",
    "#                 inp[u:u+4,v:v+4,: ] = torch.tensor( [255,0,255])\n",
    "#             except:\n",
    "#                 pass\n",
    "#         display( Image.fromarray( np.uint8(inp.numpy())) )\n",
    "        \n",
    "        print(f'Failed due to missing corsspondences,{ objectPoints.shape[0]}, Filtering given depth: {m.shape}, index {index.shape}')\n",
    "        return False, torch.eye(4, dtype= u_map.dtype, device=u_map.device )\n",
    "    # set current guess as the inital estimate\n",
    "\n",
    "    rvec = R.from_matrix(h_real_est[:3,:3]).as_rotvec().astype(np.float32)\n",
    "    tvec = h_real_est[:3,3].numpy().astype(np.float32)\n",
    "    # calculate PnP between the pixels coordinates in the real image and the corrosponding points in the origin frame\n",
    "    start = time.time()\n",
    "    if mode == \"refine\":\n",
    "        r_vec2, t_vec2 = cv2.solvePnPRefineLM(copy.deepcopy(objectPoints), \\\n",
    "            copy.deepcopy(imagePoints), \n",
    "            K_real.numpy(), \n",
    "            dist, \n",
    "            copy.deepcopy(rvec),\n",
    "            copy.deepcopy( tvec))\n",
    "    elif mode == \"iterative1000\" :\n",
    "        retval, r_vec2, t_vec2, inliers = cv2.solvePnPRansac(copy.deepcopy(objectPoints), \\\n",
    "            copy.deepcopy(imagePoints), \n",
    "            K_real.numpy(), \n",
    "            dist, \n",
    "            iterationsCount = 1000, reprojectionError= 0.3)\n",
    "        r_vec2 = r_vec2[:,None]\n",
    "    elif mode == \"iterative100\" :\n",
    "        retval, r_vec2, t_vec2, inliers = cv2.solvePnPRansac(copy.deepcopy(objectPoints), \\\n",
    "            copy.deepcopy(imagePoints), \n",
    "            K_real.numpy(), \n",
    "            dist, \n",
    "            iterationsCount = 100, reprojectionError= 0.3)\n",
    "        r_vec2 = r_vec2[:,None]\n",
    "    elif mode == \"iterative100reER1\":\n",
    "        retval, r_vec2, t_vec2, inliers = cv2.solvePnPRansac(copy.deepcopy(objectPoints), \\\n",
    "            copy.deepcopy(imagePoints), \n",
    "            K_real.numpy(), \n",
    "            dist, \n",
    "            iterationsCount = 1000, reprojectionError= 1)\n",
    "        r_vec2 = r_vec2[:,None]\n",
    "    print(f\"Ransac time mode {mode} took: {time.time()-st}s\")\n",
    "\n",
    "    \n",
    "\n",
    "    h = rvec_tvec_to_H(r_vec2[:,0],t_vec2)\n",
    "    return True,torch.from_numpy( h).type(u_map.dtype) \n",
    "\n",
    "if False:\n",
    "    h = flow_to_trafo_PnP(real_br = real_br[b].clone(),\n",
    "        real_tl = (real_tl[b]).clone() ,\n",
    "        ren_br = (ren_br[b]).clone(),\n",
    "        ren_tl = (ren_tl[b]).clone(),\n",
    "        flow_mask = (flow_mask[b]).clone(),\n",
    "        u_map = (u_map[b].type( typ )).clone(),\n",
    "        v_map = (v_map[b].type( typ )).clone(), \n",
    "        K_real = (K_real.type( typ )).clone(),\n",
    "        K_ren = (K_ren.type( typ )).clone(),\n",
    "        real_d = (real_d[b].type( typ )).clone(),\n",
    "        render_d = (render_d[b].type( typ )).clone(),\n",
    "        h_real = (h_real[b].type(typ )).clone(),\n",
    "        h_render = (h_render[b].type( typ )).clone(),\n",
    "        h_real_est = h_real_est,\n",
    "        mp = model_points[0],\n",
    "        img = real_img_original[0].clone() )\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14025"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonfrey/miniconda3/envs/track_latest/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ero_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3544533ebb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mflow_mask_eroded\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflow_mask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mero_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0msuc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m### GT Label PnP ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ero_out' is not defined"
     ]
    }
   ],
   "source": [
    "from visu import plot_two_pcd\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "visualizer = Visualizer('/home/jonfrey/Debug', None)\n",
    "\n",
    "K_ren = torch.tensor( dataset_train._backend.get_camera('data_syn/0019', K=True), device=device ) \n",
    "K_ren = torch.tensor( dataset_train._backend.get_camera('data/0049', K=True), device=device ) \n",
    "\n",
    "max_iter = 100\n",
    "names = ['ID','ADD-S GT','ADD-S INITAL','ADD-S Random', 'ADD-S Bias']\n",
    "df = pd.DataFrame(columns=names)\n",
    "df_failed = pd.DataFrame(columns= ['ID'])\n",
    "        \n",
    "for j,batch in enumerate(dataloader_test):\n",
    "    batch = batch[0]\n",
    "    if j > max_iter-1:\n",
    "        break\n",
    "    if j % 10 == 0:\n",
    "        print(f\"Processed {j}/{max_iter}\")\n",
    "\n",
    "    b = 0    \n",
    "    model_points = batch[4]\n",
    "    idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "    label = batch[7]\n",
    "    real_img_original = batch[8]\n",
    "    cam = batch[9]\n",
    "    gt_rot_wxyz, gt_trans, unique_desig = batch[10:13] # unique_desig[1] contains the idx starting at 1 for the first object \n",
    "    bs = model_points.shape[0]\n",
    "    if batch[13] is False:\n",
    "        print('Continue')\n",
    "        continue\n",
    "    real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "    pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "    u_map, v_map, flow_mask, bb, depth_render_original= batch[24:]\n",
    "    real_tl, real_br, ren_tl, ren_br = bb \n",
    "    \n",
    "    data = torch.cat([real_img, render_img], dim=1)\n",
    "    uv_gt = torch.stack( [u_map, v_map], dim=3 ).permute(0,3,1,2)\n",
    "    \n",
    "    K_real = torch.tensor( [[cam[b,2],0,cam[b,0]],[b,cam[b,3],cam[b,1]],[0,0,1]], device=device )\n",
    "    \n",
    "    #get inital estimate of the poistion given by the dataloader\n",
    "    h_real_est = torch.eye(4,device=device)\n",
    "    h_real_est[:3,:3] = quat_to_rot(pred_rot_wxyz[b][None,:], conv='wxyz', device=device)\n",
    "    h_real_est[:3,3] = torch.tensor( pred_trans[b].clone().detach() ,device=device )\n",
    "    \n",
    "    \n",
    "    ### GT Estimate ###\n",
    "    typ = u_map.dtype\n",
    "    fmt = flow_mask.dtype\n",
    "    flow_mask_eroded  = (flow_mask * ero_out.type(torch.float32)).type(fmt)[:,0]\n",
    "    suc = True\n",
    "    ### GT Label PnP ###\n",
    "    suc_, gt_h_est = flow_to_trafo_PnP(\n",
    "        real_br = real_br[b].clone(),\n",
    "        real_tl = (real_tl[b]).clone(), \n",
    "        ren_br = (ren_br[b]).clone(), \n",
    "        ren_tl = (ren_tl[b]).clone(),\n",
    "        flow_mask = (flow_mask[b]).clone(), \n",
    "        u_map = (u_map[b].type( typ )).clone(),\n",
    "        v_map = (v_map[b].type( typ )).clone(), \n",
    "        K_real = (K_real.type( typ )).clone(),\n",
    "        K_ren = (K_ren.type( typ )).clone(),\n",
    "        real_d = (real_d[b].type( typ )).clone(),\n",
    "        render_d = (render_d[b].type( typ )).clone(),\n",
    "        h_real = (h_real[b].type( typ )).clone(), \n",
    "        h_render = (h_render[b].type( typ )).clone(),\n",
    "        h_real_est = h_real_est,\n",
    "        mp = model_points[b].clone(),\n",
    "        img = real_img_original[b].clone())\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    \n",
    "    ### N1 ###\n",
    "    lvl = 5\n",
    "    n1 = torch.rand(v_map[b].shape, dtype = typ)\n",
    "    n2 = torch.rand(v_map[b].shape, dtype = typ)\n",
    "    u_map_clone = (u_map[b]).type( typ ).clone() + (n1 - 0.5) * 2 * lvl\n",
    "    v_map_clone = (v_map[b]).type( typ ).clone() + (n2 - 0.5) * 2 * lvl\n",
    "    \n",
    "    suc_, n1_h_est = flow_to_trafo_PnP(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask_eroded[b]), \n",
    "        u_map = u_map_clone, \n",
    "        v_map = v_map_clone, \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )),\n",
    "        h_real_est = h_real_est.clone(),\n",
    "        mp = model_points[b].clone(),\n",
    "        img = real_img_original[b].clone())\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    \n",
    "    ### N2 ###\n",
    "    lvl = 5\n",
    "    n1 = float(torch.rand((1), dtype = typ ))\n",
    "    n2 = float(torch.rand((1), dtype = typ ))\n",
    "    u_map_clone = (u_map[b]).type( typ ).clone() + float( (n1 - 0.5) * 2 * lvl )\n",
    "    v_map_clone = (v_map[b]).type( typ ).clone() + float( (n2 - 0.5) * 2 * lvl )\n",
    "    \n",
    "    suc_, n2_h_est = flow_to_trafo_PnP(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask_eroded[b]), \n",
    "        u_map = u_map_clone, \n",
    "        v_map = v_map_clone, \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )),\n",
    "        h_real_est = h_real_est.clone(),\n",
    "        mp = model_points[b].clone(),\n",
    "        img = real_img_original[b].clone())\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    if not suc: \n",
    "        df_failed = df_failed.append({'ID': int( unique_desig[1])}, ignore_index=True)\n",
    "        continue \n",
    "        \n",
    "    p = model_points.shape[1]\n",
    "    target = torch.bmm( model_points, torch.transpose(h_real[:,:3,:3], 1,2 ) ) + h_real[:,:3,3][:,None,:].repeat(1,p,1)\n",
    "\n",
    "        \n",
    "    # Compute ADD-S\n",
    "    adds_res_gt_flow = float(criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = gt_h_est[None].type( target.dtype)).detach())\n",
    "    adds_res_n1 = float(criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = n1_h_est[None].type( target.dtype) ).detach())\n",
    "    adds_res_n2 = float(criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = n2_h_est[None].type( target.dtype) ).detach())\n",
    "    adds_init = float(criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = h_real_est[None].type( target.dtype)).detach())\n",
    "    \n",
    "    \n",
    "    test_values = [int( unique_desig[1]), adds_res_gt_flow, adds_init, adds_res_n1, adds_res_n2]\n",
    "    res = {names[i]: test_values[i] for i in range(len(names))} \n",
    "    df = df.append(res, ignore_index=True)\n",
    "    \n",
    "    print(f'Not Iteration {j} < 0.02 ADD-S, GT', adds_res_gt_flow)\n",
    "    \n",
    "    if adds_res_gt_flow > 0.2 :\n",
    "\n",
    "#             break\n",
    "#         continue\n",
    "#         print(\"Real depth map cropped\")\n",
    "        real_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "#        real_depth_img = Drawer().disp_img_1d(depth_render_original[0][0].numpy(),ret=True)\n",
    "#         real_depth_img = np.repeat( real_depth_img[:,:,None],3, axis=2)\n",
    "#         print(\"Render depth map cropped\")\n",
    "#         render_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "#         render_depth_img = np.repeat( render_depth_img[:,:,None],3, axis=2)\n",
    "#         Drawer().disp_img_1d(flow_mask[0],ret=True)\n",
    "\n",
    "#         sub = max(1,int( P_real_in_center.shape[0]/100 ) )\n",
    "#         plot_two_pcd_line(P_real_in_center[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "#         plot_two_pcd_line(P_real_trafo[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "\n",
    "#         print(f\"Real Image, Estimated Points given GT Flow {P_real_in_center.shape}\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_img_original[b].cpu().numpy(),\n",
    "#                         points =copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = h_real[0].numpy(),\n",
    "#                         method='def')\n",
    "#         print(\"Real Image Cropped, Estimated Points given GT Flow\")\n",
    "#         visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_depth_img,\n",
    "#                         tl = real_tl[0],\n",
    "#                         br = real_br[0],\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "\n",
    "#         fil = label == unique_desig[1]    \n",
    "#         real_img_original[b][ fil[0][:,:,None].repeat(1,1,3) ] = 255\n",
    "#         print(\"Real Image, Estimated Points given GT Flow with label is white\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_img_original[b].cpu().numpy(),\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"GT: Render Image Cropped\")\n",
    "        visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= render_img[0].numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            tl = ren_tl[0],\n",
    "                            br = ren_br[0],\n",
    "                            store = False,\n",
    "                            jupyter=True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = h_render[0].numpy(),\n",
    "                            method='def')\n",
    "        print(\"PNP, Real Image\")\n",
    "        visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= real_img_original[b].cpu().numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            store = False,\n",
    "                            jupyter= True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = pnp_h_gt.clone().numpy(),\n",
    "                            method='def')\n",
    "        \n",
    "        print(\"PNP: Render Image Cropped\")\n",
    "        visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= render_img[0].numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            tl = ren_tl[0],\n",
    "                            br = ren_br[0],\n",
    "                            store = False,\n",
    "                            jupyter=True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = h_render[0].clone().numpy(),\n",
    "                            method='def')\n",
    "\n",
    "#         print('Valid Flow Mask')\n",
    "#         visualizer.plot_segmentation('tag', 1, flow_mask_eroded[b] , store=False, method='def', jupyter=True)\n",
    "#         visualizer.plot_segmentation('tag', 1, flow_mask[b]  , store=False, method='def', jupyter=True)\n",
    "\n",
    "        print(\"Corrospondence\")\n",
    "        visualizer.plot_corrospondence(tag=f'_',\n",
    "                                           epoch=0,\n",
    "                                            u_map=u_map[0], \n",
    "                                            v_map=v_map[0], \n",
    "                                            flow_mask=flow_mask[0], \n",
    "                                            real_img=real_img[0], \n",
    "                                            render_img=render_img[0],\n",
    "                                            store=False,\n",
    "                                            jupyter=True,\n",
    "                                            coloful=True,\n",
    "                                            method='def',\n",
    "                                            res_h=10,\n",
    "                                            res_w=10)\n",
    "        \n",
    "         \n",
    "    #if float(adds_res_pnp_gt.detach()) > 0.02 :\n",
    "     #   break\n",
    "# print(f'Result, GT Avg', sum(adds_gt_ls)/len(adds_gt_ls), 'Errode Avg', sum(adds_erode_ls)/len(adds_erode_ls) )\n",
    "print(\"Finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random: Takeing Ground Truth Flow and adding noise uniform in between +20 -20 pixels for each pixel')\n",
    "print('Bias: Takeing Ground Truth Flow and offsetting the flow by a random value in between +20 -20 pixels for the full image')\n",
    "print( df['ID'].value_counts() )\n",
    "# print( df_failed['ID_Failed'].value_counts())\n",
    "df.groupby(['ID']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df_failed['ID'].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MEAN:\\n\", df.mean() )\n",
    "print(\"\\n \\nSTD:\\n\", df.std() )\n",
    "print(\"\\n \\nMAX:\\n\", df.max())\n",
    "print(\"\\n \\nIDXMAX:\\n\", df.idxmax())\n",
    "\n",
    "idxmax = df.idxmax()[0]\n",
    "print(f'\\n \\nWorst Sample Index is {idxmax}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_latest",
   "language": "python",
   "name": "track_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
