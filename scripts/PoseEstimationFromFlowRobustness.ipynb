{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonfrey/PLR3/src/loaders_v2/dataset_ycb.py:426: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  init_trans = torch.normal(mean=torch.tensor(gt_trans), std=nt)\n",
      "/home/jonfrey/PLR3/src/helper/bounding_box.py:203: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  masked_idx = (d != 0).nonzero()\n",
      "/home/jonfrey/miniconda3/envs/track_latest/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "os.chdir('/home/jonfrey/PLR3')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd() + '/src'))\n",
    "sys.path.append(os.path.join(os.getcwd() + '/lib'))\n",
    "\n",
    "import loaders_v2\n",
    "from loaders_v2 import GenericDataset\n",
    "from rotations import * \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from visu import plot_pcd, Visualizer\n",
    "import copy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from helper import re_quat\n",
    "from PIL import Image, ImageDraw\n",
    "from deep_im import LossAddS\n",
    "import copy\n",
    "#from deep_im import flow_to_trafo\n",
    "from visu import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import k3d\n",
    "exp_cfg_path = '/home/jonfrey/PLR3/yaml/exp/exp_ws_deepim_debug_natrix.yml'\n",
    "exp_cfg_path = '/home/jonfrey/PLR3/yaml/exp/exp_evaluate_pose_estimation.yml'\n",
    "env_cfg_path = '/home/jonfrey/PLR3/yaml/env/env_natrix_jonas.yml'\n",
    "\n",
    "h = 480\n",
    "w = 640\n",
    "\n",
    "\n",
    "def load_from_file(p):\n",
    "    if os.path.isfile(p):\n",
    "        with open(p, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return data\n",
    "\n",
    "exp = load_from_file(exp_cfg_path)\n",
    "env = load_from_file(env_cfg_path)\n",
    "\n",
    "dataset_train = GenericDataset(\n",
    "    cfg_d=exp['d_train'],\n",
    "    cfg_env=env)\n",
    "\n",
    "batch = dataset_train[13450][0] #bann 10450\n",
    "points, choose, img, target, model_points, idx = batch[0:6]\n",
    "depth_img, label_img, img_orig, cam = batch[6:10]\n",
    "gt_rot_wxyz, gt_trans, unique_desig = batch[10:13]\n",
    "\n",
    "real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "pred_rot_wxyz, pred_trans, pred_points, h_render,h_real, render_img_original = batch[18:24]\n",
    "u_map, v_map, flow_mask,  bb = batch[24:]\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drawer():\n",
    "    def __init__(self):\n",
    "        self.im_in_plot = 0\n",
    "        self.data = []\n",
    "        \n",
    "    def disp_img_1d(self,img,hold=False, save=False, nr=0 , ret=False):\n",
    "        self.data.append(img)\n",
    "        p = '/home/jonfrey/Debug/Midterm2/'\n",
    "        \n",
    "        if not hold:\n",
    "            fig = plt.figure(figsize=(6*2*len(self.data),7))\n",
    "            ax = []\n",
    "            for j,a in enumerate(self.data):\n",
    "                ax.append( fig.add_subplot(1,len(self.data), j+1)  )\n",
    "                \n",
    "                ax[-1].get_xaxis().set_visible(False)\n",
    "                ax[-1].get_yaxis().set_visible(False)\n",
    "                pos = ax[-1].imshow( a, cmap='Reds' )\n",
    "                \n",
    "                fig.colorbar(pos, ax=ax[-1])\n",
    "            plt.show()\n",
    "            if save:\n",
    "                fig.savefig(p+str(nr)+'.png', dpi=300)\n",
    "                \n",
    "            if ret: \n",
    "                if isinstance( self.data[0], torch.Tensor):\n",
    "                    self.data[0] = self.data[0].numpy()\n",
    "                    print('CONV')\n",
    "                    \n",
    "                print(self.data[0].shape)\n",
    "                a = np.max(self.data[0])\n",
    "                b = np.min(self.data[0])\n",
    "                \n",
    "                d = (self.data[0]-float(b))\n",
    "                d = (d / ((float(a)-float(b))) )*255 \n",
    "                d = np.uint8(d)\n",
    "                img = Image.fromarray( d )\n",
    "                return d\n",
    "            self.data = []\n",
    "            self.ax = []\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "desig = unique_desig[0]\n",
    "\n",
    "# [('data/0003/001742',), tensor([8], dtype=torch.int32)]\n",
    "\n",
    "desig = 'data/0003/001742'\n",
    "\n",
    "_p_ycb = \"/media/scratch1/jonfrey/datasets/YCB_Video_Dataset\"\n",
    "depth = np.array(Image.open(\n",
    "    '{0}/{1}-depth.png'.format(_p_ycb, desig)))\n",
    "depth.shape\n",
    "\n",
    "label = np.array(Image.open(\n",
    "    '{0}/{1}-label.png'.format(_p_ycb, desig)))\n",
    "img = np.array(Image.open(\n",
    "    '{0}/{1}-color.png'.format(_p_ycb, desig)))\n",
    "\n",
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "def disp_alignment(depth, label, real):\n",
    "    data = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    t = real\n",
    "    data[:,:,:3] = t.numpy() # red patch in upper left\n",
    "    data_depth[:,:,:3] = t.numpy()\n",
    "    data[:,:,3] = 70\n",
    "    data[:,:,3][label==8] = 255\n",
    "    \n",
    "    min_val = torch.min( depth[depth!=0] )\n",
    "    max_val = torch.max( depth[depth!=0] )\n",
    "    val = torch.clamp( ((depth-min_val) // (max_val-min_val))*255, 0, 255)\n",
    "    \n",
    "    img = Image.fromarray(data, 'RGBA')\n",
    "    display(img)\n",
    "\n",
    "def plot_mask(mask):\n",
    "    min_val = torch.min( mask )\n",
    "    max_val = float( max(1,torch.max( mask )) )\n",
    "    mask = torch.clamp( (mask-min_val) / (max_val-min_val)*255 ,0,255)\n",
    "    \n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth[:,:,3] = 255\n",
    "    for i in range(480):\n",
    "        for j in range(640):\n",
    "            data_depth[i,j,:4] = np.array( cmaplist[ int(mask[i,j])] )*255\n",
    "    data_depth[:,:,3] = 255\n",
    "    data_depth[:,:,3][label==2] = 255\n",
    "    img_depth = Image.fromarray(data_depth, 'RGBA')\n",
    "    display(img_depth)\n",
    "\n",
    "batch = dataset_train._backend.getElement( desig, 8)\n",
    "batch = batch #bann 10450   \n",
    "model_points = batch[4]\n",
    "idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "real_img_original = batch[8]\n",
    "cam = batch[9]\n",
    "real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "u_map, v_map, flow_mask, bb = batch[24:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "#disp_alignment(real_d[0], gt_label_cropped, real_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'ycb', 'objects': 21, 'num_points': 1000, 'num_pt_mesh_small': 2300, 'num_pt_mesh_large': 2300, 'obj_list_fil': None, 'obj_list_sym': [12, 15, 18, 19, 20], 'batch_list_cfg': {'sequence_names': None, 'seq_length': 1, 'no_list_for_sequence_len_one': True, 'fixed_length': True, 'sub_sample': 1, 'mode': 'train', 'add_syn_to_train': True}, 'noise_cfg': {'status': False, 'noise_trans': 0}, 'output_cfg': {'overfitting_nr_idx': -1, 'vm_in_dataloader': True, 'noise_translation': 0.02, 'noise_rotation': 20, 'return_same_size_tensors': True, 'force_one_object_visible': True, 'status': False, 'refine': False, 'add_depth_image': True, 'add_mask_image': True, 'norm_render': False, 'color_jitter_render': {'active': False, 'cfg': [0.2, 0.2, 0.2, 0.05]}, 'norm_real': False, 'color_jitter_real': {'active': False, 'cfg': [0.2, 0.2, 0.2, 0.05]}, 'visu': {'status': True, 'return_img': True}}}\n"
     ]
    }
   ],
   "source": [
    "exp['d_train'][\"output_cfg\"]['overfitting_nr_idx'] = -1\n",
    "print(exp['d_train'])\n",
    "dataset_train = GenericDataset(\n",
    "    cfg_d=exp['d_train'],\n",
    "    cfg_env=env)\n",
    "device = 'cuda'\n",
    "exp['loader']['batch_size'] = 1\n",
    "exp['loader']['pin_memory'] = False\n",
    "exp['loader']['shuffle'] = False\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                                       **exp['loader'])\n",
    "criterion_adds = LossAddS(sym_list=exp['d_train']['obj_list_sym'])\n",
    "\n",
    "def eroision(t,size=3):\n",
    "    \"t: tensor shape BS, C, H,W\"\n",
    "    out_c = t.shape[1]\n",
    "    kernel_tensor = torch.ones( (out_c,1,size,size) )\n",
    "    print(size, kernel_tensor, t.shape)\n",
    "    return torch.nn.functional.conv2d(t, kernel_tensor, padding=(int((size)/2), int((size)/2))) == (size*size)\n",
    "\n",
    "def eroision_batch(t,t_size):\n",
    "    \"t: tensor shape BS, C, H,W\"\n",
    "    \"t_size: tensor shape BS\"\n",
    "    out_c = t.shape[1]\n",
    "    for b in range( t.shape[0] ):\n",
    "        size = int( t_size[b] )\n",
    "        kernel_tensor = torch.ones( (out_c,1,size,size) )\n",
    "        t[b] = (torch.nn.functional.conv2d(t[b][None], kernel_tensor, padding=(int((size)/2), int((size)/2))) == (size*size))[0,:,:t.shape[2], :t.shape[3]]\n",
    "    return t\n",
    "\n",
    "def plot_two_pcd_line(x, y, point_size=0.005, c1='g', c2='r'):\n",
    "    if c1 == 'b':\n",
    "        k = 245\n",
    "    elif c1 == 'g':\n",
    "        k = 25811000\n",
    "    elif c1 == 'r':\n",
    "        k = 11801000\n",
    "    elif c1 == 'black':\n",
    "        k = 2580\n",
    "    else:\n",
    "        k = 2580\n",
    "\n",
    "    if c2 == 'b':\n",
    "        k2 = 245\n",
    "    elif c2 == 'g':\n",
    "        k2 = 25811000\n",
    "    elif c2 == 'r':\n",
    "        k2 = 11801000\n",
    "    elif c2 == 'black':\n",
    "        k2 = 2580\n",
    "    else:\n",
    "        k2 = 2580\n",
    "\n",
    "    col1 = np.ones(x.shape[0]) * k\n",
    "    col2 = np.ones(y.shape[0]) * k2\n",
    "    plot = k3d.plot(name='points')\n",
    "    plt_points = k3d.points(x, col1.astype(np.uint32), point_size=point_size)\n",
    "    plot += plt_points\n",
    "    plt_points = k3d.points(y, col2.astype(np.uint32), point_size=point_size)\n",
    "    plot += plt_points\n",
    "    for i in range(min(100,x.shape[0]) ):\n",
    "        plot += k3d.line([x[i],y[i]],shader='mesh', width=0.0005, color=0xff0000)\n",
    "    \n",
    "    plt_points.shader = '3d'\n",
    "    plot.display()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.7223],\n",
      "        [0.0000, 1.0000, 0.0000, 0.7223],\n",
      "        [0.0000, 0.0000, 1.0000, 0.7223],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([[1.7223, 1.7223, 1.7223],\n",
      "        [1.7223, 1.7223, 1.7223],\n",
      "        [1.7223, 1.7223, 1.7223],\n",
      "        ...,\n",
      "        [1.7223, 1.7223, 1.7223],\n",
      "        [1.7223, 1.7223, 1.7223],\n",
      "        [1.7223, 1.7223, 1.7223]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from helper import anal_tensor\n",
    "\n",
    "\n",
    "def solve_transform(keypoints, gt_keypoints):\n",
    "    \"\"\"\n",
    "    keypoints: N x K x 3\n",
    "    gt_keypoints: K x 3\n",
    "    return: N x 4 x 4 transformation matrix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keypoints = keypoints.clone()\n",
    "        gt_keypoints = gt_keypoints.clone()\n",
    "        N, K, _ = keypoints.shape\n",
    "        center = keypoints.mean(dim=1)\n",
    "        gt_center = gt_keypoints.mean(dim=0)\n",
    "        keypoints -= center[:, None, :]\n",
    "        gt_keypoints -= gt_center[None]\n",
    "        matrix = keypoints.transpose(2, 1) @ gt_keypoints[None]\n",
    "        U, S, V = torch.svd(matrix)\n",
    "        Vt = V.transpose(2, 1)\n",
    "        Ut = U.transpose(2, 1)\n",
    "\n",
    "        d = (V @ Ut).det()\n",
    "        I = torch.eye(3, 3, dtype=gt_center.dtype, device= keypoints.device)[None].repeat(N, 1, 1)\n",
    "        I[:, 2, 2] = d.clone()\n",
    "\n",
    "        R = U @ I @ Vt\n",
    "        T = torch.zeros(N, 4, 4, dtype=gt_center.dtype, device= keypoints.device)\n",
    "        T[:, 0:3, 0:3] = R\n",
    "        T[:, 0:3, 3] = center[None] - (R @ gt_center[None :, None])[:, :, 0]\n",
    "        T[:, 3, 3] = 1.0\n",
    "\n",
    "        return T\n",
    "    except RuntimeError as error:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        print(\"Something went wrong\")\n",
    "\n",
    "        # costume implementation \n",
    "def solve_transform2(A,B):\n",
    "    if A.shape[0] > B.shape[0]:\n",
    "        x=torch.arange(A.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:B.shape[0]]\n",
    "        A = torch.index_select(A, 0, out)\n",
    "    if A.shape[0] < B.shape[0]:\n",
    "        x=torch.arange(B.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:A.shape[0]]\n",
    "        B = torch.index_select(B, 0, out)\n",
    "\n",
    "    #A = torch.choice(A,B.shape[0])\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    m = A.shape[1]\n",
    "    centroid_A = torch.mean(A, dim=0)\n",
    "    centroid_B = torch.mean(B, dim=0)\n",
    "\n",
    "    AA = (A - centroid_A)\n",
    "    BB = (B - centroid_B)\n",
    "    H = AA.transpose(0,1) @ BB\n",
    "    U, S, Vt = torch.svd(H)\n",
    "    R = Vt @ U.transpose(0,1)\n",
    "    if torch.det(R) < 0:\n",
    "        Vt[m-1,:] *= -1\n",
    "        R = Vt.transpose(0,1) @ U.transpose(0,1)\n",
    "\n",
    "    # translation\n",
    "    t = centroid_B - (R @ centroid_A)\n",
    "    # homogeneous transformation\n",
    "    T = torch.eye(m+1, device= A.device)\n",
    "    T[:m, :m] = R\n",
    "    T[:m, m] = t\n",
    "    return T\n",
    "\n",
    "NR = 1000\n",
    "DIM = 3\n",
    "A = torch.ones( (NR,DIM), dtype= torch.float32)\n",
    "B = A*1.7223\n",
    "T = solve_transform2(A,B)\n",
    "print(T)\n",
    "def get_H(pcd):\n",
    "    pcd_ret = torch.ones( (pcd.shape[0],pcd.shape[1]+1),device=pcd.device, dtype=pcd.dtype )\n",
    "    pcd_ret[:,:3] = pcd\n",
    "    return pcd_ret\n",
    "\n",
    "A_hom = get_H(A)\n",
    "A_hom2 = A_hom @ T.T\n",
    "print(A_hom2[:,:3])\n",
    "\n",
    "def filter_pcd_given_depthmap(pcd, depth, scal= 10000):\n",
    "    \"\"\"\n",
    "    pcd = Nx3 troch.float32\n",
    "    depth = N torch.float32\n",
    "\n",
    "    return N torch.bool\n",
    "    \"\"\"\n",
    "    m1 = (depth/scal) > 0.2\n",
    "    #print( \"Thorwn away values\", (depth/scal) < 0.2 )\n",
    "    return m1\n",
    "\n",
    "    val_d = depth[ m1 ]\n",
    "    mean = torch.mean(val_d)\n",
    "    new_d = depth - mean\n",
    "    tol = 0.5\n",
    "    m2 = torch.abs( new_d/scal ) < tol \n",
    "    return m1 * m2\n",
    "    \n",
    "def filter_pcd( pcd, tol = 0.6):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        pcd : Nx3 torch.float32\n",
    "    returns:\n",
    "        mask : NX3 torch.bool \n",
    "    \"\"\"\n",
    "    m = torch.mean(pcd, dim = 0)\n",
    "    comp = m[None,:].repeat(pcd.shape[0],1) + tol\n",
    "    mean_free = pcd-m[None,:].repeat(comp.shape[0],1)\n",
    "    mask = torch.norm( mean_free,  dim= 1) > tol\n",
    "    #print(f\"filter_pcd PRE: {pcd.shape}, POST: {float(torch.sum(mask[:,None].repeat(1,3) == False ))/3.0}\")\n",
    "    return mask[:,None].repeat(1,3) == False\n",
    "\n",
    "def filter_pcd_cor(pcd1, pcd2, max_mean_deviation=0.2):\n",
    "    \n",
    "    dif = torch.norm( pcd1-pcd2 , dim= 1)\n",
    "    mean = torch.mean(dif, dim = 0)\n",
    "    mean_free = torch.abs(dif-mean)\n",
    "    #print(f\"filter_pcd_cor PRE: {pcd1.shape[0]}, POST: {torch.sum(mean_free < max_mean_deviation)}\")\n",
    "    return mean_free < max_mean_deviation\n",
    "    \n",
    "def flow_to_trafo(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      real_br: torch.tensor torch.Size([2])\n",
    "      real_tl: torch.tensor torch.Size([2])\n",
    "      ren_br: torch.tensor torch.Size([2])\n",
    "      ren_tl: torch.tensor torch.Size([2])\n",
    "      flow_mask: torch.Size([480, 640])\n",
    "      u_map: torch.Size([480, 640])\n",
    "      v_map: torch.Size([480, 640])\n",
    "      K_real: torch.Size([3, 3])\n",
    "      K_ren: torch.Size([3, 3])\n",
    "      real_d: torch.Size([480, 640]) \n",
    "      render_d: torch.Size([480, 640])\n",
    "      h_real: torch.Size([4, 4])\n",
    "      h_render: torch.Size([4, 4])\n",
    "    output:\n",
    "      P_real_in_center: torch.Size([N, 3])\n",
    "      P_ren_in_center: torch.Size([N, 3]) \n",
    "      P_real_trafo: torch.Size([N, 3])\n",
    "      T_res: torch.Size([4, 4])\n",
    "      \n",
    "      The output rotation T_res is defined in the Camera coordinate frame. \n",
    "      Therfore premultiply the T_Res with h_render to get the new h_real_new !!!\n",
    "    \"\"\"\n",
    "    for k in kwargs.keys():\n",
    "        pass\n",
    "        #print(f\"Variable: {k}, Type {type(kwargs[k])}, Dtype{kwargs[k].dtype}, Shape{kwargs[k].shape}\")\n",
    "    real_br = kwargs['real_br']\n",
    "    real_tl = kwargs['real_tl']\n",
    "    ren_br = kwargs['ren_br']\n",
    "    ren_tl = kwargs['ren_tl']\n",
    "    flow_mask = kwargs['flow_mask']\n",
    "    u_map = kwargs['u_map']\n",
    "    v_map = kwargs['v_map']\n",
    "    K_real = kwargs['K_real']\n",
    "    K_ren = kwargs['K_ren']\n",
    "    real_d = kwargs['real_d']\n",
    "    render_d = kwargs['render_d']\n",
    "    h_real = kwargs['h_real']\n",
    "    h_render = kwargs['h_render']\n",
    "    # print(\"Flow mask input reduces to \", flow_mask.shape, torch.sum(flow_mask))\n",
    "\n",
    "    # Grid for upsampled real\n",
    "    a = float(real_br[0]-real_tl[0])/480*1.0000001\n",
    "    b = float(real_br[1]-real_tl[1])/640*1.0000001\n",
    "    grid_real_h = torch.arange(int(real_tl[0]) ,int(real_br[0]) , a, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_real_w = torch.arange(int(real_tl[1]) ,int(real_br[1]) , b, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "\n",
    "    # Grid for upsampled ren\n",
    "    a = float(ren_br[0]-ren_tl[0])/480*1.0000001\n",
    "    b = float(ren_br[1]-ren_tl[1])/640*1.0000001\n",
    "    c = 0\n",
    "    \n",
    "    grid_ren_h = torch.arange(int(ren_tl[0]) ,int(ren_br[0]) , a, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_ren_w = torch.arange(int(ren_tl[1]) ,int(ren_br[1]) , b, device=u_map.device)[None,:].repeat(480,1)\n",
    "    # Calculate valid depth map for rendered image\n",
    "    render_d_ind_h = torch.arange(0 ,480 , 1, device=u_map.device)[:,None].repeat(1,640)\n",
    "    render_d_ind_w= torch.arange(0 ,640 , 1, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "    render_d_ind_h = torch.clamp(torch.round((render_d_ind_h - u_map).type(torch.float32)) ,0,479).type( torch.long )[flow_mask]\n",
    "    render_d_ind_w = torch.clamp(torch.round((render_d_ind_w - v_map).type(torch.float32)),0,639).type( torch.long )[flow_mask] \n",
    "    index = render_d_ind_h*640 + render_d_ind_w # hacky indexing along two dimensions\n",
    "    ren_d_masked  = render_d.flatten()[index]\n",
    "    \n",
    "    # Project depth map to the pointcloud real\n",
    "    cam_scale = 10000\n",
    "\n",
    "    anal_tensor( flow_mask, 'flow_mask')\n",
    "    anal_tensor( grid_real_w, 'grid_real_w')\n",
    "    anal_tensor( grid_real_h, 'grid_real_h')\n",
    "    anal_tensor( u_map, 'u_map')\n",
    "    \n",
    "    real_pixels = torch.stack( [grid_real_w[flow_mask], grid_real_h[flow_mask], torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)[flow_mask]], dim=1 ).type(u_map.dtype)\n",
    "    K_inv = torch.inverse(K_real.type(torch.float32)).type(u_map.dtype)\n",
    "    P_real = K_inv @ real_pixels.T\n",
    "    P_real = P_real * real_d[flow_mask] / cam_scale\n",
    "    P_real = P_real.T\n",
    "    \n",
    "    \n",
    "    # Project depth map to the pointcloud render\n",
    "    K_ren_inv = torch.inverse(K_ren.type(torch.float32)).type(u_map.dtype)\n",
    "    \n",
    "    ren_pixels = torch.stack( [ grid_ren_w, \n",
    "                            grid_ren_h,\n",
    "                            torch.ones(grid_ren_h.shape)], \n",
    "                        dim=2 )\n",
    "    ren_pixels = ren_pixels.view(-1,3)\n",
    "    P_ren_new = K_ren_inv.type(u_map.dtype) @ ren_pixels.T.type(u_map.dtype)\n",
    "    P_ren_new = (P_ren_new * render_d.view(1,-1).type(u_map.dtype) / cam_scale).T\n",
    "    P_ren_new = P_ren_new [index]\n",
    "\n",
    "#     ren_pixels = torch.stack( [grid_ren_w[flow_mask] - v_map[flow_mask], \n",
    "#                             grid_ren_h[flow_mask] - u_map[flow_mask],\n",
    "#                             torch.ones(grid_ren_h.shape, device = u_map.device,  dtype= u_map.dtype )[flow_mask]], \n",
    "#                             dim=1 ).type(u_map.dtype)\n",
    "#     P_ren = K_ren_inv @ ren_pixels.T\n",
    "#     P_ren = P_ren * ren_d_masked / cam_scale\n",
    "#     P_ren = P_ren.T\n",
    "\n",
    "    P_ren = P_ren_new\n",
    "    \n",
    "    # Filter the pointclouds given the depthmap\n",
    "    m_ren_depth = filter_pcd_given_depthmap(P_ren, ren_d_masked )\n",
    "    m_real_depth = filter_pcd_given_depthmap(P_real, real_d[flow_mask])\n",
    "    m_total =  m_ren_depth * m_real_depth\n",
    "    min_points = 20\n",
    "    if torch.sum(m_total) < min_points:\n",
    "        print('Violated filter pcd_given_depthmap min points constrain in flow_to_trafo')\n",
    "        return P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "    # P_ren = P_ren[m_total] \n",
    "    # P_real = P_real[m_total]\n",
    "    # print('Number of points before after depth filter', P_ren.shape)\n",
    "  \n",
    "    # Do not transfrom to center coordinate system\n",
    "    P_real_in_center = P_real                      \n",
    "    P_ren_in_center = P_ren \n",
    "    #print(\"real\")\n",
    "    m_real = filter_pcd( P_real_in_center )\n",
    "    #print(\"render\")\n",
    "    m_ren = filter_pcd( P_ren_in_center )\n",
    "    #print(\"M Real left overs:\", torch.sum( m_real[:,0]), m_real.shape )\n",
    "    #print(\"M Ren left overs:\", torch.sum( m_ren[:,0]), m_ren.shape )\n",
    "    \n",
    "    m_tot = m_real * m_ren\n",
    "    if torch.sum(m_tot) < min_points:\n",
    "        print('Violated filter pcd min points constrain in flow_to_trafo')\n",
    "        return P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[m_tot[:,0]]\n",
    "    P_ren_in_center = P_ren_in_center[m_tot[:,0]]\n",
    "    #print(\"M Real left overs applied shape:\", P_real_in_center.shape )\n",
    "    #print(\"M Ren left overs  applied shape:\", P_ren_in_center.shape )\n",
    "    \n",
    "    # anal_tensor(  P_real_in_center, 'P_real_in_center m_tot masked')\n",
    "\n",
    "    # Max mean deviation\n",
    "    m_new = filter_pcd_cor(P_real_in_center, P_ren_in_center)\n",
    "    \n",
    "    if torch.sum(m_new) < min_points:\n",
    "        print('Violated filter pcd_cor min points constrain in flow_to_trafo')\n",
    "        return P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[m_new] - h_real[:3,3]\n",
    "    P_ren_in_center = P_ren_in_center[m_new] - h_render[:3,3]\n",
    "    \n",
    "    # Get transformation#\n",
    "    #print('Number of points final', P_real_in_center.shape)\n",
    "    pts_trafo = min( P_real_in_center.shape[0], 1000 )\n",
    "    idx = torch.randperm( P_real_in_center.shape[0] )[0:pts_trafo]\n",
    "    if idx.shape[0] < 10:\n",
    "        print('EEEEEEEEEEEEEEEEEEEERRRRRRRRRRRRRor')\n",
    "        return P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[idx]\n",
    "    P_ren_in_center = P_ren_in_center[idx]\n",
    "    \n",
    "    \n",
    "    T_res_ken = solve_transform( P_real_in_center[None].type(torch.float64 ) , P_ren_in_center.type(torch.float64 ) ).type(u_map.dtype )\n",
    "    P_ren_ken = get_H( P_ren_in_center )\n",
    "    P_ren_ken = (P_ren_ken @ T_res_ken[0].T)[:,:3]\n",
    "    \n",
    "    \n",
    "    P_ren_jonas = get_H( P_ren_in_center )\n",
    "    T_res_tot = torch.eye(4)\n",
    "    P_ren_total = get_H( P_ren_in_center )\n",
    "    \n",
    "    for i in range (0,10):\n",
    "        T_res_jonas = solve_transform2( P_ren_jonas[:,:3], P_real_in_center).type(u_map.dtype)[None,:,:]\n",
    "        P_ren_jonas = (P_ren_jonas @ T_res_jonas[0].T)\n",
    "        \n",
    "        T_res_tot = T_res_tot @ T_res_jonas[0]\n",
    "        \n",
    "        #print(f'New {i}', torch.sum( torch.norm( P_real_in_center-P_ren_jonas[:,:3], dim=1 )))\n",
    "\n",
    "    \n",
    "    P_ren_total = (P_ren_total @ T_res_tot.T)[:,:3]\n",
    "    \n",
    "    #print(f'KEN ', torch.sum( torch.norm( P_real_in_center-P_ren_ken, dim=1 )))\n",
    "    #print(f'Total ', torch.sum( torch.norm( P_real_in_center- P_ren_total, dim=1 )))\n",
    "    \n",
    "    T_res = T_res_ken\n",
    "    \n",
    "    # Transform the real points according to calculated transformation\n",
    "    P_hr = torch.ones( (P_real_in_center.shape[0],4 ) , device=u_map.device, dtype= u_map.dtype)\n",
    "    P_hr[:,:3] = P_real_in_center\n",
    "    P_real_trafo = (torch.inverse( T_res[0].type(torch.float32) ).type(u_map.dtype ) @ copy.deepcopy(P_hr).T).T [:,:3]\n",
    "\n",
    "    mask_close_to_camera = P_real_in_center[:,2] > 0.01\n",
    "    #print(\"SUM\", torch.sum(mask_close_to_camera) )\n",
    "    #print(\"shape\", mask_close_to_camera.shape)\n",
    "    \n",
    "\n",
    "    P_real_in_center = P_real_in_center[mask_close_to_camera]\n",
    "    P_ren_in_center = P_ren_in_center[mask_close_to_camera]\n",
    "    P_real_trafo = P_real_trafo[mask_close_to_camera]\n",
    "    #print(\"shape P_real_trafo\", P_real_trafo.shape)\n",
    "    \n",
    "    sub = max(1,int( P_real_in_center.shape[0]/50 ) )\n",
    "    # plot_two_pcd_line(P_real_in_center[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "    # plot_two_pcd_line(P_real_trafo[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "    \n",
    "    \n",
    "    return P_real_in_center, P_ren_in_center, P_real_trafo, T_res[0]\n",
    "#print('Okay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P_real_in_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-36795852f339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mP_real_in_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_ren_in_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_real_trafo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_T_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mT_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve_transform\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mP_real_trafo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mP_ren_in_center\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# P_hr = torch.ones( (P_real_in_center.shape[0],4 ) , device=\"cpu\", dtype= torch.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'P_real_in_center' is not defined"
     ]
    }
   ],
   "source": [
    "P_real_in_center, P_ren_in_center, P_real_trafo, gt_T_res\n",
    "\n",
    "T_res = solve_transform( P_real_trafo[None].type(torch.float64 ) , P_ren_in_center.type(torch.float64 ) ).type(torch.float32)\n",
    "\n",
    "# P_hr = torch.ones( (P_real_in_center.shape[0],4 ) , device=\"cpu\", dtype= torch.float32)\n",
    "# P_hr[:,:3] = P_real_in_center\n",
    "# P_real_trafo2 = (torch.inverse( T_res[0].type(torch.float32) ).type(torch.float32 ) @ copy.deepcopy(P_hr).T).T [:,:3]\n",
    "    \n",
    "    \n",
    "# plot_two_pcd_line(P_real_trafo[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "# plot_two_pcd_line(P_real_trafo2[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Processed 0/5\n",
      "Iteration 0, GT 0.37210410833358765 ERRODED 0.26329484581947327\n",
      "torch.Size([0, 3])\n",
      "number of point with a distance greater 0.2 0 \n",
      "number of point with a distance greater 0.02 0 \n",
      "number of point with a distance smaller 0.02 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonfrey/miniconda3/envs/track_latest/lib/python3.7/site-packages/ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, GT 0.33457842469215393 ERRODED 0.30314794182777405\n",
      "torch.Size([2, 3])\n",
      "number of point with a distance greater 0.2 0 \n",
      "number of point with a distance greater 0.02 2 \n",
      "number of point with a distance smaller 0.02 0 \n",
      "Iteration 2, GT 0.2000366598367691 ERRODED 0.19722352921962738\n",
      "torch.Size([3, 3])\n",
      "number of point with a distance greater 0.2 0 \n",
      "number of point with a distance greater 0.02 3 \n",
      "number of point with a distance smaller 0.02 0 \n",
      "Iteration 3, GT 0.2473229169845581 ERRODED 0.21554017066955566\n",
      "torch.Size([24, 3])\n",
      "number of point with a distance greater 0.2 0 \n",
      "number of point with a distance greater 0.02 4 \n",
      "number of point with a distance smaller 0.02 20 \n",
      "Iteration 4, GT 0.29877397418022156 ERRODED 0.31761690974235535\n",
      "torch.Size([438, 3])\n",
      "number of point with a distance greater 0.2 0 \n",
      "number of point with a distance greater 0.02 438 \n",
      "number of point with a distance smaller 0.02 0 \n",
      "Result, GT Avg 0.29056321680545805 Errode Avg 0.25936467945575714\n"
     ]
    }
   ],
   "source": [
    "# Robustness analysis: \n",
    "# Load Sample\n",
    "# Predict Pose\n",
    "# Calculate ADD-S\n",
    "# Store array containing UniqueDesig + ADD-S Value\n",
    "\n",
    "# Try out what happens if flow dilatation is used. \n",
    "# Maybe the normal filtering aswell. \n",
    "# Check if there is some higher error for other objects\n",
    "from visu import plot_two_pcd\n",
    "device = 'cpu'\n",
    "\n",
    "desig_ls= []\n",
    "adds_gt_ls = []\n",
    "adds_init_ls = []\n",
    "adds_erode_ls = []\n",
    "adds_noise1_ls = []\n",
    "adds_noise2_ls = []\n",
    "visualizer = Visualizer('/home/jonfrey/Debug', None)\n",
    "print('START')\n",
    "\n",
    "\n",
    "K_ren = torch.tensor( dataset_train._backend.get_camera('data_syn/0019', K=True), device=device ) \n",
    "max_iter = 5\n",
    "import itertools\n",
    "\n",
    "if idxmax != -1:\n",
    "    print(idxmax, \"is not equal to -1 load batch\")\n",
    "    batch = next(itertools.islice(dataloader_train, idxmax, None))[0]\n",
    "    # raise Exception\n",
    "\n",
    "for j,ba in enumerate(dataloader_train):\n",
    "    #if j != 7:\n",
    "    #    continue\n",
    "\n",
    "    if j > max_iter-1:\n",
    "        break\n",
    "        \n",
    "    if j % 10 == 0:\n",
    "        print(f\"Processed {j}/{max_iter}\")\n",
    "#     if j != idxmax:\n",
    "#         continue \n",
    "    if idxmax == -1:  \n",
    "        batch = ba[0] #bann 10450   \n",
    "    if idxmax != -1 and j > 1:\n",
    "        break\n",
    "        \n",
    "    model_points = batch[4]\n",
    "    idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "    label = batch[7]\n",
    "    real_img_original = batch[8]\n",
    "    cam = batch[9]\n",
    "    gt_rot_wxyz, gt_trans, unique_desig = batch[10:13] # unique_desig[1] contains the idx starting at 1 for the first object \n",
    "\n",
    "    log_scalars = {}\n",
    "    bs = model_points.shape[0]\n",
    "\n",
    "    # check if skip\n",
    "    if batch[13] is False:\n",
    "        print('Continue')\n",
    "        continue\n",
    "\n",
    "    real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "    pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "    u_map, v_map, flow_mask, bb = batch[24:]\n",
    "    data = torch.cat([real_img, render_img], dim=1)\n",
    "\n",
    "    ero_in = (gt_label_cropped ==  unique_desig[1])[:,None,:,:].type(torch.float32) # BS,C,H,W\n",
    "    def get_scale_for_erosion(ero_in):\n",
    "        res = torch.sum ( ero_in, dim = (2,3))\n",
    "        res[res < 5000] = 5\n",
    "        res[res < 10000] = 10\n",
    "        res[res < 30000] = 20\n",
    "        res[res < 40000] = 25\n",
    "        res[res < 50000] = 30\n",
    "        res[res >= 50000] = 40\n",
    "        return res\n",
    "\n",
    "    t_size = get_scale_for_erosion(ero_in)\n",
    "    ero_out = eroision_batch(ero_in,t_size)\n",
    "\n",
    "    uv_gt = torch.stack( [u_map, v_map], dim=3 ).permute(0,3,1,2)\n",
    "    real_tl, real_br, ren_tl, ren_br = bb \n",
    "\n",
    "    b = 0\n",
    "    K_real = torch.tensor( [[cam[b,2],0,cam[b,0]],[b,cam[b,3],cam[b,1]],[0,0,1]], device=device )\n",
    "\n",
    "    h_real_est = torch.eye(4,device=device)\n",
    "    h_real_est[:3,:3] = quat_to_rot(pred_rot_wxyz[b][None,:], conv='wxyz', device=device)\n",
    "    h_real_est[:3,3] = torch.tensor( pred_trans[b].clone().detach() ,device=device )\n",
    "\n",
    "    ### GT Estimate ###\n",
    "    typ = u_map.dtype\n",
    "    indx = gt_label_cropped[0] == unique_desig[1] \n",
    "    fmt = flow_mask.dtype\n",
    "    #print(f\"--- Bounding Box Rendered: {ren_tl, ren_br}\")\n",
    "    #print (\"-----------gt-------------\")\n",
    "    #print(f\"FLOW MASK ERODE, {flow_mask.shape}\")\n",
    "    P_real_in_center, P_ren_in_center, P_real_trafo, gt_T_res = flow_to_trafo(\n",
    "        real_br = real_br[b],\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask[b]), \n",
    "        u_map = copy.deepcopy(u_map[b].type( typ )),\n",
    "        v_map = copy.deepcopy(v_map[b].type( typ )), \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    gt_h_est =  gt_T_res @ h_render[0].type(typ)\n",
    "\n",
    "    ### Noise1 ###\n",
    "    noise1 = 5\n",
    "    noi = np.random.uniform(-noise1,noise1,u_map[b].shape)\n",
    "    noi = torch.tensor( noi )\n",
    "    noi_ = np.random.uniform(-noise1,noise1,u_map[b].shape)\n",
    "    noi_ = torch.tensor( noi_ )\n",
    "    #print (\"-----------noise1-------------\")\n",
    "    _,_,_, noise1_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask[b]), \n",
    "        u_map = copy.deepcopy((u_map[b]+noi).type( typ )), \n",
    "        v_map = copy.deepcopy((v_map[b]+noi_).type( typ )), \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "\n",
    "    noise1_h_est =  noise1_T_res @ h_render[0].type(typ)\n",
    "\n",
    "    ### Noise2 ###\n",
    "    noise2 = 10\n",
    "    val1 = np.random.uniform(-noise2,noise2,(1))[0]\n",
    "    noi = np.zeros( u_map[b].shape )\n",
    "    noi[:,:] = val1\n",
    "    noi = torch.tensor( noi )\n",
    "\n",
    "    val1 = np.random.uniform(-noise2,noise2,(1))[0]\n",
    "    noi_ = np.zeros( u_map[b].shape )\n",
    "    noi_[:,:] = val1\n",
    "    noi_ = torch.tensor( noi_ )\n",
    "    #print (f\"-----------noise2--------{u_map[b].shape}-----\")\n",
    "    _,_,_, noise2_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask[b]), \n",
    "        u_map = copy.deepcopy((u_map[b]+noi).type( typ )), \n",
    "        v_map = copy.deepcopy((v_map[b]+noi_).type( typ )), \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    noise2_h_est =  noise2_T_res @ h_render[0].type(typ)\n",
    "\n",
    "\n",
    "    ### Erode Label ###\n",
    "    #print (\"-----------Eroded-------------\")\n",
    "    flow_mask  = (flow_mask * ero_out.type(torch.float32)).type(fmt)[:,0]\n",
    "    #print(f\"FLOW MASK ERODE, {flow_mask.shape}\")\n",
    "    _,_,_, ero_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask[b]), \n",
    "        u_map = copy.deepcopy((u_map[b]).type( typ )), \n",
    "        v_map = copy.deepcopy((v_map[b]).type( typ )), \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    ero_h_est =  ero_T_res @ h_render[0].type(typ) # set rotation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mask = (flow_mask == True)\n",
    "\n",
    "    # Target Model-points Transformed\n",
    "    p = model_points.shape[1]\n",
    "    target = torch.bmm( model_points, torch.transpose(h_real[:,:3,:3], 1,2 ) ) + h_real[:,:3,3][:,None,:].repeat(1,p,1)\n",
    "\n",
    "    # Compute ADD-S\n",
    "    adds_res_gt_flow = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = gt_h_est[None].type( target.dtype) )\n",
    "    adds_res_gt_flow_eroded = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = ero_h_est[None].type( target.dtype) )\n",
    "    adds_res_noise1 = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = noise1_h_est[None].type( target.dtype) )\n",
    "    adds_res_noise2 = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = noise2_h_est[None].type( target.dtype) )\n",
    "    adds_init = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = h_real_est[None].type( target.dtype))\n",
    "\n",
    "    #print(f'Iteration {j}, GT', float(adds_res_gt_flow), 'ERRO', float( adds_res_gt_flow_eroded))\n",
    "    #print(f'Iteration {j}, No1' ,float(adds_res_noise1), 'No2', float(adds_res_noise2), 'Init', float(adds_init))\n",
    "    desig_ls.append(unique_desig)\n",
    "    adds_gt_ls.append(float(adds_res_gt_flow.detach()))\n",
    "    adds_erode_ls.append(float(adds_res_gt_flow_eroded.detach()))\n",
    "    adds_init_ls.append(float(adds_init.detach()))\n",
    "    adds_noise1_ls.append(float(adds_res_noise1.detach()))\n",
    "    adds_noise2_ls.append(float(adds_res_noise2.detach()))\n",
    "\n",
    "    plot1 = False\n",
    "    if adds_erode_ls[-1] > 0.02 :\n",
    "        print(f'Iteration {j}, GT', adds_gt_ls[-1], 'ERRODED', float( adds_res_gt_flow_eroded)   )\n",
    "        #plot_two_pcd_line(P_real_trafo[::sub], P_ren_in_center[::sub] )\n",
    "        no = torch.norm(P_real_trafo- P_ren_in_center, dim=1)\n",
    "        print( P_real_trafo.shape )\n",
    "        print( f'number of point with a distance greater 0.2 {torch.sum( no>0.2 )} ' )\n",
    "        print( f'number of point with a distance greater 0.02 {torch.sum( no>0.02 )} ' )\n",
    "        print( f'number of point with a distance smaller 0.02 {torch.sum( no<0.02 )} ' )\n",
    "        \n",
    "        \n",
    "        plot1 = False\n",
    "        \n",
    "        \n",
    "    if idxmax != -1:\n",
    "        plot1 = True\n",
    "        \n",
    "    if plot1:\n",
    "        print(\"Real depth map cropped\")\n",
    "        real_depth_img = Drawer().disp_img_1d(real_d[b].numpy(),ret=True)\n",
    "        real_depth_img = np.repeat( real_depth_img[:,:,None],3, axis=2)\n",
    "        print(\"Render depth map cropped\")\n",
    "        render_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "        render_depth_img = np.repeat( render_depth_img[:,:,None],3, axis=2)\n",
    "        Drawer().disp_img_1d(flow_mask[0],ret=True)\n",
    "\n",
    "\n",
    "        sub = max(1,int( P_real_in_center.shape[0]/100 ) )\n",
    "        plot_two_pcd_line(P_real_in_center[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "        plot_two_pcd_line(P_real_trafo[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "\n",
    "        print(f\"Real Image, Estimated Points given GT Flow {P_real_in_center.shape}\")\n",
    "        visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "                        epoch = 1,\n",
    "                        img= real_img_original[b].cpu().numpy(),\n",
    "                        points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "                        store = False,\n",
    "                        jupyter=True,\n",
    "                        K = K_real.cpu().numpy(),\n",
    "                        H = np.eye(4),\n",
    "                        method='def')\n",
    "        print(\"Real Image Cropped, Estimated Points given GT Flow\")\n",
    "        visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "                        epoch = 1,\n",
    "                        img= real_depth_img,\n",
    "                        tl = real_tl[0],\n",
    "                        br = real_br[0],\n",
    "                        points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "                        store = False,\n",
    "                        jupyter=True,\n",
    "                        K = K_real.cpu().numpy(),\n",
    "                        H = np.eye(4),\n",
    "                        method='def')\n",
    "\n",
    "        fil = label == unique_desig[1]    \n",
    "        real_img_original[b][ fil[0][:,:,None].repeat(1,1,3) ] = 255\n",
    "        print(\"Real Image, Estimated Points given GT Flow with label is white\")\n",
    "        visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "                        epoch = 1,\n",
    "                        img= real_img_original[b].cpu().numpy(),\n",
    "                        points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "                        store = False,\n",
    "                        jupyter=True,\n",
    "                        K = K_real.cpu().numpy(),\n",
    "                        H = np.eye(4),\n",
    "                        method='def')\n",
    "\n",
    "        print(\"Render Image, Rendered Points not transformed reprojected\")\n",
    "        visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= render_img_original[b].cpu().numpy(),\n",
    "                            points = copy.deepcopy(P_ren_in_center.cpu().numpy()),\n",
    "                            store = False,\n",
    "                            jupyter=True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = np.eye(4),\n",
    "                            method='def')\n",
    "        print(\"Render Image Cropped, Rendered Points not transformed reprojected\")\n",
    "        visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= render_depth_img,\n",
    "                            points = copy.deepcopy(P_ren_in_center.cpu().numpy()),\n",
    "                            tl = ren_tl[0],\n",
    "                            br = ren_br[0],\n",
    "                            store = False,\n",
    "                            jupyter=True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = np.eye(4),\n",
    "                            method='def')\n",
    "        print(\"Corrospondence\")\n",
    "\n",
    "        print(real_img.shape, render_img.shape,flow_mask.shape, u_map.shape)\n",
    "        visualizer.plot_corrospondence(tag=f'_',\n",
    "                                           epoch=0,\n",
    "                                            u_map=u_map[0], \n",
    "                                            v_map=v_map[0], \n",
    "                                            flow_mask=flow_mask[0], \n",
    "                                            real_img=real_img[0], \n",
    "                                            render_img=render_img[0],\n",
    "                                            store=False,\n",
    "                                            jupyter=True,\n",
    "                                            coloful=True,\n",
    "                                            method='def',\n",
    "                                            res_h=1,\n",
    "                                            res_w=1)\n",
    "\n",
    "        \n",
    "print(f'Result, GT Avg', sum(adds_gt_ls)/len(adds_gt_ls), 'Errode Avg', sum(adds_erode_ls)/len(adds_erode_ls) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "data = np.array( [adds_gt_ls,adds_init_ls,adds_erode_ls,adds_noise1_ls,adds_noise2_ls] ).T\n",
    "df = pandas.DataFrame( data, columns = ['ADD-S GT','ADD-S INITAL','ADD-S Eroded Mask','ADD-S Noise All Random','ADD-S Noise Same Random'] )\n",
    "print(df)\n",
    "print(\"MEAN\", df.mean() )\n",
    "print(\"STD\", df.std() )\n",
    "print(\"MAX\", df.max(), df.idxmax() )\n",
    "idxmax = df.idxmax()[0]\n",
    "print(idxmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax = -1\n",
    "#batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_comp = np.concatenate( [real_img.cpu().numpy(), render_img.cpu().numpy() ], axis=1).astype(np.uint8)\n",
    "cropped_comp_img = Image.fromarray(cropped_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "def plot(mask):\n",
    "    min_val = torch.min( mask )\n",
    "    max_val = float( max(1,torch.max( mask )) )\n",
    "    mask = torch.clamp( (mask-min_val) / (max_val-min_val)*255 ,0,255)\n",
    "    \n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth[:,:,3] = 255\n",
    "#     print(val.shape)\n",
    "    for i in range(480):\n",
    "        for j in range(640):\n",
    "#             print(int( val[i,j]),  cmaplist[ int(val[i,j])])\n",
    "            data_depth[i,j,:4] = np.array( cmaplist[ int(mask[i,j])] )*255\n",
    "    data_depth[:,:,3] = 255\n",
    "    data_depth[:,:,3][label==2] = 255\n",
    "    img_depth = Image.fromarray(data_depth, 'RGBA')\n",
    "    display(img_depth)\n",
    "    \n",
    "def plot_depth(depth):\n",
    "    min_val = torch.min( depth[depth!=0] )\n",
    "    max_val = float( max(1,torch.max( depth )) )\n",
    "    mask = torch.clamp( (depth-min_val) / (max_val-min_val)*255 ,0,255)\n",
    "    \n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth[:,:,3] = 255\n",
    "#     print(val.shape)\n",
    "    for i in range(480):\n",
    "        for j in range(640):\n",
    "#             print(int( val[i,j]),  cmaplist[ int(val[i,j])])\n",
    "            data_depth[i,j,:4] = np.array( cmaplist[ int(mask[i,j])] )*255\n",
    "    data_depth[:,:,3] = 255\n",
    "    data_depth[:,:,3][label==2] = 255\n",
    "    \n",
    "    img_depth = Image.fromarray(data_depth, 'RGBA')\n",
    "    \n",
    "    display(img_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import BoundingBox\n",
    "\n",
    "bb = BoundingBox(p1 = torch.tensor([100,100], dtype=torch.float32),p2 = torch.tensor([490,670], dtype=torch.float32))\n",
    "img = torch.tensor(render_depth_img).type(torch.float32)\n",
    "#crop_ren = bb.crop(img).unsqueeze(0)\n",
    "#crop_ren.shape\n",
    "\n",
    "def crop(img, tl,br, max_height=480, max_width=640, scale=False, mode='nearest'):\n",
    "\n",
    "    h = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "\n",
    "    img_pad = torch.zeros((int(h + 2*max_height), int(w + 2*max_width), img.shape[2]))\n",
    "    \n",
    "    img_pad[max_height:max_height+h,max_width:max_width+w] = img\n",
    "    res = img_pad[ int(max_height+tl[0]) : int(max_height+br[0]), int(max_width+tl[1]) : int(max_width+br[1])] # H W C\n",
    "    if scale:\n",
    "        res = res.permute(2,0,1)[None] #BS C H W\n",
    "        res  = torch.nn.functional.interpolate(res, size=(480,640), mode=mode)  \n",
    "        res = res[0].permute(1,2,0) # H W C\n",
    "        \n",
    "    return res\n",
    " \n",
    "\n",
    "res = crop(img, bb.tl, bb.br, scale = True, mode='nearest')\n",
    "\n",
    "def plot_torch(t):\n",
    "    if t.shape[0] == 3:\n",
    "        t = t.permute(1,2,0)\n",
    "    t = t/float(torch.max(t)/255)\n",
    "    \n",
    "    t = np.uint8(t.numpy())\n",
    "    display( Image.fromarray(t ) )\n",
    "print(res.shape)\n",
    "plot_torch(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def solve_transform(keypoints, gt_keypoints):\n",
    "    \"\"\"\n",
    "    keypoints: N x K x 3\n",
    "    gt_keypoints: K x 3\n",
    "    return: N x 4 x 4 transformation matrix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keypoints = keypoints.clone()\n",
    "        gt_keypoints = gt_keypoints.clone()\n",
    "        N, K, _ = keypoints.shape\n",
    "        center = keypoints.mean(dim=1)\n",
    "        gt_center = gt_keypoints.mean(dim=0)\n",
    "        keypoints -= center[:, None, :]\n",
    "        gt_keypoints -= gt_center[None]\n",
    "        matrix = keypoints.transpose(2, 1) @ gt_keypoints[None]\n",
    "        U, S, V = torch.svd(matrix)\n",
    "        Vt = V.transpose(2, 1)\n",
    "        Ut = U.transpose(2, 1)\n",
    "\n",
    "        d = (V @ Ut).det()\n",
    "        I = torch.eye(3, 3, dtype=gt_center.dtype, device= keypoints.device)[None].repeat(N, 1, 1)\n",
    "        I[:, 2, 2] = d.clone()\n",
    "\n",
    "        R = U @ I @ Vt\n",
    "        T = torch.zeros(N, 4, 4, dtype=gt_center.dtype, device= keypoints.device)\n",
    "        T[:, 0:3, 0:3] = R\n",
    "        T[:, 0:3, 3] = center[None] - (R @ gt_center[None :, None])[:, :, 0]\n",
    "        T[:, 3, 3] = 1.0\n",
    "\n",
    "        return T\n",
    "    except RuntimeError as error:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        print(\"Something went wrong\")\n",
    "\n",
    "        # costume implementation \n",
    "def solve_transform2(A,B):\n",
    "    if A.shape[0] > B.shape[0]:\n",
    "        x=torch.arange(A.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:B.shape[0]]\n",
    "        A = torch.index_select(A, 0, out)\n",
    "    if A.shape[0] < B.shape[0]:\n",
    "        x=torch.arange(B.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:A.shape[0]]\n",
    "        B = torch.index_select(B, 0, out)\n",
    "\n",
    "    #A = torch.choice(A,B.shape[0])\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    m = A.shape[1]\n",
    "    centroid_A = torch.mean(A, dim=0)\n",
    "    centroid_B = torch.mean(B, dim=0)\n",
    "\n",
    "    AA = (A - centroid_A)\n",
    "    BB = (B - centroid_B)\n",
    "    H = AA.transpose(0,1) @ BB\n",
    "    U, S, Vt = torch.svd(H)\n",
    "    R = Vt @ U.transpose(0,1)\n",
    "    if torch.det(R) < 0:\n",
    "        Vt[m-1,:] *= -1\n",
    "        R = Vt.transpose(0,1) @ U.transpose(0,1)\n",
    "\n",
    "    # translation\n",
    "    t = centroid_B - (R @ centroid_A)\n",
    "    # homogeneous transformation\n",
    "    T = torch.eye(m+1, device= A.device)\n",
    "    T[:m, :m] = R\n",
    "    T[:m, m] = t\n",
    "    return T\n",
    "\n",
    "NR = 1000\n",
    "DIM = 3\n",
    "A = torch.rand( (NR,DIM), dtype= torch.float32)\n",
    "B = torch.rand( (NR,DIM), dtype= torch.float32)\n",
    "T = solve_transform2(A,B)\n",
    "T_ken = solve_transform(B[None,:,:],A)[0]\n",
    "print('Jonas\\n', T, '\\n \\nKens Impl \\n',T_ken)\n",
    "def get_H(pcd):\n",
    "    pcd_ret = torch.ones( (pcd.shape[0],pcd.shape[1]+1),device=pcd.device, dtype=pcd.dtype )\n",
    "    pcd_ret[:,:3] = pcd\n",
    "    return pcd_ret\n",
    "\n",
    "# A_hom = get_H(A)\n",
    "# A_hom2 = A_hom @ T.T\n",
    "# print(A_hom2[:,:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_latest",
   "language": "python",
   "name": "track_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
