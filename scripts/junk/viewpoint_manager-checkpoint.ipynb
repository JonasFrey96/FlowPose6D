{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('/home/jonfrey/PLR')\n",
    "sys.path.append('src')\n",
    "sys.path.append('src/dense_fusion')\n",
    "\n",
    "from loaders_v2 import Backend, ConfigLoader, GenericDataset\n",
    "from PIL import Image\n",
    "import copy\n",
    "from helper import re_quat\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "exp_cfg = ConfigLoader().from_file(\n",
    "    '/home/jonfrey/PLR/src/loaders_v2/test/dataset_cfgs.yml')\n",
    "env_cfg = ConfigLoader().from_file(\n",
    "    '/home/jonfrey/PLR/src/loaders_v2/test/env_ws.yml')\n",
    "\n",
    "generic = GenericDataset(\n",
    "    cfg_d=exp_cfg['d_ycb'],\n",
    "    cfg_env=env_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "obj_names = list(generic._backend._name_to_idx.keys())\n",
    "\n",
    "store = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/viewpoints_renderings'\n",
    "img_dict = {}\n",
    "\n",
    "\n",
    "def plt_img(img):\n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(1, 1, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot_vec(R):\n",
    "    x = R[:,2,1]-R[:,1,2]\n",
    "    y = R[:,0,2]-R[:,2,0]\n",
    "    z = R[:,1,0]-R[:,0,1]\n",
    "    \n",
    "    r = torch.norm( torch.stack( [x,y,z],dim = 1 ))\n",
    "    t = R[:,0,0] + R[:,1,1] + R[:,2,2]\n",
    "    phi = torch.atan2(r,t-1)\n",
    "    return phi\n",
    "\n",
    "from scipy.stats import special_ortho_group\n",
    "\n",
    "mat = np.array( special_ortho_group.rvs(dim = 3, size = 10) )\n",
    "Rin = torch.from_numpy(mat).type(torch.float32).cuda()\n",
    "\n",
    "q =  get_rot_vec(Rin)\n",
    "\n",
    "print(q)\n",
    "print(R.from_matrix(mat).as_rotvec()-q.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "import copy\n",
    "R2 = R.from_euler('xyz',[0.3,0.6,0], degrees=False).as_matrix()\n",
    "\n",
    "\n",
    "    \n",
    "class Quat():\n",
    "    def __init__(self,matrix, device):\n",
    "        \"\"\"\n",
    "        matrix torch.Tensor Nx3x3\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        num_rotations = matrix.shape[0]\n",
    "        decision_matrix = torch.empty((num_rotations, 4), dtype=torch.float32, device=self.device)\n",
    "        decision_matrix[:, :3] = matrix.diagonal(dim1=1, dim2=2)\n",
    "        decision_matrix[:, -1] = decision_matrix[:, :3].sum(dim=1)\n",
    "        choices = decision_matrix.argmax(axis=1)\n",
    "\n",
    "        quat = torch.empty((num_rotations, 4), dtype=torch.float32, device=self.device)\n",
    "#         print(\"quat\",quat,choices)\n",
    "        ind = torch.nonzero(choices != 3)\n",
    "#         print('ind',ind)\n",
    "        i = choices[ind]\n",
    "        j = (i + 1) % 3\n",
    "        k = (j + 1) % 3\n",
    "\n",
    "        quat[ind, i] = 1 - decision_matrix[ind, -1] + 2 * matrix[ind, i, i]\n",
    "        quat[ind, j] = matrix[ind, j, i] + matrix[ind, i, j]\n",
    "        quat[ind, k] = matrix[ind, k, i] + matrix[ind, i, k]\n",
    "        quat[ind, 3] = matrix[ind, k, j] - matrix[ind, j, k]\n",
    "\n",
    "        ind = torch.nonzero(choices == 3)[0]\n",
    "        quat[ind, 0] = matrix[ind, 2, 1] - matrix[ind, 1, 2]\n",
    "        quat[ind, 1] = matrix[ind, 0, 2] - matrix[ind, 2, 0]\n",
    "        quat[ind, 2] = matrix[ind, 1, 0] - matrix[ind, 0, 1]\n",
    "        quat[ind, 3] = 1 + decision_matrix[ind, -1]\n",
    "        \n",
    "        print('norm', torch.norm(quat, dim=1)[:, None])\n",
    "        quat /= torch.norm(quat, dim=1)[:, None]\n",
    "        self._quat = quat\n",
    "#         print(quat)\n",
    "    def as_rotvec(self):\n",
    "        quat = copy.copy(self._quat)\n",
    "        # w > 0 to ensure 0 <= angle <= pi\n",
    "        quat[quat[:, 3] < 0] *= -1\n",
    "\n",
    "        angle = 2 * torch.atan2(torch.norm(quat[:, :3], dim=1), quat[:, 3])\n",
    "\n",
    "        small_angle = (angle <= 1e-3)\n",
    "        large_angle = ~small_angle\n",
    "\n",
    "        num_rotations = self._quat.shape[0]\n",
    "        scale = torch.empty(num_rotations, device=self.device)\n",
    "        scale[small_angle] = (2 + angle[small_angle] ** 2 / 12 +\n",
    "                              7 * angle[small_angle] ** 4 / 2880)\n",
    "        scale[large_angle] = (angle[large_angle] /\n",
    "                              torch.sin(angle[large_angle] / 2))\n",
    "\n",
    "        rotvec = scale[:, None] * quat[:, :3]\n",
    "\n",
    "        return rotvec\n",
    "\n",
    "from scipy.stats import special_ortho_group\n",
    "\n",
    "mat = np.array( special_ortho_group.rvs(dim = 3, size = 100) )\n",
    "Rin = torch.from_numpy(mat).type(torch.float32).cuda()\n",
    "\n",
    "q = Quat( Rin,device='cuda:0' )\n",
    "# print(q._quat)\n",
    "print(q.as_rotvec())\n",
    "# print(R.from_matrix(mat).as_rotvec())\n",
    "\n",
    "\n",
    "print(R.from_matrix(mat).as_rotvec()-q.as_rotvec().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from PIL import Image\n",
    "import pickle as pkl\n",
    "import cv2\n",
    "def angle(m1,m2):\n",
    "    return np.arccos ( (np.trace((m2.dot(m1.T))) -1)/2 )\n",
    "import torch\n",
    "\n",
    "from kornia.geometry.conversions import rotation_matrix_to_angle_axis\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "def angle_gen(mat, n_mat): \n",
    "    \"\"\"\n",
    "    mat target dim: 3X3 \n",
    "    n_mat dim: Nx3x3\n",
    "    \n",
    "    returns distance betweem the rotation matrixes dim: N\n",
    "    \"\"\"\n",
    "    dif = []\n",
    "    for i in range (n_mat.shape[0]):\n",
    "        if i == 0:\n",
    "            print( 'out', mat.dot(n_mat[i,:,:].T))\n",
    "        r, _ = cv2.Rodrigues(mat.dot(n_mat[i,:,:].T))\n",
    "        if i == 0:\n",
    "            print('r',r )\n",
    "        dif.append( np.linalg.norm(r))\n",
    "    \n",
    "    print( 'dif',dif[:10])\n",
    "    return np.array(dif)\n",
    "#     m = n_mat.dot(mat.T)\n",
    "#     arr = np.clip (  (m[:,0,0]+m[:,1,1]+m[:,2,2] - 1 )/2 , a_min= -1 ,a_max= 1 )\n",
    "#     return arr\n",
    "\n",
    "            \n",
    "def angle_batch_torch(mat, n_mat):   \n",
    "    \"\"\"\n",
    "    mat target dim: BSx3X3 \n",
    "    n_mat dim: BSxNx3x3\n",
    "    \n",
    "    return BSXN\n",
    "    \"\"\"\n",
    "    bs = mat.shape[0]  \n",
    "    out = torch.matmul(mat, torch.transpose(n_mat,2,3) ).view(-1,3,3)\n",
    "    tmp = R.from_matrix( out.cpu().numpy() )\n",
    "    \n",
    "    vectors = torch.from_numpy(tmp.as_rotvec()).cuda().view(-1,3)\n",
    "    vectors = torch.norm(vectors, dim=1).view(bs,-1,1)   \n",
    "    vectors = torch.abs(vectors)\n",
    "    idx_argmin = torch.argmin(vectors, dim=1)\n",
    "    return idx_argmin\n",
    "\n",
    "def angle_batch_torch_full(mat, n_mat):   \n",
    "    \"\"\"\n",
    "    mat target dim: BSx3X3 \n",
    "    n_mat dim: BSxNx3x3\n",
    "    \n",
    "    return BSXN\n",
    "    \"\"\"\n",
    "    bs = mat.shape[0]  \n",
    "    out = torch.matmul(mat, torch.transpose(n_mat,2,3) ).view(-1,3,3)\n",
    "    \n",
    "# #     tmp = R.from_matrix( out.cpu().numpy() )\n",
    "#     vectors1 =  Quat(out,device='cuda:0').as_rotvec() \n",
    "#     print('vectors1',vectors1.shape)\n",
    "    \n",
    "#     tmp = R.from_matrix( out.cpu().numpy() )\n",
    "# #     torch.from_numpy(tmp.as_rotvec()).cuda()\n",
    "\n",
    "#     vectors = torch.from_numpy(tmp.as_rotvec()).cuda().view(-1,3)\n",
    "#     print('vectors2',vectors.shape)\n",
    "#     a = torch.argmax(torch.abs(vectors1-vectors),dim=0)\n",
    "#     print(a)\n",
    "#     print(a,vectors1[a], vectors[a])\n",
    "    \n",
    "#     vectors = torch.norm(vectors, dim=1).view(bs,-1,1)   \n",
    "    \n",
    "    vectors = get_rot_vec(out).view(bs,-1,1)  \n",
    "    vectors = torch.abs(vectors)\n",
    "    idx_argmin = torch.argmin(vectors, dim=1)\n",
    "    return idx_argmin\n",
    "\n",
    "            \n",
    "class ViewpointManager():\n",
    "    \n",
    "    def __init__(self, store, name_to_idx):\n",
    "        self.store = store\n",
    "        self.name_to_idx = name_to_idx\n",
    "        self.idx_to_name = {}\n",
    "            \n",
    "        for key, value in self.name_to_idx.items():\n",
    "            self.idx_to_name[value] = key\n",
    "            \n",
    "        self._load()\n",
    "        \n",
    "    def _load(self):\n",
    "        self.img_dict = {}\n",
    "        self.pose_dict = {}\n",
    "        self.cam_dict = {}\n",
    "        self.depth_dict = {}\n",
    "        self.sim_dict = {}\n",
    "        \n",
    "        for obj in self.name_to_idx.keys():\n",
    "            idx = self.name_to_idx[obj]\n",
    "            self.pose_dict[idx] = torch.tensor( pkl.load( open( f'{self.store}/{obj}/pose.pkl', \"rb\" ) ) ).type(torch.float32).cuda()\n",
    "            self.cam_dict[idx] = torch.tensor( pkl.load( open( f'{self.store}/{obj}/cam.pkl', \"rb\" ) ) ).type(torch.float32).cuda()\n",
    "            \n",
    "    def get_closest_image(self, idx, mat):\n",
    "        \"\"\"\n",
    "        idx: start at 1 and goes to num_obj!\n",
    "        \"\"\"\n",
    "        st = time.time()\n",
    "        dif = angle_gen(mat, self.pose_dict[idx][:,:3,:3].cpu().numpy() )\n",
    "        idx_argmin = np.argmin(np.abs(dif))\n",
    "        \n",
    "        print('single image idx', idx_argmin, 'value', dif[idx_argmin] )\n",
    "        st = time.time()\n",
    "        obj = self.idx_to_name[idx]\n",
    "       \n",
    "        st = time.time()\n",
    "        img = Image.open(f'{self.store}/{obj}/{idx_argmin}-color.png')\n",
    "        depth = Image.open(f'{self.store}/{obj}/{idx_argmin}-depth.png') \n",
    "        target = self.pose_dict[idx][idx_argmin,:3,:3] \n",
    "        return self.pose_dict[idx][idx_argmin],\\\n",
    "                self.cam_dict[idx][idx_argmin],\\\n",
    "                img,\\\n",
    "                depth, target, idx_argmin\n",
    "            \n",
    "    def get_closest_image_single(self, idx, mat):\n",
    "        idx = idx.unsqueeze(0)\n",
    "        mat = mat.unsqueeze(0)\n",
    "        return self.get_closest_image_batch(idx, mat)\n",
    "            \n",
    "    def get_closest_image_batch(self, idx, mat):\n",
    "        \"\"\"\n",
    "        mat: BSx3x3\n",
    "        idx: BSx1\n",
    "        \"\"\"\n",
    "        bs = idx.shape[0]\n",
    "        print(\"idx.shape\",idx.shape)\n",
    "        n_mat = self.pose_dict[int(idx)][:,:3,:3]\n",
    "        n_mat.unsqueeze(0)\n",
    "        \n",
    "        n_mat = n_mat.repeat((bs,1,1,1) )\n",
    "        print('n_mat.shape',n_mat.shape)\n",
    "        print('mat.shape',mat.shape)\n",
    "        best_match_idx = angle_batch_torch_full(mat, n_mat) \n",
    "        \n",
    "        img = []\n",
    "        depth = []\n",
    "        target = []\n",
    "        \n",
    "        for j,i in enumerate( idx.tolist() ):\n",
    "            best_match = int(best_match_idx[j])\n",
    "            \n",
    "            obj = self.idx_to_name[ i ]\n",
    "            print(\"best_match\",best_match, best_match_idx)\n",
    "            \n",
    "            img.append( Image.open(f'{self.store}/{obj}/{best_match}-color.png') )\n",
    "            depth.append( Image.open(f'{self.store}/{obj}/{best_match}-depth.png') ) \n",
    "            target.append( self.pose_dict[ i ][best_match,:3,:3]  )\n",
    "        \n",
    "        return img, depth, target\n",
    "        \n",
    "                           \n",
    "vm = ViewpointManager(store, generic._backend._name_to_idx)\n",
    "print( generic._backend._name_to_idx)\n",
    "# apply the same to verify it with an image\n",
    "import scipy.io as scio\n",
    "model = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/models'\n",
    "base = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/data/0003'\n",
    "for i in range (1,500):\n",
    "    desig = '0'*(6-len(str(i)))+str(i)\n",
    "#     desig = '000550'\n",
    "    print(desig)\n",
    "    store = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/viewpoints_renderings'\n",
    "\n",
    "    img = Image.open('{0}/{1}-color.png'.format(base, desig))\n",
    "    obj = '025_mug'#'005_tomato_soup_can'\n",
    "    obj_idx = generic._backend._name_to_idx[obj]\n",
    "\n",
    "    meta = scio.loadmat('{0}/{1}-meta.mat'.format(base, desig))\n",
    "\n",
    "    obj_tmp = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "    obj_idx_in_list = int(np.argwhere(obj_tmp == obj_idx))\n",
    "    target_r = np.array(meta['poses'][:, :, obj_idx_in_list][:, 0:3])\n",
    "    target_t = np.array(\n",
    "        [meta['poses'][:, :, obj_idx_in_list][:, 3:4].flatten()])[0,:]\n",
    "\n",
    "    plt_img(img)\n",
    "    start = time.time()\n",
    "    start = time.time()\n",
    "    p,c,img,depth,target,idx_argmin = vm.get_closest_image(idx = obj_idx, mat =target_r )\n",
    "    print( \"Total Time: \", time.time()-start)\n",
    "    plt_img(img)\n",
    "\n",
    "\n",
    "    t_target_r = torch.tensor( target_r, dtype=torch.float32 ).cuda()\n",
    "    t_obj_idx = torch.tensor( obj_idx, dtype=torch.int64 ).cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    img, depth, target = vm.get_closest_image_single(idx = t_obj_idx, mat = t_target_r)\n",
    "    print( \"Total Time: \", time.time()-start)\n",
    "\n",
    "\n",
    "    # p,c,img,depth,target,idx_argmin = vm.get_closest_image(idx = obj_idx, mat = torch.Tensor(target_r).cuda())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt_img(img[0])\n",
    "    print(p)\n",
    "    print(\"selected rotation\", target)                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "bs = 10\n",
    "input2 = torch.rand(bs, 4, 3, 3)  # Nx3x4\n",
    "input1 = torch.rand(bs, 1, 3, 3)  # Nx3x4\n",
    "\n",
    "out = angle_batch_torch(input1,input2)\n",
    "out.shape\n",
    "print(out)\n",
    "\n",
    "\n",
    "# mat.dot(n_mat[i,:,:].T\n",
    "\n",
    "# rotation_matrix_to_angle_axis\n",
    "\n",
    "# for i in range (input1.shape[0] ):\n",
    "#     r, _ = cv2.Rodrigues(mat.dot(n_mat[i,:,:].T))\n",
    "#     dif.append( np.linalg.norm(r))\n",
    "# # return np.array(dif)\n",
    "\n",
    "# output = tgm.rotation_matrix_to_quaternion(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import special_ortho_group\n",
    "mat_tmp = np.array( special_ortho_group.rvs(dim = 3, size = 10000) )\n",
    "mat_ref = np.array( special_ortho_group.rvs(dim = 3, size = 1) )\n",
    "\n",
    "an = angle(mat_tmp[0,:,:], mat_ref)\n",
    "\n",
    "an2 = angle_gen(mat_ref, mat_tmp)\n",
    "\n",
    "print( an2[0], an )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( vm.pose_dict[obj_idx].shape )\n",
    "\n",
    "m2 = vm.pose_dict[obj_idx][99,:3,:3]\n",
    "angle_smallest = 0\n",
    "best = 0\n",
    "for i in range (0, vm.pose_dict[obj_idx].shape[0] ):\n",
    "    m1 = vm.pose_dict[obj_idx][i,:3,:3]\n",
    "    if angle(m1,m2) > angle_smallest and angle(m1,m2) > 0.00001:\n",
    "        angle_smallest = angle(m1,m2)\n",
    "        best = i \n",
    "print(best, angle_smallest)\n",
    "\n",
    "img = Image.open(f'{store}/{obj}/{99}-color.png')\n",
    "img2 = Image.open(f'{store}/{obj}/{best}-color.png')\n",
    "plt_img(img)\n",
    "\n",
    "plt_img(img2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "mat = np.eye(3)\n",
    "R2 = R.from_euler('xyz',[0.3,0.6,0], degrees=False).as_matrix()\n",
    "\n",
    "np.arccos ( (np.trace((R2.dot(mat.T)))-1)/2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "# verfiy loading speed \n",
    "\n",
    "vm = ViewpointManager(store, generic._backend._name_to_idx)\n",
    "ref = R.from_euler('xyz',[130,10,50], degrees=True).as_matrix()\n",
    "s = time.time()\n",
    "p,c,img,depth = vm.get_closest_image(idx = 20,mat = ref)\n",
    "print( time.time()-s )\n",
    "\n",
    "target = ref.dot( np.array([1,0,0]) )\n",
    "render = vm.pose_dict[1][:,:3,:3].dot(np.array([1,0,0]))\n",
    "\n",
    "sel = p[:3,:3].dot(np.array([1,0,0]))\n",
    "\n",
    "\n",
    "# let plot the selected viewpoint vs the randonly generated one \n",
    "import k3d\n",
    "plot = k3d.plot(name='points')\n",
    "points = render.tolist()\n",
    "\n",
    "points.append( target.tolist() )\n",
    "points.append( sel.tolist() )\n",
    "\n",
    "point_size = 0.05\n",
    "x_rgb = (0,0,255)\n",
    "x_col = []\n",
    "for i in range (0, len(points)):\n",
    "    if i == len(points)-2:\n",
    "        x_rgb = (255,0,0)\n",
    "        #target red\n",
    "        \n",
    "    elif i == len(points)-1:\n",
    "        x_rgb = (0,255,0)\n",
    "        #sel green\n",
    "    else: \n",
    "        x_rgb = (0,0,255)\n",
    "    rgb_int = int('%02x%02x%02x' % x_rgb, 16)\n",
    "    x_col.append( rgb_int ) \n",
    "\n",
    "plt_points = k3d.points(points, np.array(x_col).astype(np.uint32), point_size=point_size)\n",
    "plot += plt_points\n",
    "\n",
    "plt_points.shader='3d'\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = R.from_euler('xyz',[130,10,50], degrees=True).as_matrix()\n",
    "print(ref)\n",
    "\n",
    "trans = [0,0,0.4] \n",
    "translate = np.eye(4)\n",
    "translate[:3,3] = trans\n",
    "\n",
    "rotate = np.eye(4)\n",
    "\n",
    "rotate[:3,:3] = ref\n",
    "\n",
    "print(\"rot_trans,\" , rotate.dot(translate) )\n",
    "print(\"trans_rot,\" , translate.dot(rotate) )\n",
    "\n",
    "h = np.eye(4) \n",
    "h[:3,:3] = ref\n",
    "h[:3,3] = trans\n",
    "print(\"homo\", h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = ViewpointManager(store, generic._backend._name_to_idx)\n",
    "\n",
    "# apply the same to verify it with an image\n",
    "import scipy.io as scio\n",
    "model = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/models'\n",
    "base = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/data/0003'\n",
    "desig = '000010'\n",
    "store = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/viewpoints_renderings'\n",
    "\n",
    "img = Image.open('{0}/{1}-color.png'.format(base, desig))\n",
    "obj = '005_tomato_soup_can'#'005_tomato_soup_can'\n",
    "obj_idx_tomoto_soup = 4\n",
    "\n",
    "meta = scio.loadmat('{0}/{1}-meta.mat'.format(base, desig))\n",
    "\n",
    "obj_tmp = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "obj_idx_in_list = int(np.argwhere(obj_tmp == obj_idx_tomoto_soup))\n",
    "target_r = np.array(meta['poses'][:, :, obj_idx_in_list][:, 0:3])\n",
    "target_t = np.array(\n",
    "    [meta['poses'][:, :, obj_idx_in_list][:, 3:4].flatten()])[0,:]\n",
    "\n",
    "\n",
    "plt_img(img)\n",
    "\n",
    "print( target_r ) \n",
    "p,c,img,depth  = vm.get_closest_image(idx = obj_idx_tomoto_soup, mat = target_r)\n",
    "plt_img(img)\n",
    "print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track",
   "language": "python",
   "name": "track"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
