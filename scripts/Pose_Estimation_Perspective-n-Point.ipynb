{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "os.chdir('/home/jonfrey/PLR3')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd() + '/src'))\n",
    "sys.path.append(os.path.join(os.getcwd() + '/lib'))\n",
    "\n",
    "import loaders_v2\n",
    "from loaders_v2 import GenericDataset\n",
    "from rotations import * \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from visu import plot_pcd, Visualizer\n",
    "import copy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from helper import re_quat\n",
    "from PIL import Image, ImageDraw\n",
    "from deep_im import LossAddS\n",
    "import copy\n",
    "#from deep_im import flow_to_trafo\n",
    "from visu import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import k3d\n",
    "#exp_cfg_path = '/home/jonfrey/PLR3/yaml/exp/exp_ws_deepim_debug_natrix.yml'\n",
    "\n",
    "env_cfg_path = '/home/jonfrey/PLR3/yaml/env/env_natrix_jonas.yml'\n",
    "exp_cfg_path = '/home/jonfrey/PLR3/yaml/exp/exp_evaluate_pose_estimation.yml'\n",
    "h = 480\n",
    "w = 640\n",
    "import k3d\n",
    "\n",
    "def load_from_file(p):\n",
    "    if os.path.isfile(p):\n",
    "        with open(p, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return data\n",
    "\n",
    "exp = load_from_file(exp_cfg_path)\n",
    "env = load_from_file(env_cfg_path)\n",
    "\n",
    "dataset_train = GenericDataset(\n",
    "    cfg_d=exp['d_train'],\n",
    "    cfg_env=env)\n",
    "\n",
    "batch = dataset_train[13450][0] #bann 10450\n",
    "points, choose, img, target, model_points, idx = batch[0:6]\n",
    "depth_img, label_img, img_orig, cam = batch[6:10]\n",
    "gt_rot_wxyz, gt_trans, unique_desig = batch[10:13]\n",
    "\n",
    "real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "pred_rot_wxyz, pred_trans, pred_points, h_render,h_real, render_img_original = batch[18:24]\n",
    "u_map, v_map, flow_mask,  bb , depth_render_original= batch[24:]\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ray Tracering \n",
    "# DepthMap -> Reproject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Helper Functions (not needed)\n",
    "class Drawer():\n",
    "    def __init__(self):\n",
    "        self.im_in_plot = 0\n",
    "        self.data = []\n",
    "        \n",
    "    def disp_img_1d(self,img,hold=False, save=False, nr=0 , ret=False):\n",
    "        self.data.append(img)\n",
    "        p = '/home/jonfrey/Debug/Midterm2/'\n",
    "        \n",
    "        if not hold:\n",
    "            fig = plt.figure(figsize=(6*2*len(self.data),7))\n",
    "            ax = []\n",
    "            for j,a in enumerate(self.data):\n",
    "                ax.append( fig.add_subplot(1,len(self.data), j+1)  )\n",
    "                \n",
    "                ax[-1].get_xaxis().set_visible(False)\n",
    "                ax[-1].get_yaxis().set_visible(False)\n",
    "                pos = ax[-1].imshow( a, cmap='Reds' )\n",
    "                try:\n",
    "                    if a.shape[2] != 3:\n",
    "                        fig.colorbar(pos, ax=ax[-1])\n",
    "                except:\n",
    "                    pass\n",
    "            plt.show()\n",
    "            if save:\n",
    "                fig.savefig(p+str(nr)+'.png', dpi=300)\n",
    "                \n",
    "            if ret: \n",
    "                if isinstance( self.data[0], torch.Tensor):\n",
    "                    self.data[0] = self.data[0].numpy()\n",
    "                    print('CONV')\n",
    "                    \n",
    "                print(self.data[0].shape)\n",
    "                a = np.max(self.data[0])\n",
    "                b = np.min(self.data[0])\n",
    "                \n",
    "                d = (self.data[0]-float(b))\n",
    "                d = (d / ((float(a)-float(b))) )*255 \n",
    "                d = np.uint8(d)\n",
    "                img = Image.fromarray( d )\n",
    "                return d\n",
    "            self.data = []\n",
    "            self.ax = []\n",
    "            \n",
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "def disp_alignment(depth, label, real):\n",
    "    data = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    t = real\n",
    "    data[:,:,:3] = t.numpy() # red patch in upper left\n",
    "    data_depth[:,:,:3] = t.numpy()\n",
    "    data[:,:,3] = 70\n",
    "    data[:,:,3][label==8] = 255\n",
    "    \n",
    "    min_val = torch.min( depth[depth!=0] )\n",
    "    max_val = torch.max( depth[depth!=0] )\n",
    "    val = torch.clamp( ((depth-min_val) // (max_val-min_val))*255, 0, 255)\n",
    "    \n",
    "    img = Image.fromarray(data, 'RGBA')\n",
    "    display(img)\n",
    "\n",
    "def plot_mask(mask):\n",
    "    min_val = torch.min( mask )\n",
    "    max_val = float( max(1,torch.max( mask )) )\n",
    "    mask = torch.clamp( (mask-min_val) / (max_val-min_val)*255 ,0,255)\n",
    "    \n",
    "    data_depth = np.zeros((480,640,4), dtype=np.uint8)\n",
    "    data_depth[:,:,3] = 255\n",
    "    for i in range(480):\n",
    "        for j in range(640):\n",
    "            data_depth[i,j,:4] = np.array( cmaplist[ int(mask[i,j])] )*255\n",
    "    data_depth[:,:,3] = 255\n",
    "    data_depth[:,:,3][label==2] = 255\n",
    "    img_depth = Image.fromarray(data_depth, 'RGBA')\n",
    "    display(img_depth)\n",
    "\n",
    "def plot_two_pcd_line(x, y, point_size=0.005, c1='g', c2='r'):\n",
    "    if c1 == 'b':\n",
    "        k = 245\n",
    "    elif c1 == 'g':\n",
    "        k = 25811000\n",
    "    elif c1 == 'r':\n",
    "        k = 11801000\n",
    "    elif c1 == 'black':\n",
    "        k = 2580\n",
    "    else:\n",
    "        k = 2580\n",
    "\n",
    "    if c2 == 'b':\n",
    "        k2 = 245\n",
    "    elif c2 == 'g':\n",
    "        k2 = 25811000\n",
    "    elif c2 == 'r':\n",
    "        k2 = 11801000\n",
    "    elif c2 == 'black':\n",
    "        k2 = 2580\n",
    "    else:\n",
    "        k2 = 2580\n",
    "\n",
    "    col1 = np.ones(x.shape[0]) * k\n",
    "    col2 = np.ones(y.shape[0]) * k2\n",
    "    plot = k3d.plot(name='points')\n",
    "    plt_points = k3d.points(x, col1.astype(np.uint32), point_size=point_size)\n",
    "    plot += plt_points\n",
    "    plt_points = k3d.points(y, col2.astype(np.uint32), point_size=point_size)\n",
    "    plot += plt_points\n",
    "    for i in range(min(100,x.shape[0]) ):\n",
    "        plot += k3d.line([x[i],y[i]],shader='mesh', width=0.0005, color=0xff0000)\n",
    "    \n",
    "    plt_points.shader = '3d'\n",
    "    plot.display()\n",
    "\n",
    "def plot_hist(x,n_bins = 20):\n",
    "    fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "    colors = ['lime']\n",
    "    axs.hist(x, bins=n_bins, color=colors, label=colors)\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "def get_scale_for_erosion(ero_in):\n",
    "    res = torch.sum ( ero_in, dim = (2,3))\n",
    "    res[res < 5000] = 5\n",
    "    res[res < 10000] = 10\n",
    "    res[res < 30000] = 20\n",
    "    res[res < 40000] = 25\n",
    "    res[res < 50000] = 30\n",
    "    res[res >= 50000] = 40\n",
    "    return res\n",
    "\n",
    "def eroision(t,size=3):\n",
    "    \"t: tensor shape BS, C, H,W\"\n",
    "    out_c = t.shape[1]\n",
    "    kernel_tensor = torch.ones( (out_c,1,size,size) )\n",
    "    print(size, kernel_tensor, t.shape)\n",
    "    return torch.nn.functional.conv2d(t, kernel_tensor, padding=(int((size)/2), int((size)/2))) == (size*size)\n",
    "\n",
    "def eroision_batch(t,t_size):\n",
    "    \"t: tensor shape BS, C, H,W\"\n",
    "    \"t_size: tensor shape BS\"\n",
    "    out_c = t.shape[1]\n",
    "    for b in range( t.shape[0] ):\n",
    "        size = int( t_size[b] )\n",
    "        kernel_tensor = torch.ones( (out_c,1,size,size) )\n",
    "        t[b] = (torch.nn.functional.conv2d(t[b][None], kernel_tensor, padding=(int((size)/2), int((size)/2))) == (size*size))[0,:,:t.shape[2], :t.shape[3]]\n",
    "    return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a specific datapoint form the dataset (Not Needed)\n",
    "\n",
    "# [('data/0003/001742',), tensor([8], dtype=torch.int32)]\n",
    "\n",
    "desig = unique_desig[0]\n",
    "desig = 'data/0003/001742'\n",
    "\n",
    "_p_ycb = \"/media/scratch1/jonfrey/datasets/YCB_Video_Dataset\"\n",
    "depth = np.array(Image.open(\n",
    "    '{0}/{1}-depth.png'.format(_p_ycb, desig)))\n",
    "depth.shape\n",
    "\n",
    "label = np.array(Image.open(\n",
    "    '{0}/{1}-label.png'.format(_p_ycb, desig)))\n",
    "img = np.array(Image.open(\n",
    "    '{0}/{1}-color.png'.format(_p_ycb, desig)))\n",
    "batch = dataset_train._backend.getElement( desig, 8)\n",
    "batch = batch #bann 10450   \n",
    "model_points = batch[4]\n",
    "idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "real_img_original = batch[8]\n",
    "cam = batch[9]\n",
    "real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "u_map, v_map, flow_mask, bb, render_orig = batch[24:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 256\n",
    "cmap = plt.cm.get_cmap('gist_rainbow', Nc)\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loader to BS 1\n",
    "exp['loader']['batch_size'] = 1\n",
    "exp['loader']['pin_memory'] = False\n",
    "exp['loader']['shuffle'] = True\n",
    "exp['loader']['num_workers'] = 1\n",
    "\n",
    "exp['d_train'][\"output_cfg\"]['overfitting_nr_idx'] = -1\n",
    "exp['d_train'][\"flow_cfg\"]['sub'] = 1\n",
    "exp['d_train'][\"flow_cfg\"]['min_matches'] = 50\n",
    "exp['d_train'][\"flow_cfg\"]['max_matches'] = 5000\n",
    "exp['d_train'][\"flow_cfg\"]['max_iterations'] = 20000\n",
    "exp['d_train'][\"flow_cfg\"]['dil_kernel_size'] = 3\n",
    "\n",
    "dataset_train = GenericDataset(\n",
    "    cfg_d=exp['d_train'],\n",
    "    cfg_env=env)\n",
    "\n",
    "\n",
    "# get test and train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                                       **exp['loader'])\n",
    "exp['d_test'][\"output_cfg\"]['overfitting_nr_idx'] = -1\n",
    "exp['d_test'][\"output_cfg\"]['noise_translation'] = 0.02\n",
    "exp['d_test'][\"output_cfg\"]['noise_rotation'] = 40\n",
    "\n",
    "exp['d_test'][\"flow_cfg\"]['sub'] = 1\n",
    "exp['d_test'][\"flow_cfg\"]['min_matches'] = 300\n",
    "exp['d_test'][\"flow_cfg\"]['max_matches'] = 5000\n",
    "exp['d_test'][\"flow_cfg\"]['max_iterations'] = 20000\n",
    "exp['d_test'][\"flow_cfg\"]['dil_kernel_size'] = 1\n",
    "dataset_test = GenericDataset(\n",
    "    cfg_d=exp['d_test'],\n",
    "    cfg_env=env)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test,\n",
    "                                                       **exp['loader'])\n",
    "\n",
    "# get Loss function\n",
    "criterion_adds = LossAddS(sym_list=exp['d_train']['obj_list_sym'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy ICP implementation from https://github.com/ClayFlannigan/icp/blob/master/icp.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def best_fit_transform(A, B):\n",
    "    '''\n",
    "    Calculates the least-squares best-fit transform that maps corresponding points A to B in m spatial dimensions\n",
    "    Input:\n",
    "      A: Nxm numpy array of corresponding points\n",
    "      B: Nxm numpy array of corresponding points\n",
    "    Returns:\n",
    "      T: (m+1)x(m+1) homogeneous transformation matrix that maps A on to B\n",
    "      R: mxm rotation matrix\n",
    "      t: mx1 translation vector\n",
    "    '''\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    # get number of dimensions\n",
    "    m = A.shape[1]\n",
    "\n",
    "    # translate points to their centroids\n",
    "    centroid_A = np.mean(A, axis=0)\n",
    "    centroid_B = np.mean(B, axis=0)\n",
    "    AA = A - centroid_A\n",
    "    BB = B - centroid_B\n",
    "\n",
    "    # rotation matrix\n",
    "    H = np.dot(AA.T, BB)\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = np.dot(Vt.T, U.T)\n",
    "\n",
    "    # special reflection case\n",
    "    if np.linalg.det(R) < 0:\n",
    "       Vt[m-1,:] *= -1\n",
    "       R = np.dot(Vt.T, U.T)\n",
    "\n",
    "    # translation\n",
    "    t = centroid_B.T - np.dot(R,centroid_A.T)\n",
    "\n",
    "    # homogeneous transformation\n",
    "    T = np.identity(m+1)\n",
    "    T[:m, :m] = R\n",
    "    T[:m, m] = t\n",
    "\n",
    "    return T, R, t\n",
    "\n",
    "\n",
    "def nearest_neighbor(src, dst):\n",
    "    '''\n",
    "    Find the nearest (Euclidean) neighbor in dst for each point in src\n",
    "    Input:\n",
    "        src: Nxm array of points\n",
    "        dst: Nxm array of points\n",
    "    Output:\n",
    "        distances: Euclidean distances of the nearest neighbor\n",
    "        indices: dst indices of the nearest neighbor\n",
    "    '''\n",
    "\n",
    "    assert src.shape == dst.shape\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=1)\n",
    "    neigh.fit(dst)\n",
    "    distances, indices = neigh.kneighbors(src, return_distance=True)\n",
    "    return distances.ravel(), indices.ravel()\n",
    "\n",
    "\n",
    "def icp(A, B, init_pose=None, max_iterations=20, tolerance=0.001):\n",
    "    '''\n",
    "    The Iterative Closest Point method: finds best-fit transform that maps points A on to points B\n",
    "    Input:\n",
    "        A: Nxm numpy array of source mD points\n",
    "        B: Nxm numpy array of destination mD point\n",
    "        init_pose: (m+1)x(m+1) homogeneous transformation\n",
    "        max_iterations: exit algorithm after max_iterations\n",
    "        tolerance: convergence criteria\n",
    "    Output:\n",
    "        T: final homogeneous transformation that maps A on to B\n",
    "        distances: Euclidean distances (errors) of the nearest neighbor\n",
    "        i: number of iterations to converge\n",
    "    '''\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    # get number of dimensions\n",
    "    m = A.shape[1]\n",
    "\n",
    "    # make points homogeneous, copy them to maintain the originals\n",
    "    src = np.ones((m+1,A.shape[0]))\n",
    "    dst = np.ones((m+1,B.shape[0]))\n",
    "    src[:m,:] = np.copy(A.T)\n",
    "    dst[:m,:] = np.copy(B.T)\n",
    "\n",
    "    # apply the initial pose estimation\n",
    "    if init_pose is not None:\n",
    "        src = np.dot(init_pose, src)\n",
    "\n",
    "    prev_error = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # find the nearest neighbors between the current source and destination points\n",
    "        distances, indices = nearest_neighbor(src[:m,:].T, dst[:m,:].T)\n",
    "\n",
    "        # compute the transformation between the current source and nearest destination points\n",
    "        T,_,_ = best_fit_transform(src[:m,:].T, dst[:m,indices].T)\n",
    "\n",
    "        # update the current source\n",
    "        src = np.dot(T, src)\n",
    "\n",
    "        # check error\n",
    "        mean_error = np.mean(distances)\n",
    "        if np.abs(prev_error - mean_error) < tolerance:\n",
    "            break\n",
    "        prev_error = mean_error\n",
    "\n",
    "    # calculate final transformation\n",
    "    T,_,_ = best_fit_transform(A, src[:m,:].T)\n",
    "\n",
    "    return T, distances, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax = -1 \n",
    "def get_H(pcd):\n",
    "    pcd_ret = torch.ones( (pcd.shape[0],pcd.shape[1]+1),device=pcd.device, dtype=pcd.dtype )\n",
    "    pcd_ret[:,:3] = pcd\n",
    "    return pcd_ret\n",
    "\n",
    "def eval_T(P_real_in_center, P_ren_in_center, T_res):\n",
    "        \"\"\"\n",
    "        NR,3\n",
    "        NR,3 \n",
    "        4,4\n",
    "        \"\"\"\n",
    "        P_ren_H = get_H( P_ren_in_center )\n",
    "        P_ren_trafo =  (P_ren_H @ T_res.T)[:,:3]\n",
    "        L2_dis_post = torch.mean( torch.norm( P_real_in_center-P_ren_trafo, dim=1 ) )\n",
    "        L2_dis_pre = torch.mean( torch.norm( P_real_in_center-P_ren_in_center, dim=1 ) )\n",
    "        return L2_dis_post, L2_dis_pre  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from helper import anal_tensor\n",
    "\n",
    "def solve_transform(keypoints, gt_keypoints):\n",
    "    \"\"\"\n",
    "    keypoints: N x K x 3\n",
    "    gt_keypoints: K x 3\n",
    "    return: N x 4 x 4 transformation matrix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keypoints = keypoints.clone()\n",
    "        gt_keypoints = gt_keypoints.clone()\n",
    "        N, K, _ = keypoints.shape\n",
    "        center = keypoints.mean(dim=1)\n",
    "        gt_center = gt_keypoints.mean(dim=0)\n",
    "        keypoints -= center[:, None, :]\n",
    "        gt_keypoints -= gt_center[None]\n",
    "        matrix = keypoints.transpose(2, 1) @ gt_keypoints[None]\n",
    "        U, S, V = torch.svd(matrix)\n",
    "        \n",
    "        Vt = V.transpose(2, 1)\n",
    "        Ut = U.transpose(2, 1)\n",
    "\n",
    "        d = (V @ Ut).det()\n",
    "        I = torch.eye(3, 3, dtype=gt_center.dtype, device= keypoints.device)[None].repeat(N, 1, 1)\n",
    "        I[:, 2, 2] = d.clone()\n",
    "\n",
    "        R = U @ I @ Vt\n",
    "        T = torch.zeros(N, 4, 4, dtype=gt_center.dtype, device= keypoints.device)\n",
    "        T[:, 0:3, 0:3] = R\n",
    "        T[:, 0:3, 3] = center[None] - (R @ gt_center[None :, None])[:, :, 0]\n",
    "        T[:, 3, 3] = 1.0\n",
    "\n",
    "        return T\n",
    "    except RuntimeError as error:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        print(\"Something went wrong\")\n",
    "\n",
    "# costume implementation \n",
    "def solve_transform2(A,B):\n",
    "    if A.shape[0] > B.shape[0]:\n",
    "        x=torch.arange(A.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:B.shape[0]]\n",
    "        A = torch.index_select(A, 0, out)\n",
    "    if A.shape[0] < B.shape[0]:\n",
    "        x=torch.arange(B.shape[0],device=A.device)\n",
    "        out = torch.randperm(x.numel(),device=A.device)[:A.shape[0]]\n",
    "        B = torch.index_select(B, 0, out)\n",
    "\n",
    "    #A = torch.choice(A,B.shape[0])\n",
    "\n",
    "    assert A.shape == B.shape\n",
    "\n",
    "    m = A.shape[1]\n",
    "    centroid_A = torch.mean(A, dim=0)\n",
    "    centroid_B = torch.mean(B, dim=0)\n",
    "\n",
    "    AA = (A - centroid_A)\n",
    "    BB = (B - centroid_B)\n",
    "    H = AA.transpose(0,1) @ BB\n",
    "    U, S, Vt = torch.svd(H)\n",
    "    R = Vt @ U.transpose(0,1)\n",
    "    if torch.det(R) < 0:\n",
    "        Vt[m-1,:] *= -1\n",
    "        R = Vt.transpose(0,1) @ U.transpose(0,1)\n",
    "\n",
    "    # translation\n",
    "    t = centroid_B - (R @ centroid_A)\n",
    "    # homogeneous transformation\n",
    "    T = torch.eye(m+1, device= A.device)\n",
    "    T[:m, :m] = R\n",
    "    T[:m, m] = t\n",
    "    return T\n",
    "\n",
    "\n",
    "# NR = 1000\n",
    "# DIM = 3\n",
    "# A = torch.ones( (NR,DIM), dtype= torch.float32)\n",
    "# B = A*1.7223\n",
    "# T = solve_transform2(A,B)\n",
    "# print(T)\n",
    "\n",
    "\n",
    "# A_hom = get_H(A)\n",
    "# A_hom2 = A_hom @ T.T\n",
    "# print(A_hom2[:,:3])\n",
    "\n",
    "def filter_pcd_given_depthmap(pcd, depth, scal= 10000):\n",
    "    \"\"\"\n",
    "    pcd = Nx3 troch.float32\n",
    "    depth = N torch.float32\n",
    "\n",
    "    return N torch.bool\n",
    "    \"\"\"\n",
    "    m1 = (depth/scal) > 0.01\n",
    "    #print( \"Thorwn away values\", (depth/scal) < 0.2 )\n",
    "    return m1\n",
    "\n",
    "    val_d = depth[ m1 ]\n",
    "    mean = torch.mean(val_d)\n",
    "    new_d = depth - mean\n",
    "    tol = 0.5\n",
    "    m2 = torch.abs( new_d/scal ) < tol \n",
    "    return m1 * m2\n",
    "    \n",
    "def filter_pcd( pcd, tol = 0.6):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        pcd : Nx3 torch.float32\n",
    "    returns:\n",
    "        mask : NX3 torch.bool \n",
    "    \"\"\"\n",
    "    return pcd[:,2] > 0.05\n",
    "    \n",
    "    m = torch.mean(pcd, dim = 0)\n",
    "    comp = m[None,:].repeat(pcd.shape[0],1) + tol\n",
    "    mean_free = pcd-m[None,:].repeat(comp.shape[0],1)\n",
    "    mask = torch.norm( mean_free,  dim= 1) > tol\n",
    "    #print(f\"filter_pcd PRE: {pcd.shape}, POST: {float(torch.sum(mask[:,None].repeat(1,3) == False ))/3.0}\")\n",
    "    return mask == False\n",
    "\n",
    "def filter_pcd_cor(pcd1, pcd2, max_mean_deviation=0.2):\n",
    "    \n",
    "    dif = torch.norm( pcd1-pcd2 , dim= 1)\n",
    "    mean = torch.mean(dif, dim = 0)\n",
    "    mean_free = torch.abs(dif-mean)\n",
    "    #print(f\"filter_pcd_cor PRE: {pcd1.shape[0]}, POST: {torch.sum(mean_free < max_mean_deviation)}\")\n",
    "    return mean_free < max_mean_deviation\n",
    "    \n",
    "def flow_to_trafo(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      real_br: torch.tensor torch.Size([2])\n",
    "      real_tl: torch.tensor torch.Size([2])\n",
    "      ren_br: torch.tensor torch.Size([2])\n",
    "      ren_tl: torch.tensor torch.Size([2])\n",
    "      flow_mask: torch.Size([480, 640])\n",
    "      u_map: torch.Size([480, 640])\n",
    "      v_map: torch.Size([480, 640])\n",
    "      K_real: torch.Size([3, 3])\n",
    "      K_ren: torch.Size([3, 3])\n",
    "      real_d: torch.Size([480, 640]) \n",
    "      render_d: torch.Size([480, 640])\n",
    "      h_real: torch.Size([4, 4])\n",
    "      h_render: torch.Size([4, 4])\n",
    "    output:\n",
    "      P_real_in_center: torch.Size([N, 3])\n",
    "      P_ren_in_center: torch.Size([N, 3]) \n",
    "      P_real_trafo: torch.Size([N, 3])\n",
    "      T_res: torch.Size([4, 4])\n",
    "      \n",
    "      The output rotation T_res is defined in the Camera coordinate frame. \n",
    "      Therfore premultiply the T_Res with h_render to get the new h_real_new !!!\n",
    "    \"\"\"\n",
    "    for k in kwargs.keys():\n",
    "        pass\n",
    "        #print(f\"Variable: {k}, Type {type(kwargs[k])}, Dtype{kwargs[k].dtype}, Shape{kwargs[k].shape}\")\n",
    "    real_br = kwargs['real_br']\n",
    "    real_tl = kwargs['real_tl']\n",
    "    ren_br = kwargs['ren_br']\n",
    "    ren_tl = kwargs['ren_tl']\n",
    "    flow_mask = kwargs['flow_mask']\n",
    "    u_map = kwargs['u_map']\n",
    "    v_map = kwargs['v_map']\n",
    "    K_real = kwargs['K_real']\n",
    "    K_ren = kwargs['K_ren']\n",
    "    real_d = kwargs['real_d']\n",
    "    render_d = kwargs['render_d']\n",
    "    h_real = kwargs['h_real']\n",
    "    h_render = kwargs['h_render']\n",
    "    plot_pcd = kwargs.get('plot_pcd',False)\n",
    "  \n",
    "    # Grid for upsampled real\n",
    "    grid_real_h = torch.linspace(int(real_tl[0]) ,int(real_br[0]) , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_real_w = torch.linspace(int(real_tl[1]) ,int(real_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "\n",
    "    # Grid for upsampled ren\n",
    "    c = 0\n",
    "    \n",
    "    grid_ren_h = torch.linspace(int(ren_tl[0]) ,int(ren_br[0]) , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_ren_w = torch.linspace(int(ren_tl[1]) ,int(ren_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    # Calculate valid depth map for rendered image\n",
    "    render_d_ind_h = torch.linspace(0 ,479 , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    render_d_ind_w= torch.linspace(0 ,639 , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "    render_d_ind_h = torch.clamp(torch.round((render_d_ind_h - u_map).type(torch.float32)) ,0,479).type( torch.long )[flow_mask]\n",
    "    render_d_ind_w = torch.clamp(torch.round((render_d_ind_w - v_map).type(torch.float32)),0,639).type( torch.long )[flow_mask] \n",
    "    index = render_d_ind_h*640 + render_d_ind_w # hacky indexing along two dimensions\n",
    "    ren_d_masked  = render_d.flatten()[index]\n",
    "    \n",
    "    # Project depth map to the pointcloud real\n",
    "    cam_scale = 10000\n",
    "\n",
    "    real_pixels = torch.stack( [grid_real_w[flow_mask], grid_real_h[flow_mask], torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)[flow_mask]], dim=1 ).type(u_map.dtype)\n",
    "    K_inv = torch.inverse(K_real.type(torch.float32)).type(u_map.dtype)\n",
    "    P_real = K_inv @ real_pixels.T\n",
    "    P_real = P_real * real_d[flow_mask] / cam_scale\n",
    "    P_real = P_real.T\n",
    "    \n",
    "    # Project depth map to the pointcloud render\n",
    "    K_ren_inv = torch.inverse(K_ren.type(torch.float32)).type(u_map.dtype)\n",
    "    ren_pixels = torch.stack( [grid_ren_w[flow_mask] - v_map[flow_mask], \n",
    "                            grid_ren_h[flow_mask] - u_map[flow_mask],\n",
    "                            torch.ones(grid_ren_h.shape, device = u_map.device,  dtype= u_map.dtype )[flow_mask]], \n",
    "                            dim=1 ).type(u_map.dtype)\n",
    "    P_ren = K_ren_inv @ ren_pixels.T\n",
    "    P_ren = P_ren * ren_d_masked / cam_scale\n",
    "    P_ren = P_ren.T\n",
    "\n",
    "    # Filter the pointclouds given the depthmap\n",
    "#     m_ren_depth = filter_pcd_given_depthmap(P_ren, ren_d_masked)\n",
    "#     m_real_depth = filter_pcd_given_depthmap(P_real, real_d[flow_mask])\n",
    "#     m_total =  m_ren_depth * m_real_depth\n",
    "    \n",
    "    min_points = 20\n",
    "#     if torch.sum(m_total) < min_points:\n",
    "#         print(f'Violation filter pcd_given_depthmap: P_in: {P_ren.shape[0]} P_out: {torch.sum(m_total)}')\n",
    "#         return False, P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "#     P_ren = P_ren[m_total] \n",
    "#     P_real = P_real[m_total]\n",
    "    # anal_tensor(  P_ren, 'P_ren m_total masked')\n",
    "\n",
    "    # Do not transfrom to center coordinate system\n",
    "    P_real_in_center = P_real                      \n",
    "    P_ren_in_center = P_ren \n",
    "    \n",
    "    m_real = filter_pcd( P_real_in_center )\n",
    "    m_ren = filter_pcd( P_ren_in_center )\n",
    "    m_tot = m_real * m_ren\n",
    "    if torch.sum(m_tot) < min_points:\n",
    "        print(f'Violation filter_pcd: P_in: { P_ren_in_center.shape[0]} P_out: {torch.sum(m_tot)}')\n",
    "        return False, P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[m_tot]\n",
    "    P_ren_in_center = P_ren_in_center[m_tot]\n",
    "  \n",
    "    # Max mean deviation\n",
    "    m_new = filter_pcd_cor(P_real_in_center, P_ren_in_center)\n",
    "    \n",
    "    if torch.sum(m_new) < min_points:\n",
    "        print(f'Violation filter_pcd_cor: P_in: { P_ren_in_center.shape[0]} P_out: {torch.sum(m_new)}')\n",
    "        return False, P_real, P_ren, P_real, torch.eye(4, dtype= u_map.dtype, device=u_map.device)\n",
    "\n",
    "    P_real_in_center = P_real_in_center[m_new]\n",
    "    P_ren_in_center = P_ren_in_center[m_new]\n",
    "\n",
    "    # random shuffel\n",
    "    pts_trafo = min( P_real_in_center.shape[0], 1000 )\n",
    "    idx = torch.randperm( P_real_in_center.shape[0] )[0:pts_trafo]\n",
    "    P_real_in_center = P_real_in_center[idx]\n",
    "    P_ren_in_center = P_ren_in_center[idx]\n",
    "\n",
    "    T_res = solve_transform( P_real_in_center[None].type(torch.float64 ) , P_ren_in_center.type(torch.float64 ) ).type(u_map.dtype )\n",
    "    \n",
    "    # Transform the real points according to calculated transformation\n",
    "    P_hr = torch.ones( (P_real_in_center.shape[0],4 ) , device=u_map.device, dtype= u_map.dtype)\n",
    "    P_hr[:,:3] = P_real_in_center\n",
    "    P_real_trafo = (torch.inverse( T_res[0].type(torch.float32) ).type(u_map.dtype ) @ copy.deepcopy(P_hr).T).T [:,:3]\n",
    "\n",
    "    return True, P_real_in_center, P_ren_in_center, P_real_trafo, T_res[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "cv2.__version__\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import k3d \n",
    "def rvec_tvec_to_H(r_vec,t_vec):\n",
    "    # get homogenous output transformation\n",
    "    rot = R.from_rotvec(r_vec)\n",
    "    h = np.eye(4)\n",
    "    h[:3,:3] = rot.as_matrix()\n",
    "    h[:3,3] = t_vec.T\n",
    "#     print(h)\n",
    "    return h\n",
    "class SingleObjectADDLoss:\n",
    "    def asymmetric(self, gt_T, T_hat, model_points):\n",
    "        R_hat = T_hat[:3, :3]\n",
    "        t_hat = T_hat[:3, 3, None]\n",
    "        model_points = model_points[:, :, None]\n",
    "        predicted = ((R_hat @ model_points) + t_hat)[:, :, 0]\n",
    "        R_gt = gt_T[:3, :3]\n",
    "        t_gt = gt_T[:3, 3, None]\n",
    "        ground_truth = ((R_gt @ model_points) + t_gt)[:, :, 0]\n",
    "        return (ground_truth - predicted).norm(dim=1).mean()\n",
    "\n",
    "    def symmetric(self, gt_T, T_hat, model_points):\n",
    "        ones = torch.ones(\n",
    "            model_points.shape[0], 1, dtype=model_points.dtype).to(gt_T.device)\n",
    "        points = torch.cat([model_points, ones], dim=1)[:, :, None]\n",
    "        ground_truth = (gt_T @ points)[:, :3, 0]\n",
    "        predicted = (T_hat @ points)[:, :3, 0]\n",
    "        dima = (ground_truth[None] - predicted[:, None]).norm(dim=2)\n",
    "        min_values, _ = dima.min(dim=1)\n",
    "        return min_values.mean(dim=0)\n",
    "    \n",
    "    def pcd(self, ground_truth, predicted):\n",
    "        dima = (ground_truth[None] - predicted[:, None]).norm(dim=2)\n",
    "        min_values, _ = dima.min(dim=1)\n",
    "        return min_values.mean(dim=0)\n",
    "    \n",
    "def flow_to_trafo_pnp(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      real_br: torch.tensor torch.Size([2])\n",
    "      real_tl: torch.tensor torch.Size([2])\n",
    "      ren_br: torch.tensor torch.Size([2])\n",
    "      ren_tl: torch.tensor torch.Size([2])\n",
    "      flow_mask: torch.Size([480, 640])\n",
    "      u_map: torch.Size([480, 640])\n",
    "      v_map: torch.Size([480, 640])\n",
    "      K_real: torch.Size([3, 3])\n",
    "      K_ren: torch.Size([3, 3])\n",
    "      real_d: torch.Size([480, 640]) \n",
    "      render_d: torch.Size([480, 640])\n",
    "      h_real: torch.Size([4, 4])\n",
    "      h_render: torch.Size([4, 4])\n",
    "    output:\n",
    "      P_real_in_center: torch.Size([N, 3])\n",
    "      P_ren_in_center: torch.Size([N, 3]) \n",
    "      P_real_trafo: torch.Size([N, 3])\n",
    "      T_res: torch.Size([4, 4])\n",
    "      \n",
    "      The output rotation T_res is defined in the Camera coordinate frame. \n",
    "      Therfore premultiply the T_Res with h_render to get the new h_real_new !!!\n",
    "    \"\"\"\n",
    "    real_br = kwargs['real_br']\n",
    "    real_tl = kwargs['real_tl']\n",
    "    ren_br = kwargs['ren_br']\n",
    "    ren_tl = kwargs['ren_tl']\n",
    "    flow_mask = kwargs['flow_mask']\n",
    "    u_map = kwargs['u_map']\n",
    "    v_map = kwargs['v_map']\n",
    "    K_real = kwargs['K_real']\n",
    "    K_ren = kwargs['K_ren']\n",
    "    real_d = kwargs['real_d']\n",
    "    render_d = kwargs['render_d']\n",
    "    h_real = kwargs['h_real']\n",
    "    h_render = kwargs['h_render']\n",
    "    h_real_est = kwargs['h_real_est']\n",
    "    mp = kwargs['mp']\n",
    "    img = kwargs['img']\n",
    "\n",
    "    # Grid for upsampled real\n",
    "    grid_real_h = torch.linspace(int(real_tl[0]) ,int(real_br[0]) , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_real_w = torch.linspace(int(real_tl[1]) ,int(real_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "    # Grid for upsampled ren\n",
    "    c = 0\n",
    "    grid_ren_h = torch.linspace(int(ren_tl[0]) ,int(ren_br[0]), 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_ren_w = torch.linspace(int(ren_tl[1]) ,int(ren_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    # Calculate valid depth map for rendered image\n",
    "    render_d_ind_h = torch.linspace(0 ,479 , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    render_d_ind_w= torch.linspace(0 ,639 , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "\n",
    "    render_d_ind_h = torch.clamp((render_d_ind_h - u_map).type(torch.float64) ,0,479).type( torch.long )[flow_mask]\n",
    "    render_d_ind_w = torch.clamp((render_d_ind_w - v_map).type(torch.float32),0,639).type( torch.long )[flow_mask] \n",
    "    index = render_d_ind_h*640 + render_d_ind_w # hacky indexing along two dimensions\n",
    "\n",
    "    ren_d_masked  = render_d.flatten()[index]\n",
    "\n",
    "    # Project depth map to the pointcloud real\n",
    "    cam_scale = 10000\n",
    "\n",
    "    real_pixels = torch.stack( [grid_real_w[flow_mask], grid_real_h[flow_mask], torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)[flow_mask]], dim=1 ).type(u_map.dtype)\n",
    "    K_inv = torch.inverse(K_real.type(torch.float64))\n",
    "    P_real = K_inv @ real_pixels.T.type(torch.float64)\n",
    "    P_real = P_real.type(torch.float64) * real_d[flow_mask] / cam_scale\n",
    "    P_real = P_real.T\n",
    "\n",
    "    # Project depth map to the pointcloud render\n",
    "    K_ren_inv = torch.inverse(K_ren.type(torch.float64)).type(torch.float64)\n",
    "    ren_pixels = torch.stack( [grid_ren_w[flow_mask] - v_map[flow_mask], \n",
    "                            grid_ren_h[flow_mask] - u_map[flow_mask],\n",
    "                            torch.ones(grid_ren_h.shape, device = u_map.device,  dtype=torch.float64 )[flow_mask]], \n",
    "                            dim=1 ).type(torch.float64)\n",
    "\n",
    "    print(K_ren_inv, K_inv)\n",
    "    cam_scale = 10000\n",
    "    P_ren = K_ren_inv @ ren_pixels.T\n",
    "    P_ren = P_ren * ren_d_masked / cam_scale\n",
    "    P_ren = P_ren.T\n",
    "\n",
    "\n",
    "    # m_ren = filter_pcd( P_ren)\n",
    "    # min_points = 50\n",
    "    # if torch.sum(m_ren) < min_points:\n",
    "    #     print(f'Violation filter_pcd: P_in: { P_ren_in_center.shape[0]} P_out: {torch.sum(m_tot)}')\n",
    "\n",
    "\n",
    "    # P_real = P_real[m_ren]\n",
    "    # P_ren = P_ren[m_ren]\n",
    "    real_pixels_fil = real_pixels #[m_ren]\n",
    "\n",
    "\n",
    "    grid_ren_h = torch.linspace(int(ren_tl[0]) ,int(ren_br[0]), 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_ren_w = torch.linspace(int(ren_tl[1]) ,int(ren_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    crop_d_pixels = torch.stack( [grid_ren_w.flatten(), grid_ren_h.flatten(), torch.ones(grid_ren_w.shape, device = u_map.device,  dtype= u_map.dtype).flatten()], dim=1 ).type(u_map.dtype)\n",
    "    K_inv = torch.inverse(K_ren.type(torch.float64)).type(u_map.dtype)\n",
    "\n",
    "    P_crop_d = K_inv @ crop_d_pixels.T.type(u_map.dtype)\n",
    "    P_crop_d = P_crop_d.type(u_map.dtype) * render_d.flatten() / cam_scale\n",
    "    P_crop_d = P_crop_d.T\n",
    "    P_crop_d  = P_crop_d[index] \n",
    "    m = filter_pcd( P_crop_d)\n",
    "    P_crop_d  = P_crop_d[ m ]\n",
    "    P_real = P_real[m]\n",
    "\n",
    "    P_ren = P_crop_d\n",
    "    print(P_ren.shape, P_real.shape,index.shape)\n",
    "\n",
    "    # random shuffel\n",
    "\n",
    "    pts_trafo = min(P_real.shape[0], 50000)\n",
    "    idx = torch.randperm( P_real.shape[0] )[0:pts_trafo]\n",
    "    P_real = P_real[idx]\n",
    "    P_ren = P_ren[idx]\n",
    "    real_pixels_fil = real_pixels_fil[idx]\n",
    "    \n",
    "#     #################################################################\n",
    "#     ################# Evaluate Depth Map ############################\n",
    "     \n",
    "#     adds = SingleObjectADDLoss()\n",
    "    \n",
    "#     P_ren_h2 = get_H(P_ren.clone())\n",
    "#     P_ren_h_trafo2 = P_ren_h2 @ torch.inverse(h_render.clone()).T\n",
    "#     a = torch.abs(P_ren_h_trafo2[:,:3])\n",
    "#     m1 = a < 0.1\n",
    "#     m1 = torch.sum(m1.type(torch.float32), dim=1 ) > 2\n",
    "#     P_ren_h_trafo2 = P_ren_h_trafo2[m1]\n",
    "    \n",
    "    \n",
    "#     #ken_adds = adds.pcd(P_ren_h_trafo2[:,:3], mp)\n",
    "    \n",
    "#     pts = min( P_ren_h_trafo2.shape[0], 1000)\n",
    "#     idx_de = torch.randperm(  P_ren_h_trafo2.shape[0] )[0:pts]\n",
    "#     idx_mp = torch.randperm( pts )[0:pts]\n",
    "#     id_sym = exp['d_train']['obj_list_sym'][0]\n",
    "#     ids = torch.tensor([[id_sym]])\n",
    "#     st = time.time()\n",
    "#     adds_error = criterion_adds(P_ren_h_trafo2[idx_de][:,:3][None].clone(), mp[idx_mp][None].clone(), idx=ids, H = torch.eye(4)[None])\n",
    "#     ids = torch.tensor([[999]])\n",
    "#     st2 = time.time()\n",
    "    \n",
    "#     print(F'adds_error {adds_error} s1 {time.time()-st}, s2 {time.time()-st2}')\n",
    "    \n",
    "#     if adds_error > 0.02:\n",
    "#         plot_two_pcd_line(P_ren_h_trafo2[idx_de][:,:3].numpy(), mp[idx_mp].clone().numpy() )\n",
    "    \n",
    "#     ##################################################################\n",
    "\n",
    "    # random shuffel\n",
    " \n",
    "    \n",
    "    sub = int(P_ren.shape[0]/200)\n",
    "    tar = get_H(mp)\n",
    "    tar = (tar @ h_render.T)[::5,:3]\n",
    "    print(\"Check alignment betweem rendered pointcloud and the cad model rotated to the pose of the PCD\")\n",
    "    plot_two_pcd(P_ren[::sub,:3].numpy(), tar.numpy())\n",
    "    \n",
    "    # The same filtering is performed for the PnP as for the Pointcloud matching approach! \n",
    "    \n",
    "    \n",
    "    # Transform the known rendered point corrspondences to the origin frame. \n",
    "    P_ren_h = get_H(P_ren)\n",
    "    P_ren_h_trafo = P_ren_h.clone() @ torch.inverse(h_render).T\n",
    "    \n",
    "    \n",
    "    mp_real = get_H(mp)\n",
    "    mp_real = (mp_real @ h_real.T)[::5,:3]\n",
    "\n",
    "    # PNP estimation\n",
    "    objectPoints = P_ren_h_trafo[:,:3].clone().numpy() #P_ren.numpy()    \n",
    "    imagePoints = real_pixels_fil[:,:2].numpy()\n",
    "    dist = np.array( [[0.0,0.0,0.0,0.0]] )\n",
    "    div = 1 # int(objectPoints.shape[0]/1000)\n",
    "\n",
    "    if objectPoints.shape[0] < 8:\n",
    "        print('Failed due to missing corsspondences')\n",
    "    # set current guess as the inital estimate\n",
    "\n",
    "    rvec = R.from_matrix(h_real_est[:3,:3]).as_rotvec().astype(np.float32)\n",
    "    tvec = h_real_est[:3,3].numpy().astype(np.float32)\n",
    "    # calculate PnP between the pixels coordinates in the real image and the corrosponding points in the origin frame\n",
    "    r_vec2, t_vec2 = cv2.solvePnPRefineLM(copy.deepcopy(objectPoints), \\\n",
    "        copy.deepcopy(imagePoints), \n",
    "        K_real.numpy(), \n",
    "        dist, \n",
    "        copy.deepcopy(rvec),\n",
    "        copy.deepcopy( tvec))\n",
    "    r_vec2 = r_vec2[:,None]\n",
    "    h = rvec_tvec_to_H(r_vec2[:,0],t_vec2)\n",
    "    print(\"H predicted \\n\",h, \"\\n H real \\n\", h_real)\n",
    "    P_ren_h_trafo_out = P_ren_h_trafo @ h.T\n",
    "    sub = int(P_ren_h_trafo_out.shape[0]/500 )\n",
    "    plot_two_pcd_line(P_ren_h_trafo_out[::sub,:3].numpy(), mp_real.clone().numpy() )\n",
    "    \n",
    "#     # plotting\n",
    "#     img = np.uint8(img.clone().numpy())\n",
    "#     for p in range(imagePoints.shape[0]):\n",
    "        \n",
    "#         u = min(479,int(imagePoints[p,1]) )\n",
    "#         v = min(639,int(imagePoints[p,0]) )\n",
    "#         img[u,v, :] = np.array([138,43,226])\n",
    "    \n",
    "#     Drawer().disp_img_1d(img, hold=False )\n",
    "#     visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#         epoch = 1,\n",
    "#         img= img,\n",
    "#         points = P_ren_h_trafo_out[:2,:3],\n",
    "#         store = False,\n",
    "#         jupyter= True,\n",
    "#         K = K_real.cpu().numpy(),\n",
    "#         H = np.eye(4),\n",
    "#         method='right')\n",
    "    \n",
    "#     visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#         epoch = 1,\n",
    "#         img= img,\n",
    "#         points = P_ren_h_trafo_out[:,:3],\n",
    "#         store = False,\n",
    "#         jupyter= True,\n",
    "#         K = K_real.cpu().numpy(),\n",
    "#         H = np.eye(4),\n",
    "#         method='left')\n",
    "\n",
    "    \n",
    "    \n",
    "    return True, torch.from_numpy( h).type(u_map.dtype) \n",
    "# print(model_points[0].shape, real_img.shape)\n",
    "\n",
    "flow_to_trafo_pnp(\n",
    "    real_br = real_br[b].clone(),\n",
    "    real_tl = (real_tl[b]).clone(), \n",
    "    ren_br = (ren_br[b]).clone(), \n",
    "    ren_tl = (ren_tl[b]).clone(),\n",
    "    flow_mask = (flow_mask[b]).clone(), \n",
    "    u_map = (u_map[b].type( typ )).clone(),\n",
    "    v_map = (v_map[b].type( typ )).clone(), \n",
    "    K_real = (K_real.type( typ )).clone(),\n",
    "    K_ren = (K_ren.type( typ )).clone(),\n",
    "    real_d = (real_d[b].type( typ )).clone(),\n",
    "    render_d = (render_d[b].type( typ )).clone(),\n",
    "    h_real = (h_real[b].type( typ )).clone(), \n",
    "    h_render = (h_render[b].type( typ )).clone(),\n",
    "    h_real_est = h_real_est,\n",
    "    mp = model_points[0],\n",
    "    img = real_img_original[0].clone())\n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from visu import plot_two_pcd\n",
    "\n",
    "import pandas as pd\n",
    "# idxmax can be set to != -1 to load specific sample\n",
    "\n",
    "idxmax = -1 \n",
    "device = 'cpu'\n",
    "desig_ls= []\n",
    "adds_gt_ls = []\n",
    "adds_init_ls = []\n",
    "adds_erode_ls = []\n",
    "adds_n1_ls = []\n",
    "adds_n2_ls = []\n",
    "\n",
    "adds_pnp_gt_ls = []\n",
    "adds_pnp_ero_ls = []\n",
    "\n",
    "failed_desig_ls = []\n",
    "visualizer = Visualizer('/home/jonfrey/Debug', None)\n",
    "print('START')\n",
    "\n",
    "K_ren = torch.tensor( dataset_train._backend.get_camera('data_syn/0019', K=True), device=device ) \n",
    "K_ren = torch.tensor( dataset_train._backend.get_camera('data/0049', K=True), device=device ) \n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "if idxmax != -1:\n",
    "    print(idxmax, \"is not equal to -1 load batch\")\n",
    "    batch = next(itertools.islice(dataloader_train, idxmax, None))[0]\n",
    "cou = 0\n",
    "print(\"START\")\n",
    "names = ['ID','ADD-S GT','ADD-S INITAL','ADD-S Eroded Mask', 'ADD-S Random', 'ADD-S Bias', 'ADD-S PNP GT', 'ADD-S PNP Ero']\n",
    "df = pd.DataFrame(columns=names)\n",
    "        \n",
    "for j,ba in enumerate(dataloader_test):\n",
    "    \n",
    "    if j > max_iter-1:\n",
    "        break\n",
    "    if j % 10 == 0:\n",
    "        print(f\"Processed {j}/{max_iter}\")\n",
    "\n",
    "    if idxmax == -1:  \n",
    "        batch = ba[0]\n",
    "    # use first sample in batch \n",
    "    b = 0\n",
    "    \n",
    "    model_points = batch[4]\n",
    "    idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "    label = batch[7]\n",
    "    real_img_original = batch[8]\n",
    "    cam = batch[9]\n",
    "    gt_rot_wxyz, gt_trans, unique_desig = batch[10:13] # unique_desig[1] contains the idx starting at 1 for the first object \n",
    "    bs = model_points.shape[0]\n",
    "    if batch[13] is False:\n",
    "        print('Continue')\n",
    "        continue\n",
    "    real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "    pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "    u_map, v_map, flow_mask, bb, depth_render_original= batch[24:]\n",
    "    real_tl, real_br, ren_tl, ren_br = bb \n",
    "    \n",
    "    data = torch.cat([real_img, render_img], dim=1)\n",
    "    uv_gt = torch.stack( [u_map, v_map], dim=3 ).permute(0,3,1,2)\n",
    "    \n",
    "    \n",
    "    ero_in = (gt_label_cropped ==  unique_desig[1])[:,None,:,:].type(torch.float32) # BS,C,H,W\n",
    "    t_size = get_scale_for_erosion(ero_in)\n",
    "    ero_out = eroision_batch(ero_in,t_size)\n",
    "    \n",
    "    # get camera\n",
    "    K_real = torch.tensor( [[cam[b,2],0,cam[b,0]],[b,cam[b,3],cam[b,1]],[0,0,1]], device=device )\n",
    "    \n",
    "    #get inital estimate of the poistion given by the dataloader\n",
    "    h_real_est = torch.eye(4,device=device)\n",
    "    h_real_est[:3,:3] = quat_to_rot(pred_rot_wxyz[b][None,:], conv='wxyz', device=device)\n",
    "    h_real_est[:3,3] = torch.tensor( pred_trans[b].clone().detach() ,device=device )\n",
    "    \n",
    "    #print(h_render)\n",
    "    #break\n",
    "    \n",
    "    ### GT Estimate ###\n",
    "    typ = u_map.dtype\n",
    "    fmt = flow_mask.dtype\n",
    "    flow_mask_eroded  = (flow_mask * ero_out.type(torch.float32)).type(fmt)[:,0]\n",
    "    suc = True\n",
    "    suc_, pnp_h_gt = pnp(\n",
    "        real_br = real_br[b].clone(),\n",
    "        real_tl = (real_tl[b]).clone(), \n",
    "        ren_br = (ren_br[b]).clone(), \n",
    "        ren_tl = (ren_tl[b]).clone(),\n",
    "        flow_mask = (flow_mask[b]).clone(), \n",
    "        u_map = (u_map[b].type( typ )).clone(),\n",
    "        v_map = (v_map[b].type( typ )).clone(), \n",
    "        K_real = (K_real.type( typ )).clone(),\n",
    "        K_ren = (K_ren.type( typ )).clone(),\n",
    "        real_d = (real_d[b].type( typ )).clone(),\n",
    "        render_d = (render_d[b].type( typ )).clone(),\n",
    "        h_real = (h_real[b].type( typ )).clone(), \n",
    "        h_render = (h_render[b].type( typ )).clone(),\n",
    "        h_real_est = h_real_est,\n",
    "        mp = model_points[0].clone(),\n",
    "        img = real_img_original[0].clone())\n",
    "    ran_suc = suc_\n",
    "    suc = suc_ and suc\n",
    "    #TODO number for pnp-h-ero is faked\n",
    "    pnp_h_ero = pnp_h_gt\n",
    "#     suc_, pnp_h_ero = flow_to_trafo_pnp(\n",
    "#         real_br = real_br[b].clone(),\n",
    "#         real_tl = (real_tl[b]).clone(), \n",
    "#         ren_br = (ren_br[b]).clone(), \n",
    "#         ren_tl = (ren_tl[b]).clone(),\n",
    "#         flow_mask = (flow_mask_eroded[b]).clone(), \n",
    "#         u_map = (u_map[b].type( typ )).clone(),\n",
    "#         v_map = (v_map[b].type( typ )).clone(), \n",
    "#         K_real = (K_real.type( typ )).clone(),\n",
    "#         K_ren = (K_ren.type( typ )).clone(),\n",
    "#         real_d = (real_d[b].type( typ )).clone(),\n",
    "#         render_d = (render_d[b].type( typ )).clone(),\n",
    "#         h_real = (h_real[b].type( typ )).clone(), \n",
    "#         h_render = (h_render[b].type( typ )).clone(),\n",
    "#         h_real_est = h_real_est,\n",
    "#         mp = model_points[0].clone() )\n",
    "#     ran_suc = ran_suc and suc_\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    \n",
    "     ### GT Label ###\n",
    "    suc_, P_real_in_center, P_ren_in_center, P_real_trafo, gt_T_res = flow_to_trafo(\n",
    "        real_br = real_br[b].clone(),\n",
    "        real_tl = (real_tl[b]).clone(), \n",
    "        ren_br = (ren_br[b]).clone(), \n",
    "        ren_tl = (ren_tl[b]).clone(),\n",
    "        flow_mask = (flow_mask[b]).clone(), \n",
    "        u_map = (u_map[b].type( typ )).clone(),\n",
    "        v_map = (v_map[b].type( typ )).clone(), \n",
    "        K_real = (K_real.type( typ )).clone(),\n",
    "        K_ren = (K_ren.type( typ )).clone(),\n",
    "        real_d = (real_d[b].type( typ )).clone(),\n",
    "        render_d = (render_d[b].type( typ )).clone(),\n",
    "        h_real = (h_real_est.type( typ )).clone(), \n",
    "        h_render = (h_render[b].type( typ )).clone())\n",
    "    gt_h_est =  gt_T_res @ h_render[0].type(typ)\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    ### Erode Label ###\n",
    "    \n",
    "    suc_,_,_,_, ero_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask_eroded[b]), \n",
    "        u_map = copy.deepcopy((u_map[b]).type( typ )), \n",
    "        v_map = copy.deepcopy((v_map[b]).type( typ )), \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    ero_h_est =  ero_T_res @ h_render[0].type(typ) # set rotation\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    \n",
    "    ### N1 ###\n",
    "    lvl = 20\n",
    "    n1 = torch.rand(v_map[b].shape, dtype = typ)\n",
    "    n2 = torch.rand(v_map[b].shape, dtype = typ)\n",
    "    u_map_clone = (u_map[b]).type( typ ).clone() + (n1 - 0.5) * 2 * lvl\n",
    "    v_map_clone = (v_map[b]).type( typ ).clone() + (n2 - 0.5) * 2 * lvl\n",
    "    \n",
    "    suc_,_,_,_, n1_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask_eroded[b]), \n",
    "        u_map = u_map_clone, \n",
    "        v_map = v_map_clone, \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    n1_h_est =  n1_T_res @ h_render[0].type(typ) # set rotation\n",
    "    suc = suc_ and suc\n",
    "    ### N2 ###\n",
    "    lvl = 20\n",
    "    n1 = float(torch.rand((1), dtype = typ ))\n",
    "    n2 = float(torch.rand((1), dtype = typ ))\n",
    "    u_map_clone = (u_map[b]).type( typ ).clone() + float( (n1 - 0.5) * 2 * lvl )\n",
    "    v_map_clone = (v_map[b]).type( typ ).clone() + float( (n2 - 0.5) * 2 * lvl )\n",
    "    \n",
    "    \n",
    "    suc_,_,_,_, n2_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask_eroded[b]), \n",
    "        u_map = u_map_clone, \n",
    "        v_map = v_map_clone, \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    n2_h_est =  n2_T_res @ h_render[0].type(typ) # set rotation\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    if not ran_suc:\n",
    "        failed_desig_ls.append( int( unique_desig[1]) )\n",
    "        print(f\"Ransac failed for {int( unique_desig[1])} {unique_desig[0]}\")\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    mask = (flow_mask == True)\n",
    "    # Target Model-points Transformed\n",
    "    p = model_points.shape[1]\n",
    "    target = torch.bmm( model_points, torch.transpose(h_real[:,:3,:3], 1,2 ) ) + h_real[:,:3,3][:,None,:].repeat(1,p,1)\n",
    "\n",
    "    # Compute ADD-S\n",
    "    adds_res_gt_flow = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = gt_h_est[None].type( target.dtype) )\n",
    "    adds_res_gt_flow_eroded = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = ero_h_est[None].type( target.dtype) )\n",
    "    adds_res_n1 = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = n1_h_est[None].type( target.dtype) )\n",
    "    adds_res_n2 = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = n2_h_est[None].type( target.dtype) )\n",
    "    adds_res_pnp_gt = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = pnp_h_gt[None].type( target.dtype) )\n",
    "    adds_res_pnp_ero = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = pnp_h_ero[None].type( target.dtype) )\n",
    "    adds_init = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = h_real_est[None].type( target.dtype))\n",
    "    \n",
    "    \n",
    "    v1,v2,v3 = float(adds_res_gt_flow.detach()),float(adds_init.detach()), float(adds_res_gt_flow_eroded.detach())\n",
    "    v4,v5,v6,v7 = float(adds_res_n1.detach()),float(adds_res_n2.detach()),float(adds_res_pnp_gt.detach()), float(adds_res_pnp_ero.detach())\n",
    "    \n",
    "    test_values = [int( unique_desig[1]),v1,v2,v3,v4,v5,v6,v7]\n",
    "    res = {names[i]: test_values[i] for i in range(len(names))} \n",
    "    df = df.append(res, ignore_index=True)\n",
    "        \n",
    "    \n",
    "\n",
    "    # Append results to list\n",
    "    desig_ls.append(int( unique_desig[1]) )\n",
    "    adds_init_ls.append(float(adds_init.detach()))\n",
    "    adds_gt_ls.append(float(adds_res_gt_flow.detach()))\n",
    "    adds_erode_ls.append(float(adds_res_gt_flow_eroded.detach()))    \n",
    "    adds_n1_ls.append(float(adds_res_n1.detach()))\n",
    "    adds_n2_ls.append(float(adds_res_n2.detach()))\n",
    "    adds_pnp_gt_ls.append(float(adds_res_pnp_gt.detach()))\n",
    "    adds_pnp_ero_ls.append(float(adds_res_pnp_ero.detach()))\n",
    "\n",
    "#     if adds_gt_ls[-1] > 0.02 :\n",
    "#         print(f'Failed Iteration {j} > 0.02 ADD-S, GT', adds_gt_ls[-1] , 'ERRODED', adds_erode_ls[-1]  )\n",
    "        \n",
    "#         no = torch.norm(P_real_trafo- P_ren_in_center, dim=1)\n",
    "#         print( f'    shape of use points: ',P_real_trafo.shape )\n",
    "#         print( f'    number of point with a distance greater 0.2 {torch.sum( no>0.2 )} ' )\n",
    "#         print( f'    number of point with a distance greater 0.02 {torch.sum( no>0.02 )} ' )\n",
    "#         print( f'    number of point with a distance smaller 0.02 {torch.sum( no<0.02 )} ' )\n",
    "        \n",
    "        \n",
    "#         plot1 = True\n",
    "#     else:\n",
    "#         print(f'Suc Iteration {j} < 0.02 ADD-S, GT', adds_gt_ls[-1] , 'ERRODED', adds_erode_ls[-1]  )\n",
    "#         plot1 = False\n",
    "#     if idxmax != -1:\n",
    "#         plot1 = True\n",
    "        \n",
    "    \n",
    "    print(f'Not Iteration {j} < 0.02 ADD-S, GT', adds_gt_ls[-1] , 'ERRODED', adds_erode_ls[-1],'PnP', adds_pnp_gt_ls[-1]   )\n",
    "        \n",
    "    if adds_pnp_gt_ls[-1] > 0.02 :\n",
    "        cou += 1\n",
    "#         if cou > 5:\n",
    "#             break\n",
    "#         continue\n",
    "#         print(\"Real depth map cropped\")\n",
    "        real_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "#        real_depth_img = Drawer().disp_img_1d(depth_render_original[0][0].numpy(),ret=True)\n",
    "#         real_depth_img = np.repeat( real_depth_img[:,:,None],3, axis=2)\n",
    "#         print(\"Render depth map cropped\")\n",
    "#         render_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "#         render_depth_img = np.repeat( render_depth_img[:,:,None],3, axis=2)\n",
    "#         Drawer().disp_img_1d(flow_mask[0],ret=True)\n",
    "\n",
    "#         sub = max(1,int( P_real_in_center.shape[0]/100 ) )\n",
    "#         plot_two_pcd_line(P_real_in_center[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "#         plot_two_pcd_line(P_real_trafo[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "\n",
    "#         print(f\"Real Image, Estimated Points given GT Flow {P_real_in_center.shape}\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_img_original[b].cpu().numpy(),\n",
    "#                         points =copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = h_real[0].numpy(),\n",
    "#                         method='def')\n",
    "#         print(\"Real Image Cropped, Estimated Points given GT Flow\")\n",
    "#         visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_depth_img,\n",
    "#                         tl = real_tl[0],\n",
    "#                         br = real_br[0],\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "\n",
    "#         fil = label == unique_desig[1]    \n",
    "#         real_img_original[b][ fil[0][:,:,None].repeat(1,1,3) ] = 255\n",
    "#         print(\"Real Image, Estimated Points given GT Flow with label is white\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_img_original[b].cpu().numpy(),\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"GT: Render Image Cropped\")\n",
    "        visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= render_img[0].numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            tl = ren_tl[0],\n",
    "                            br = ren_br[0],\n",
    "                            store = False,\n",
    "                            jupyter=True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = h_render[0].numpy(),\n",
    "                            method='def')\n",
    "        print(\"PNP, Real Image\")\n",
    "        visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= real_img_original[b].cpu().numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            store = False,\n",
    "                            jupyter= True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = pnp_h_gt.clone().numpy(),\n",
    "                            method='def')\n",
    "        \n",
    "        print(\"PNP: Render Image Cropped\")\n",
    "        visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= render_img[0].numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            tl = ren_tl[0],\n",
    "                            br = ren_br[0],\n",
    "                            store = False,\n",
    "                            jupyter=True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = h_render[0].clone().numpy(),\n",
    "                            method='def')\n",
    "\n",
    "#         print('Valid Flow Mask')\n",
    "#         visualizer.plot_segmentation('tag', 1, flow_mask_eroded[b] , store=False, method='def', jupyter=True)\n",
    "#         visualizer.plot_segmentation('tag', 1, flow_mask[b]  , store=False, method='def', jupyter=True)\n",
    "\n",
    "        print(\"Corrospondence\")\n",
    "        visualizer.plot_corrospondence(tag=f'_',\n",
    "                                           epoch=0,\n",
    "                                            u_map=u_map[0], \n",
    "                                            v_map=v_map[0], \n",
    "                                            flow_mask=flow_mask[0], \n",
    "                                            real_img=real_img[0], \n",
    "                                            render_img=render_img[0],\n",
    "                                            store=False,\n",
    "                                            jupyter=True,\n",
    "                                            coloful=True,\n",
    "                                            method='def',\n",
    "                                            res_h=10,\n",
    "                                            res_w=10)\n",
    "        \n",
    "         \n",
    "    #if float(adds_res_pnp_gt.detach()) > 0.02 :\n",
    "     #   break\n",
    "# print(f'Result, GT Avg', sum(adds_gt_ls)/len(adds_gt_ls), 'Errode Avg', sum(adds_erode_ls)/len(adds_erode_ls) )\n",
    "print(\"Finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnp(*args, **kwargs):\n",
    "    real_br = kwargs['real_br']\n",
    "    real_tl = kwargs['real_tl']\n",
    "    ren_br = kwargs['ren_br']\n",
    "    ren_tl = kwargs['ren_tl']\n",
    "    flow_mask = kwargs['flow_mask']\n",
    "    u_map = kwargs['u_map']\n",
    "    v_map = kwargs['v_map']\n",
    "    K_real = kwargs['K_real']\n",
    "    K_ren = kwargs['K_ren']\n",
    "    real_d = kwargs['real_d']\n",
    "    render_d = kwargs['render_d']\n",
    "    h_real = kwargs['h_real']\n",
    "    h_render = kwargs['h_render']\n",
    "    h_real_est = kwargs['h_real_est']\n",
    "    mp = kwargs['mp']\n",
    "    img = kwargs['img']\n",
    "    \n",
    "    typ = u_map.dtype\n",
    "\n",
    "    # Grid for upsampled real\n",
    "    grid_real_h = torch.linspace(int(real_tl[0]) ,int(real_br[0]) , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_real_w = torch.linspace(int(real_tl[1]) ,int(real_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    # Project depth map to the pointcloud real\n",
    "    cam_scale = 10000\n",
    "    real_pixels = torch.stack( [grid_real_w[flow_mask], grid_real_h[flow_mask], torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)[flow_mask]], dim=1 ).type(typ)\n",
    "    K_inv = torch.inverse(K_real.type(torch.float64)).type(typ)\n",
    "    P_real = K_inv @ real_pixels.T\n",
    "    P_real = P_real.type(torch.float64) * real_d[flow_mask] / cam_scale\n",
    "    P_real = P_real.T\n",
    "\n",
    "\n",
    "    real_pixels_fil = real_pixels\n",
    "\n",
    "\n",
    "    grid_ren_h = torch.linspace(int(ren_tl[0]) ,int(ren_br[0]), 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    grid_ren_w = torch.linspace(int(ren_tl[1]) ,int(ren_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    crop_d_pixels = torch.stack( [grid_ren_w.flatten(), grid_ren_h.flatten(), torch.ones(grid_ren_w.shape, device = u_map.device,  dtype= torch.float64).flatten()], dim=1 ).type(typ)\n",
    "    K_inv = torch.inverse(K_ren.type(torch.float64)).type(typ)\n",
    "    P_crop_d = K_inv @ crop_d_pixels.T.type(typ)\n",
    "    P_crop_d = P_crop_d.type(torch.float64) * render_d.flatten() / cam_scale\n",
    "    P_crop_d = P_crop_d.T\n",
    "\n",
    "\n",
    "    render_d_ind_h = torch.linspace(0 ,479 , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "    render_d_ind_w= torch.linspace(0 ,639 , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "    render_d_ind_h = torch.clamp((render_d_ind_h - u_map).type(torch.float64) ,0,479).type( torch.long )[flow_mask]\n",
    "    render_d_ind_w = torch.clamp((render_d_ind_w - v_map).type(torch.float32),0,639).type( torch.long )[flow_mask] \n",
    "    index = render_d_ind_h*640 + render_d_ind_w # hacky indexing along two dimensions\n",
    "\n",
    "    P_crop_d  = P_crop_d[index] \n",
    "    crop_d_pixels = crop_d_pixels[index] \n",
    "\n",
    "\n",
    "    m = filter_pcd( P_crop_d)\n",
    "    P_crop_d  = P_crop_d[ m ]\n",
    "    P_real = P_real[m]\n",
    "    real_pixels = real_pixels[m]\n",
    "    \n",
    "    P_ren = P_crop_d\n",
    "\n",
    "    # random shuffel\n",
    "    # pts_trafo = min(P_real.shape[0], 50000)\n",
    "    # idx = torch.randperm( P_real.shape[0] )[0:pts_trafo]\n",
    "    # P_real = P_real[idx]\n",
    "    # P_ren = P_ren[idx]\n",
    "    # real_pixels_fil = crop_d_pixels[idx]\n",
    "\n",
    "    nr = 10000\n",
    "    mp_render = get_H(mp)\n",
    "    mp_render = (mp_render.type(typ)@ h_render.type(typ).T)[::5,:3]\n",
    "    mp_real = get_H(mp)\n",
    "    mp_real = (mp_real.type(typ) @ h_real.type(typ).T)[::5,:3]\n",
    "\n",
    "#     visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "#         epoch = 1,\n",
    "#         img= render_img[0].numpy()  ,\n",
    "#         tl = ren_tl,\n",
    "#         br = ren_br,\n",
    "#         points = P_ren[:nr,:3],\n",
    "#         store = False,\n",
    "#         jupyter= True,\n",
    "#         K = K_ren.cpu().numpy(),\n",
    "#         H = np.eye(4),\n",
    "#         method='def')\n",
    "\n",
    "#     plot_two_pcd(mp_render.numpy(), P_ren[:nr,:3].numpy())\n",
    "\n",
    "#     #inp = render_img[0].clone()\n",
    "#     inp = render_img_original[0].clone()\n",
    "#     inp = real_img_original[0].clone()\n",
    "#     for j in range(0,nr):\n",
    "#         try:\n",
    "\n",
    "#             v = int(real_pixels[j,0])\n",
    "#             u = int(real_pixels[j,1])\n",
    "#             inp[u:u+4,v:v+4,: ] = torch.tensor( [255,0,255])\n",
    "#         except:\n",
    "#             pass\n",
    "#     display( Image.fromarray( np.uint8(inp.numpy())) )\n",
    "\n",
    "    P_ren_in_origin =  (get_H( P_ren ).type(typ) @ torch.inverse( h_render.type(torch.float64) ).type(typ).T) [:,:3]\n",
    "\n",
    "    # PNP estimation\n",
    "    objectPoints = P_ren_in_origin.clone().numpy()    \n",
    "    imagePoints = real_pixels[:,:2].numpy()\n",
    "    dist = np.array( [[0.0,0.0,0.0,0.0]] )\n",
    "\n",
    "    if objectPoints.shape[0] < 8:\n",
    "        print('Failed due to missing corsspondences')\n",
    "    # set current guess as the inital estimate\n",
    "\n",
    "    rvec = R.from_matrix(h_real_est[:3,:3]).as_rotvec().astype(np.float32)\n",
    "    tvec = h_real_est[:3,3].numpy().astype(np.float32)\n",
    "    # calculate PnP between the pixels coordinates in the real image and the corrosponding points in the origin frame\n",
    "    r_vec2, t_vec2 = cv2.solvePnPRefineLM(copy.deepcopy(objectPoints), \\\n",
    "        copy.deepcopy(imagePoints), \n",
    "        K_real.numpy(), \n",
    "        dist, \n",
    "        copy.deepcopy(rvec),\n",
    "        copy.deepcopy( tvec))\n",
    "    r_vec2 = r_vec2[:,None]\n",
    "\n",
    "    h = rvec_tvec_to_H(r_vec2[:,0],t_vec2)\n",
    "    return True,torch.from_numpy( h).type(u_map.dtype) \n",
    "\n",
    "\n",
    "h = pnp(real_br = real_br[b].clone(),\n",
    "    real_tl = (real_tl[b]).clone() ,\n",
    "    ren_br = (ren_br[b]).clone(),\n",
    "    ren_tl = (ren_tl[b]).clone(),\n",
    "    flow_mask = (flow_mask[b]).clone(),\n",
    "    u_map = (u_map[b].type( typ )).clone(),\n",
    "    v_map = (v_map[b].type( typ )).clone(), \n",
    "    K_real = (K_real.type( typ )).clone(),\n",
    "    K_ren = (K_ren.type( typ )).clone(),\n",
    "    real_d = (real_d[b].type( typ )).clone(),\n",
    "    render_d = (render_d[b].type( typ )).clone(),\n",
    "    h_real = (h_real[b].type(typ )).clone(),\n",
    "    h_render = (h_render[b].type( typ )).clone(),\n",
    "    h_real_est = h_real_est,\n",
    "    mp = model_points[0],\n",
    "    img = real_img_original[0].clone() )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clones once\n",
    "typ = torch.float64\n",
    "real_br = real_br[b].clone()\n",
    "real_tl = (real_tl[b]).clone() \n",
    "ren_br = (ren_br[b]).clone()\n",
    "ren_tl = (ren_tl[b]).clone()\n",
    "flow_mask = (flow_mask[b]).clone()\n",
    "u_map = (u_map[b].type( typ )).clone()\n",
    "v_map = (v_map[b].type( typ )).clone() \n",
    "K_real = (K_real.type( typ )).clone()\n",
    "K_ren = (K_ren.type( typ )).clone()\n",
    "real_d = (real_d[b].type( typ )).clone()\n",
    "render_d = (render_d[b].type( typ )).clone()\n",
    "h_real = (h_real[b].type(typ )).clone()\n",
    "h_render = (h_render[b].type( typ )).clone()\n",
    "h_real_est = h_real_est\n",
    "mp = model_points[0]\n",
    "img = real_img_original[0].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grid for upsampled real\n",
    "grid_real_h = torch.linspace(int(real_tl[0]) ,int(real_br[0]) , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "grid_real_w = torch.linspace(int(real_tl[1]) ,int(real_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "# Project depth map to the pointcloud real\n",
    "cam_scale = 10000\n",
    "real_pixels = torch.stack( [grid_real_w[flow_mask], grid_real_h[flow_mask], torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)[flow_mask]], dim=1 ).type(u_map.dtype)\n",
    "K_inv = torch.inverse(K_real.type(torch.float64))\n",
    "P_real = K_inv @ real_pixels.T.type(torch.float64)\n",
    "P_real = P_real.type(torch.float64) * real_d[flow_mask] / cam_scale\n",
    "P_real = P_real.T\n",
    "\n",
    "\n",
    "real_pixels_fil = real_pixels\n",
    "\n",
    "\n",
    "grid_ren_h = torch.linspace(int(ren_tl[0]) ,int(ren_br[0]), 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "grid_ren_w = torch.linspace(int(ren_tl[1]) ,int(ren_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "crop_d_pixels = torch.stack( [grid_ren_w.flatten(), grid_ren_h.flatten(), torch.ones(grid_ren_w.shape, device = u_map.device,  dtype= torch.float64).flatten()], dim=1 ).type(torch.float64)\n",
    "K_inv = torch.inverse(K_ren.type(torch.float64))\n",
    "P_crop_d = K_inv @ crop_d_pixels.T.type(torch.float64)\n",
    "P_crop_d = P_crop_d.type(torch.float64) * render_d.flatten() / cam_scale\n",
    "P_crop_d = P_crop_d.T\n",
    "\n",
    "\n",
    "\n",
    "render_d_ind_h = torch.linspace(0 ,479 , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "render_d_ind_w= torch.linspace(0 ,639 , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "render_d_ind_h = torch.clamp((render_d_ind_h - u_map).type(torch.float64) ,0,479).type( torch.long )[flow_mask]\n",
    "render_d_ind_w = torch.clamp((render_d_ind_w - v_map).type(torch.float32),0,639).type( torch.long )[flow_mask] \n",
    "index = render_d_ind_h*640 + render_d_ind_w # hacky indexing along two dimensions\n",
    "\n",
    "P_crop_d  = P_crop_d[index] \n",
    "crop_d_pixels = crop_d_pixels[index] \n",
    "\n",
    "\n",
    "m = filter_pcd( P_crop_d)\n",
    "P_crop_d  = P_crop_d[ m ]\n",
    "P_real = P_real[m]\n",
    "real_pixels = real_pixels[m]\n",
    "\n",
    "\n",
    "P_ren = P_crop_d\n",
    "\n",
    "\n",
    "print(P_ren.shape, P_real.shape,index.shape)\n",
    "\n",
    "# random shuffel\n",
    "# pts_trafo = min(P_real.shape[0], 50000)\n",
    "# idx = torch.randperm( P_real.shape[0] )[0:pts_trafo]\n",
    "# P_real = P_real[idx]\n",
    "# P_ren = P_ren[idx]\n",
    "# real_pixels_fil = crop_d_pixels[idx]\n",
    "\n",
    "nr = 10000\n",
    "mp_render = get_H(mp)\n",
    "mp_render = (mp_render.type(torch.float64) @ h_render.type(torch.float64).T)[::5,:3]\n",
    "mp_real = get_H(mp)\n",
    "mp_real = (mp_real.type(torch.float64) @ h_real.type(torch.float64).T)[::5,:3]\n",
    "\n",
    "visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "    epoch = 1,\n",
    "    img= render_img[0].numpy()  ,\n",
    "    tl = ren_tl,\n",
    "    br = ren_br,\n",
    "    points = P_ren[:nr,:3],\n",
    "    store = False,\n",
    "    jupyter= True,\n",
    "    K = K_ren.cpu().numpy(),\n",
    "    H = np.eye(4),\n",
    "    method='def')\n",
    "\n",
    "plot_two_pcd(mp_render.numpy(), P_ren[:nr,:3].numpy())\n",
    "\n",
    "#inp = render_img[0].clone()\n",
    "inp = render_img_original[0].clone()\n",
    "inp = real_img_original[0].clone()\n",
    "for j in range(0,nr):\n",
    "    try:\n",
    "        \n",
    "        v = int(real_pixels[j,0])\n",
    "        u = int(real_pixels[j,1])\n",
    "        inp[u:u+4,v:v+4,: ] = torch.tensor( [255,0,255])\n",
    "    except:\n",
    "        pass\n",
    "display( Image.fromarray( np.uint8(inp.numpy())) )\n",
    "        \n",
    "P_ren_in_origin =  (get_H( P_ren ) @ torch.inverse( h_render ).T) [:,:3]\n",
    "    \n",
    "# PNP estimation\n",
    "objectPoints = P_ren_in_origin.clone().numpy()    \n",
    "imagePoints = real_pixels[:,:2].numpy()\n",
    "dist = np.array( [[0.0,0.0,0.0,0.0]] )\n",
    "\n",
    "if objectPoints.shape[0] < 8:\n",
    "    print('Failed due to missing corsspondences')\n",
    "# set current guess as the inital estimate\n",
    "\n",
    "rvec = R.from_matrix(h_real[:3,:3]).as_rotvec().astype(np.float32)\n",
    "tvec = h_real[:3,3].numpy().astype(np.float32)\n",
    "# calculate PnP between the pixels coordinates in the real image and the corrosponding points in the origin frame\n",
    "\n",
    "print(imagePoints.shape, objectPoints.shape)\n",
    "r_vec2, t_vec2 = cv2.solvePnPRefineLM(copy.deepcopy(objectPoints), \\\n",
    "    copy.deepcopy(imagePoints), \n",
    "    K_real.numpy(), \n",
    "    dist, \n",
    "    copy.deepcopy(rvec),\n",
    "    copy.deepcopy( tvec))\n",
    "r_vec2 = r_vec2[:,None]\n",
    "\n",
    "# retval, r_vec2, t_vec2, inliers = cv2.solvePnPRansac(copy.deepcopy(objectPoints), \\\n",
    "#     copy.deepcopy(imagePoints), \n",
    "#     K_real.numpy(), \n",
    "#     dist, \n",
    "#     copy.deepcopy(rvec),\n",
    "#     copy.deepcopy(tvec), \n",
    "#     useExtrinsicGuess = True, iterationsCount = 1000, reprojectionError= 0.3 ) \n",
    "\n",
    "h = rvec_tvec_to_H(r_vec2[:,0],t_vec2)\n",
    "print(\"H predicted \\n\",h, \"\\n H real \\n\", h_real)\n",
    "\n",
    "P_ren_h_trafo_out = (get_H(P_ren_in_origin) @ h.T)[:,:3]\n",
    "sub = int(P_ren_h_trafo_out.shape[0]/500 )\n",
    "\n",
    "plot_two_pcd_line(P_ren_h_trafo_out[::sub,:3].numpy(), mp_real.clone().numpy() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing all inputs\n",
    "# plotting\n",
    "img_np = np.uint8(img.clone().numpy())\n",
    "visualizer.plot_bounding_box(\"start\", 0, img_np, rmin=int(real_tl[0]), rmax=int(real_br[0]), cmin=int(real_tl[1]), cmax=int(real_br[1]), str_width=2, store=False, jupyter=True)\n",
    "\n",
    "\n",
    "sub = int(P_ren.shape[0]/200)\n",
    "mp_render = get_H(mp)\n",
    "mp_render = (mp_render.type(torch.float64) @ h_render.type(torch.float64).T)[::5,:3]\n",
    "print(\"Check alignment betweem rendered pointcloud and the cad model rotated to the pose of the PCD\")\n",
    "plot_two_pcd(P_ren[::sub,:3].numpy(), mp_render.numpy())\n",
    "\n",
    "# # The same filtering is performed for the PnP as for the Pointcloud matching approach! \n",
    "# # Transform the known rendered point corrspondences to the origin frame. \n",
    "\n",
    "P_ren_h = get_H(P_ren)\n",
    "P_ren_h_trafo = P_ren_h.clone() @ torch.inverse(h_render).T\n",
    "\n",
    "\n",
    "# print(\"Points render in center. CAD Model canter\")\n",
    "# plot_two_pcd(P_ren_h_trafo[::sub,:3].numpy(), mp[::5].numpy())\n",
    "\n",
    "\n",
    "mp_real = get_H(mp)\n",
    "mp_real = (mp_real.type(torch.float64) @ h_real.type(torch.float64).T)[::5,:3]\n",
    "# print(\"Real points\", \"CAD Model at gt position\")\n",
    "# plot_two_pcd(mp_real.numpy(), P_real[::sub].numpy())\n",
    "\n",
    "# print(\"Corrospondence\")\n",
    "visualizer.plot_corrospondence(tag=f'_',\n",
    "           epoch=0,\n",
    "            u_map=u_map, \n",
    "            v_map=v_map, \n",
    "            flow_mask=flow_mask, \n",
    "            real_img=real_img[0], \n",
    "            render_img=render_img[0],\n",
    "            store=False,\n",
    "            jupyter=True,\n",
    "            coloful=True,\n",
    "            method='def',\n",
    "            res_h=10,\n",
    "            res_w=10)\n",
    "\n",
    "img_depth_full = Drawer().disp_img_1d( depth_render_original[0][0], ret= True)\n",
    "print(img_depth_full.shape, 'img_depth_full')\n",
    "img_depth_full = img_depth_full[:,:,None].repeat(3,2)\n",
    "visualizer.plot_bounding_box(\"start\", 0, img_depth_full, rmin=int(ren_tl[0]), rmax=int(ren_br[0]), cmin=int(ren_tl[1]), cmax=int(ren_br[1]), str_width=2, store=False, jupyter=True)\n",
    "img_depth_full = Drawer().disp_img_1d( render_d, ret= True)\n",
    "\n",
    "\n",
    "print( 'Image comp',  torch.min( depth_render_original), torch.max( depth_render_original) )\n",
    "print( 'Image Render D',  torch.min( render_d), torch.max( render_d) )\n",
    "\n",
    "full_depth = depth_render_original[0][0]\n",
    "grid_full_d_h = torch.linspace(0 ,479 , 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "grid_full_d_w = torch.linspace(0 ,639 , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "# Project depth map to the pointcloud real\n",
    "cam_scale = 10000\n",
    "print( grid_full_d_w.shape, grid_full_d_h.shape, grid_full_d_w.flatten().shape)\n",
    "full_d_pixels = torch.stack( [grid_full_d_w.flatten(), grid_full_d_h.flatten(), torch.ones(grid_full_d_w.shape, device = u_map.device,  dtype= torch.float64).flatten()], dim=1 ).type(torch.float64)\n",
    "print(full_d_pixels.shape)\n",
    "K_inv = torch.inverse(K_ren.type(torch.float64))\n",
    "P_full_d = K_inv @ full_d_pixels.T.type(torch.float64)\n",
    "print(P_full_d.shape, full_depth.flatten().shape)\n",
    "P_full_d = P_full_d.type(torch.float64) * full_depth.flatten() / cam_scale\n",
    "P_full_d = P_full_d.T\n",
    "P_full_d  = P_full_d [ filter_pcd( P_full_d) ]\n",
    "sub = int(P_full_d.shape[0]/200)\n",
    "\n",
    "print('Alignment full depth map with model points at rendered pose')\n",
    "plot_two_pcd(mp_render.numpy(), P_full_d[::sub].numpy())\n",
    "\n",
    "\n",
    "grid_ren_h = torch.linspace(int(ren_tl[0]) ,int(ren_br[0]), 480, device=u_map.device)[:,None].repeat(1,640)\n",
    "grid_ren_w = torch.linspace(int(ren_tl[1]) ,int(ren_br[1]) , 640, device=u_map.device)[None,:].repeat(480,1)\n",
    "\n",
    "crop_d_pixels = torch.stack( [grid_ren_w.flatten(), grid_ren_h.flatten(), torch.ones(grid_ren_w.shape, device = u_map.device,  dtype= torch.float64).flatten()], dim=1 ).type(torch.float64)\n",
    "K_inv = torch.inverse(K_ren.type(torch.float64))\n",
    "\n",
    "P_crop_d = K_inv @ crop_d_pixels.T.type(torch.float64)\n",
    "P_crop_d = P_crop_d.type(torch.float64) * render_d.flatten() / cam_scale\n",
    "P_crop_d = P_crop_d.T\n",
    "\n",
    "P_crop_d  = P_crop_d [ filter_pcd( P_crop_d) ]\n",
    "sub = int(P_crop_d.shape[0]/200)\n",
    "print('Alignment cropped bounding box with Model points at rendered pose')\n",
    "plot_two_pcd(mp_render.numpy(), P_crop_d[::sub].numpy())\n",
    "\n",
    "\n",
    "\n",
    "P_ren = \n",
    "\n",
    "\n",
    "\n",
    "# visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#     epoch = 1,\n",
    "#     img= img_bp,\n",
    "#     points = P_ren_h_trafo_out[:,:3],\n",
    "#     store = False,\n",
    "#     jupyter= True,\n",
    "#     K = K_real.cpu().numpy(),\n",
    "#     H = np.eye(4),\n",
    "#     method='left')\n",
    "\n",
    "\n",
    "\n",
    "# for p in range(imagePoints.shape[0]):\n",
    "#     u = min(479,int(imagePoints[p,1]) )\n",
    "#     v = min(639,int(imagePoints[p,0]) )\n",
    "#     img_np[u,v, :] = np.array([138,43,226])\n",
    "\n",
    "# Drawer().disp_img_1d(img, hold=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PNP estimation\n",
    "objectPoints = P_ren_h_trafo[:,:3].clone().numpy() #P_ren.numpy()    \n",
    "imagePoints = real_pixels_fil[:,:2].numpy()\n",
    "dist = np.array( [[0.0,0.0,0.0,0.0]] )\n",
    "div = 1 # int(objectPoints.shape[0]/1000)\n",
    "\n",
    "print(objectPoints.shape)\n",
    "print(imagePoints.shape)\n",
    "if objectPoints.shape[0] < 8:\n",
    "    print('Failed due to missing corsspondences')\n",
    "# set current guess as the inital estimate\n",
    "\n",
    "rvec = R.from_matrix(h_real[:3,:3]).as_rotvec().astype(np.float32)\n",
    "tvec = h_real[:3,3].numpy().astype(np.float32)\n",
    "# calculate PnP between the pixels coordinates in the real image and the corrosponding points in the origin frame\n",
    "r_vec2, t_vec2 = cv2.solvePnPRefineLM(copy.deepcopy(objectPoints), \\\n",
    "    copy.deepcopy(imagePoints), \n",
    "    K_real.numpy(), \n",
    "    dist, \n",
    "    copy.deepcopy(rvec),\n",
    "    copy.deepcopy( tvec))\n",
    "r_vec2 = r_vec2[:,None]\n",
    "\n",
    "retval, r_vec2, t_vec2, inliers = cv2.solvePnPRansac(copy.deepcopy(objectPoints), \\\n",
    "    copy.deepcopy(imagePoints), \n",
    "    K_real.numpy(), \n",
    "    dist, \n",
    "    copy.deepcopy(rvec),\n",
    "    copy.deepcopy(tvec), \n",
    "    useExtrinsicGuess = True, iterationsCount = 1000, reprojectionError= 0.3 ) \n",
    "\n",
    "\n",
    "         \n",
    "print(retval, inliers.shape)\n",
    "\n",
    "h = rvec_tvec_to_H(r_vec2[:,0],t_vec2)\n",
    "print(\"H predicted \\n\",h, \"\\n H real \\n\", h_real)\n",
    "P_ren_h_trafo_out = P_ren_h_trafo @ h.T\n",
    "sub = int(P_ren_h_trafo_out.shape[0]/500 )\n",
    "plot_two_pcd_line(P_ren_h_trafo_out[::sub,:3].numpy(), mp_real.clone().numpy() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#         print(\"Real depth map cropped\")\n",
    "flow_mask.shape\n",
    "#         real_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "lab = Drawer().disp_img_1d(flow_mask.numpy(),ret=True)\n",
    "#         real_depth_img = np.repeat( real_depth_img[:,:,None],3, axis=2)\n",
    "#         print(\"Render depth map cropped\")\n",
    "#         render_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "#         render_depth_img = np.repeat( render_depth_img[:,:,None],3, axis=2)\n",
    "#         Drawer().disp_img_1d(flow_mask[0],ret=True)\n",
    "Drawer().disp_img_1d(render_d.numpy())\n",
    "\n",
    "\n",
    "# plotting\n",
    "\n",
    "img_np = np.uint8(img.clone().numpy())\n",
    "for p in range(imagePoints.shape[0]):\n",
    "\n",
    "    u = min(479,int(imagePoints[p,1]) )\n",
    "    v = min(639,int(imagePoints[p,0]) )\n",
    "    img_np[u,v, :] = np.array([138,43,226])\n",
    "\n",
    "Drawer().disp_img_1d(img, hold=False )\n",
    "visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "    epoch = 1,\n",
    "    img= img_np,\n",
    "    points = P_ren_h_trafo_out[:2,:3],\n",
    "    store = False,\n",
    "    jupyter= True,\n",
    "    K = K_real.cpu().numpy(),\n",
    "    H = np.eye(4),\n",
    "    method='right')\n",
    "\n",
    "o = visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "    epoch = 1,\n",
    "    img= img_np,\n",
    "    points = P_ren_h_trafo_out[:,:3],\n",
    "    store = False,\n",
    "    jupyter= True,\n",
    "    K = K_real.cpu().numpy(),\n",
    "    H = np.eye(4),\n",
    "    method='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from visu import plot_two_pcd\n",
    "\n",
    "import pandas as pd\n",
    "# idxmax can be set to != -1 to load specific sample\n",
    "\n",
    "idxmax = -1 \n",
    "device = 'cpu'\n",
    "desig_ls= []\n",
    "adds_gt_ls = []\n",
    "adds_init_ls = []\n",
    "adds_erode_ls = []\n",
    "adds_n1_ls = []\n",
    "adds_n2_ls = []\n",
    "\n",
    "adds_pnp_gt_ls = []\n",
    "adds_pnp_ero_ls = []\n",
    "\n",
    "failed_desig_ls = []\n",
    "visualizer = Visualizer('/home/jonfrey/Debug', None)\n",
    "print('START')\n",
    "\n",
    "K_ren = torch.tensor( dataset_train._backend.get_camera('data_syn/0019', K=True), device=device ) \n",
    "K_ren = torch.tensor( dataset_train._backend.get_camera('data/0049', K=True), device=device ) \n",
    "\n",
    "max_iter = 10\n",
    "\n",
    "if idxmax != -1:\n",
    "    print(idxmax, \"is not equal to -1 load batch\")\n",
    "    batch = next(itertools.islice(dataloader_train, idxmax, None))[0]\n",
    "cou = 0\n",
    "print(\"START\")\n",
    "names = ['ID','ADD-S GT','ADD-S INITAL','ADD-S Eroded Mask', 'ADD-S Random', 'ADD-S Bias', 'ADD-S PNP GT', 'ADD-S PNP Ero']\n",
    "df = pd.DataFrame(columns=names)\n",
    "        \n",
    "for j,ba in enumerate(dataloader_test):\n",
    "    \n",
    "    if j > max_iter-1:\n",
    "        break\n",
    "    if j % 10 == 0:\n",
    "        print(f\"Processed {j}/{max_iter}\")\n",
    "\n",
    "    if idxmax == -1:  \n",
    "        batch = ba[0]\n",
    "    # use first sample in batch \n",
    "    b = 0\n",
    "    \n",
    "    model_points = batch[4]\n",
    "    idx = batch[5]  # Be carefull here the first objects starts with 0. Normally 0 is the NO object class in all other datastructures\n",
    "    label = batch[7]\n",
    "    real_img_original = batch[8]\n",
    "    cam = batch[9]\n",
    "    gt_rot_wxyz, gt_trans, unique_desig = batch[10:13] # unique_desig[1] contains the idx starting at 1 for the first object \n",
    "    bs = model_points.shape[0]\n",
    "    if batch[13] is False:\n",
    "        print('Continue')\n",
    "        continue\n",
    "    real_img, render_img, real_d, render_d, gt_label_cropped = batch[13:18]\n",
    "    pred_rot_wxyz, pred_trans, pred_points, h_render, h_real, render_img_original = batch[18:24]\n",
    "    u_map, v_map, flow_mask, bb, depth_render_original= batch[24:]\n",
    "    real_tl, real_br, ren_tl, ren_br = bb \n",
    "    \n",
    "    data = torch.cat([real_img, render_img], dim=1)\n",
    "    uv_gt = torch.stack( [u_map, v_map], dim=3 ).permute(0,3,1,2)\n",
    "    \n",
    "    \n",
    "    ero_in = (gt_label_cropped ==  unique_desig[1])[:,None,:,:].type(torch.float32) # BS,C,H,W\n",
    "    t_size = get_scale_for_erosion(ero_in)\n",
    "    ero_out = eroision_batch(ero_in,t_size)\n",
    "    \n",
    "    # get camera\n",
    "    K_real = torch.tensor( [[cam[b,2],0,cam[b,0]],[b,cam[b,3],cam[b,1]],[0,0,1]], device=device )\n",
    "    \n",
    "    #get inital estimate of the poistion given by the dataloader\n",
    "    h_real_est = torch.eye(4,device=device)\n",
    "    h_real_est[:3,:3] = quat_to_rot(pred_rot_wxyz[b][None,:], conv='wxyz', device=device)\n",
    "    h_real_est[:3,3] = torch.tensor( pred_trans[b].clone().detach() ,device=device )\n",
    "    \n",
    "    #print(h_render)\n",
    "    #break\n",
    "    \n",
    "    ### GT Estimate ###\n",
    "    typ = u_map.dtype\n",
    "    fmt = flow_mask.dtype\n",
    "    flow_mask_eroded  = (flow_mask * ero_out.type(torch.float32)).type(fmt)[:,0]\n",
    "    suc = True\n",
    "    suc_, pnp_h_gt = flow_to_trafo_pnp(\n",
    "        real_br = real_br[b].clone(),\n",
    "        real_tl = (real_tl[b]).clone(), \n",
    "        ren_br = (ren_br[b]).clone(), \n",
    "        ren_tl = (ren_tl[b]).clone(),\n",
    "        flow_mask = (flow_mask[b]).clone(), \n",
    "        u_map = (u_map[b].type( typ )).clone(),\n",
    "        v_map = (v_map[b].type( typ )).clone(), \n",
    "        K_real = (K_real.type( typ )).clone(),\n",
    "        K_ren = (K_ren.type( typ )).clone(),\n",
    "        real_d = (real_d[b].type( typ )).clone(),\n",
    "        render_d = (render_d[b].type( typ )).clone(),\n",
    "        h_real = (h_real[b].type( typ )).clone(), \n",
    "        h_render = (h_render[b].type( typ )).clone(),\n",
    "        h_real_est = h_real_est,\n",
    "        mp = model_points[0].clone(),\n",
    "        img = real_img_original[0].clone())\n",
    "    ran_suc = suc_\n",
    "    suc = suc_ and suc\n",
    "    #TODO number for pnp-h-ero is faked\n",
    "    pnp_h_ero = pnp_h_gt\n",
    "#     suc_, pnp_h_ero = flow_to_trafo_pnp(\n",
    "#         real_br = real_br[b].clone(),\n",
    "#         real_tl = (real_tl[b]).clone(), \n",
    "#         ren_br = (ren_br[b]).clone(), \n",
    "#         ren_tl = (ren_tl[b]).clone(),\n",
    "#         flow_mask = (flow_mask_eroded[b]).clone(), \n",
    "#         u_map = (u_map[b].type( typ )).clone(),\n",
    "#         v_map = (v_map[b].type( typ )).clone(), \n",
    "#         K_real = (K_real.type( typ )).clone(),\n",
    "#         K_ren = (K_ren.type( typ )).clone(),\n",
    "#         real_d = (real_d[b].type( typ )).clone(),\n",
    "#         render_d = (render_d[b].type( typ )).clone(),\n",
    "#         h_real = (h_real[b].type( typ )).clone(), \n",
    "#         h_render = (h_render[b].type( typ )).clone(),\n",
    "#         h_real_est = h_real_est,\n",
    "#         mp = model_points[0].clone() )\n",
    "#     ran_suc = ran_suc and suc_\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    \n",
    "     ### GT Label ###\n",
    "    suc_, P_real_in_center, P_ren_in_center, P_real_trafo, gt_T_res = flow_to_trafo(\n",
    "        real_br = real_br[b].clone(),\n",
    "        real_tl = (real_tl[b]).clone(), \n",
    "        ren_br = (ren_br[b]).clone(), \n",
    "        ren_tl = (ren_tl[b]).clone(),\n",
    "        flow_mask = (flow_mask[b]).clone(), \n",
    "        u_map = (u_map[b].type( typ )).clone(),\n",
    "        v_map = (v_map[b].type( typ )).clone(), \n",
    "        K_real = (K_real.type( typ )).clone(),\n",
    "        K_ren = (K_ren.type( typ )).clone(),\n",
    "        real_d = (real_d[b].type( typ )).clone(),\n",
    "        render_d = (render_d[b].type( typ )).clone(),\n",
    "        h_real = (h_real_est.type( typ )).clone(), \n",
    "        h_render = (h_render[b].type( typ )).clone())\n",
    "    gt_h_est =  gt_T_res @ h_render[0].type(typ)\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    ### Erode Label ###\n",
    "    \n",
    "    suc_,_,_,_, ero_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask_eroded[b]), \n",
    "        u_map = copy.deepcopy((u_map[b]).type( typ )), \n",
    "        v_map = copy.deepcopy((v_map[b]).type( typ )), \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    ero_h_est =  ero_T_res @ h_render[0].type(typ) # set rotation\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    \n",
    "    ### N1 ###\n",
    "    lvl = 20\n",
    "    n1 = torch.rand(v_map[b].shape, dtype = typ)\n",
    "    n2 = torch.rand(v_map[b].shape, dtype = typ)\n",
    "    u_map_clone = (u_map[b]).type( typ ).clone() + (n1 - 0.5) * 2 * lvl\n",
    "    v_map_clone = (v_map[b]).type( typ ).clone() + (n2 - 0.5) * 2 * lvl\n",
    "    \n",
    "    suc_,_,_,_, n1_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask_eroded[b]), \n",
    "        u_map = u_map_clone, \n",
    "        v_map = v_map_clone, \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    n1_h_est =  n1_T_res @ h_render[0].type(typ) # set rotation\n",
    "    suc = suc_ and suc\n",
    "    ### N2 ###\n",
    "    lvl = 20\n",
    "    n1 = float(torch.rand((1), dtype = typ ))\n",
    "    n2 = float(torch.rand((1), dtype = typ ))\n",
    "    u_map_clone = (u_map[b]).type( typ ).clone() + float( (n1 - 0.5) * 2 * lvl )\n",
    "    v_map_clone = (v_map[b]).type( typ ).clone() + float( (n2 - 0.5) * 2 * lvl )\n",
    "    \n",
    "    \n",
    "    suc_,_,_,_, n2_T_res = flow_to_trafo(\n",
    "        real_br = copy.deepcopy(real_br[b]),\n",
    "        real_tl = copy.deepcopy(real_tl[b]), \n",
    "        ren_br = copy.deepcopy(ren_br[b]), \n",
    "        ren_tl = copy.deepcopy(ren_tl[b]),\n",
    "        flow_mask = copy.deepcopy(flow_mask_eroded[b]), \n",
    "        u_map = u_map_clone, \n",
    "        v_map = v_map_clone, \n",
    "        K_real = copy.deepcopy(K_real.type( typ )),\n",
    "        K_ren = copy.deepcopy(K_ren.type( typ )),\n",
    "        real_d = copy.deepcopy(real_d[b].type( typ )),\n",
    "        render_d = copy.deepcopy(render_d[b].type( typ )),\n",
    "        h_real = copy.deepcopy(h_real_est.type( typ )), \n",
    "        h_render = copy.deepcopy(h_render[b].type( typ )))\n",
    "    n2_h_est =  n2_T_res @ h_render[0].type(typ) # set rotation\n",
    "    suc = suc_ and suc\n",
    "    \n",
    "    if not ran_suc:\n",
    "        failed_desig_ls.append( int( unique_desig[1]) )\n",
    "        print(f\"Ransac failed for {int( unique_desig[1])} {unique_desig[0]}\")\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    mask = (flow_mask == True)\n",
    "    # Target Model-points Transformed\n",
    "    p = model_points.shape[1]\n",
    "    target = torch.bmm( model_points, torch.transpose(h_real[:,:3,:3], 1,2 ) ) + h_real[:,:3,3][:,None,:].repeat(1,p,1)\n",
    "\n",
    "    # Compute ADD-S\n",
    "    adds_res_gt_flow = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = gt_h_est[None].type( target.dtype) )\n",
    "    adds_res_gt_flow_eroded = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = ero_h_est[None].type( target.dtype) )\n",
    "    adds_res_n1 = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = n1_h_est[None].type( target.dtype) )\n",
    "    adds_res_n2 = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = n2_h_est[None].type( target.dtype) )\n",
    "    adds_res_pnp_gt = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = pnp_h_gt[None].type( target.dtype) )\n",
    "    adds_res_pnp_ero = criterion_adds(target[b][None], model_points[b][None], idx[b][None], H = pnp_h_ero[None].type( target.dtype) )\n",
    "    adds_init = criterion_adds(target[b][None].clone(), model_points[b][None].clone(), idx[b][None], H = h_real_est[None].type( target.dtype))\n",
    "    \n",
    "    \n",
    "    v1,v2,v3 = float(adds_res_gt_flow.detach()),float(adds_init.detach()), float(adds_res_gt_flow_eroded.detach())\n",
    "    v4,v5,v6,v7 = float(adds_res_n1.detach()),float(adds_res_n2.detach()),float(adds_res_pnp_gt.detach()), float(adds_res_pnp_ero.detach())\n",
    "    \n",
    "    test_values = [int( unique_desig[1]),v1,v2,v3,v4,v5,v6,v7]\n",
    "    res = {names[i]: test_values[i] for i in range(len(names))} \n",
    "    df = df.append(res, ignore_index=True)\n",
    "        \n",
    "    \n",
    "\n",
    "    # Append results to list\n",
    "    desig_ls.append(int( unique_desig[1]) )\n",
    "    adds_init_ls.append(float(adds_init.detach()))\n",
    "    adds_gt_ls.append(float(adds_res_gt_flow.detach()))\n",
    "    adds_erode_ls.append(float(adds_res_gt_flow_eroded.detach()))    \n",
    "    adds_n1_ls.append(float(adds_res_n1.detach()))\n",
    "    adds_n2_ls.append(float(adds_res_n2.detach()))\n",
    "    adds_pnp_gt_ls.append(float(adds_res_pnp_gt.detach()))\n",
    "    adds_pnp_ero_ls.append(float(adds_res_pnp_ero.detach()))\n",
    "\n",
    "#     if adds_gt_ls[-1] > 0.02 :\n",
    "#         print(f'Failed Iteration {j} > 0.02 ADD-S, GT', adds_gt_ls[-1] , 'ERRODED', adds_erode_ls[-1]  )\n",
    "        \n",
    "#         no = torch.norm(P_real_trafo- P_ren_in_center, dim=1)\n",
    "#         print( f'    shape of use points: ',P_real_trafo.shape )\n",
    "#         print( f'    number of point with a distance greater 0.2 {torch.sum( no>0.2 )} ' )\n",
    "#         print( f'    number of point with a distance greater 0.02 {torch.sum( no>0.02 )} ' )\n",
    "#         print( f'    number of point with a distance smaller 0.02 {torch.sum( no<0.02 )} ' )\n",
    "        \n",
    "        \n",
    "#         plot1 = True\n",
    "#     else:\n",
    "#         print(f'Suc Iteration {j} < 0.02 ADD-S, GT', adds_gt_ls[-1] , 'ERRODED', adds_erode_ls[-1]  )\n",
    "#         plot1 = False\n",
    "#     if idxmax != -1:\n",
    "#         plot1 = True\n",
    "        \n",
    "    \n",
    "    print(f'Not Iteration {j} < 0.02 ADD-S, GT', adds_gt_ls[-1] , 'ERRODED', adds_erode_ls[-1],'PnP', adds_pnp_gt_ls[-1]   )\n",
    "        \n",
    "    if adds_pnp_gt_ls[-1] > 0.000005 :\n",
    "        cou += 1\n",
    "#         if cou > 5:\n",
    "#             break\n",
    "#         continue\n",
    "#         print(\"Real depth map cropped\")\n",
    "        real_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "#        real_depth_img = Drawer().disp_img_1d(depth_render_original[0][0].numpy(),ret=True)\n",
    "#         real_depth_img = np.repeat( real_depth_img[:,:,None],3, axis=2)\n",
    "#         print(\"Render depth map cropped\")\n",
    "#         render_depth_img = Drawer().disp_img_1d(render_d[b].numpy(),ret=True)\n",
    "#         render_depth_img = np.repeat( render_depth_img[:,:,None],3, axis=2)\n",
    "#         Drawer().disp_img_1d(flow_mask[0],ret=True)\n",
    "\n",
    "#         sub = max(1,int( P_real_in_center.shape[0]/100 ) )\n",
    "#         plot_two_pcd_line(P_real_in_center[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "#         plot_two_pcd_line(P_real_trafo[::sub].numpy(), P_ren_in_center[::sub].numpy() )\n",
    "\n",
    "#         print(f\"Real Image, Estimated Points given GT Flow {P_real_in_center.shape}\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_img_original[b].cpu().numpy(),\n",
    "#                         points =copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = h_real[0].numpy(),\n",
    "#                         method='def')\n",
    "#         print(\"Real Image Cropped, Estimated Points given GT Flow\")\n",
    "#         visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_depth_img,\n",
    "#                         tl = real_tl[0],\n",
    "#                         br = real_br[0],\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "\n",
    "#         fil = label == unique_desig[1]    \n",
    "#         real_img_original[b][ fil[0][:,:,None].repeat(1,1,3) ] = 255\n",
    "#         print(\"Real Image, Estimated Points given GT Flow with label is white\")\n",
    "#         visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "#                         epoch = 1,\n",
    "#                         img= real_img_original[b].cpu().numpy(),\n",
    "#                         points = copy.deepcopy( P_real_in_center.cpu().numpy()),\n",
    "#                         store = False,\n",
    "#                         jupyter=True,\n",
    "#                         K = K_real.cpu().numpy(),\n",
    "#                         H = np.eye(4),\n",
    "#                         method='def')\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"GT: Render Image Cropped\")\n",
    "        visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= render_img[0].numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            tl = ren_tl[0],\n",
    "                            br = ren_br[0],\n",
    "                            store = False,\n",
    "                            jupyter=True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = h_render[0].numpy(),\n",
    "                            method='def')\n",
    "        print(\"PNP, Real Image\")\n",
    "        visualizer.plot_estimated_pose( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= real_img_original[b].cpu().numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            store = False,\n",
    "                            jupyter= True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = pnp_h_gt.clone().numpy(),\n",
    "                            method='def')\n",
    "        \n",
    "        print(\"PNP: Render Image Cropped\")\n",
    "        visualizer.plot_estimated_pose_on_bb( tag = f\"_\",\n",
    "                            epoch = 1,\n",
    "                            img= render_img[0].numpy(),\n",
    "                            points = copy.deepcopy(model_points[0].cpu().numpy()),\n",
    "                            tl = ren_tl[0],\n",
    "                            br = ren_br[0],\n",
    "                            store = False,\n",
    "                            jupyter=True,\n",
    "                            K = K_ren.cpu().numpy(),\n",
    "                            H = h_render[0].clone().numpy(),\n",
    "                            method='def')\n",
    "\n",
    "#         print('Valid Flow Mask')\n",
    "#         visualizer.plot_segmentation('tag', 1, flow_mask_eroded[b] , store=False, method='def', jupyter=True)\n",
    "#         visualizer.plot_segmentation('tag', 1, flow_mask[b]  , store=False, method='def', jupyter=True)\n",
    "\n",
    "        print(\"Corrospondence\")\n",
    "        visualizer.plot_corrospondence(tag=f'_',\n",
    "                                           epoch=0,\n",
    "                                            u_map=u_map[0], \n",
    "                                            v_map=v_map[0], \n",
    "                                            flow_mask=flow_mask[0], \n",
    "                                            real_img=real_img[0], \n",
    "                                            render_img=render_img[0],\n",
    "                                            store=False,\n",
    "                                            jupyter=True,\n",
    "                                            coloful=True,\n",
    "                                            method='def',\n",
    "                                            res_h=10,\n",
    "                                            res_w=10)\n",
    "        \n",
    "         \n",
    "    if float(adds_res_pnp_gt.detach()) > 0.02 :\n",
    "        break\n",
    "# print(f'Result, GT Avg', sum(adds_gt_ls)/len(adds_gt_ls), 'Errode Avg', sum(adds_erode_ls)/len(adds_erode_ls) )\n",
    "print(\"Finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_real,K_ren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################### CROPED #########################################\n",
    "ren_tl, ren_br\n",
    "grid_real_h = torch.linspace(int(ren_tl[0,0]) ,int(ren_br[0,0]), 480)[:,None].repeat(1,640)\n",
    "grid_real_w = torch.linspace(int(ren_tl[0,1]) ,int(ren_br[0,1]), 640)[None,:].repeat(480,1)\n",
    "\n",
    "# Project depth map to the pointcloud real\n",
    "cam_scale = 10000\n",
    "\n",
    "ren_or_pixels = torch.stack( [grid_real_w, grid_real_h, torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)], dim=2 ).type(u_map.dtype)\n",
    "K_inv = torch.inverse(K_ren.type(torch.float32)).type(u_map.dtype)\n",
    "P = K_inv @ ren_or_pixels.view(( -1,3)).T\n",
    "\n",
    "P = torch.mul(P, render_d[0].view(( -1,1)).repeat((1,3)).T ) / cam_scale\n",
    "P = P.T\n",
    "\n",
    "# Transform the known rendered point corrspondences to the origin frame. \n",
    "\n",
    "P_ren_h = get_H(P)\n",
    "P_ren_h = P_ren_h [ P_ren_h[:,2] > 0.05 ]\n",
    "\n",
    "P_ren_h_trafo = P_ren_h @ torch.inverse(h_render[0]).T\n",
    "m1 = P_ren_h_trafo[:,:3] < 0.1\n",
    "m1 = torch.sum(m1.type(torch.float32), dim=1 ) > 2\n",
    "P_ren_h_trafo2 = P_ren_h_trafo[m1]\n",
    "\n",
    "sub =int( P_ren_h_trafo2.shape[0]/1000 )\n",
    "plot_two_pcd_line(P_ren_h_trafo2[::sub,:3].numpy(), model_points[0,:,:].clone().numpy() )\n",
    "                    \n",
    "############## ORIG##########################\n",
    "                           \n",
    "grid_real_h = torch.linspace(0, 479, 480)[:,None].repeat(1,640)\n",
    "grid_real_w = torch.linspace(0, 639, 640)[None,:].repeat(480,1)\n",
    "\n",
    "# Project depth map to the pointcloud real\n",
    "cam_scale = 10000\n",
    "\n",
    "ren_or_pixels = torch.stack( [grid_real_w, grid_real_h, torch.ones(grid_real_h.shape, device = u_map.device,  dtype= u_map.dtype)], dim=2 ).type(u_map.dtype)\n",
    "K_inv = torch.inverse(K_ren.type(torch.float32)).type(u_map.dtype)\n",
    "P = K_inv @ ren_or_pixels.view(( -1,3)).T\n",
    "\n",
    "P = torch.mul(P, depth_render_original[0][0].view(( -1,1)).repeat((1,3)).T ) / cam_scale\n",
    "P = P.T\n",
    "\n",
    "# Transform the known rendered point corrspondences to the origin frame. \n",
    "\n",
    "P_ren_h = get_H(P)\n",
    "P_ren_h = P_ren_h [ P_ren_h[:,2] > 0.05 ]\n",
    "\n",
    "P_ren_h_trafo = P_ren_h @ torch.inverse(h_render[0]).T\n",
    "m1 = P_ren_h_trafo[:,:3] < 0.1\n",
    "m1 = torch.sum(m1.type(torch.float32), dim=1 ) > 2\n",
    "P_ren_h_trafo2 = P_ren_h_trafo[m1]\n",
    "\n",
    "sub =int( P_ren_h_trafo2.shape[0]/1000 )\n",
    "plot_two_pcd_line(P_ren_h_trafo2[::sub,:3].numpy(), model_points[0,:,:].clone().numpy() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_real[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "#data = np.array( [desig_ls,adds_gt_ls,adds_init_ls,adds_erode_ls, adds_n1_ls, adds_n2_ls, adds_pnp_gt_ls, adds_pnp_ero_ls] ).T\n",
    "df_failed = pandas.DataFrame( np.array( [failed_desig_ls] ), columns = ['ID_Failed']) \n",
    "\n",
    "#df = pandas.DataFrame( data, columns = ['ID','ADD-S GT','ADD-S INITAL','ADD-S Eroded Mask', 'ADD-S Random', 'ADD-S Bias', 'ADD-S PNP GT', 'ADD-S PNP Ero'] )\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random: Takeing Ground Truth Flow and adding noise uniform in between +20 -20 pixels for each pixel')\n",
    "print('Bias: Takeing Ground Truth Flow and offsetting the flow by a random value in between +20 -20 pixels for the full image')\n",
    "print( df['ID'].value_counts() )\n",
    "# print( df_failed['ID_Failed'].value_counts())\n",
    "df.groupby(['ID']).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MEAN:\\n\", df.mean() )\n",
    "print(\"\\n \\nSTD:\\n\", df.std() )\n",
    "print(\"\\n \\nMAX:\\n\", df.max())\n",
    "print(\"\\n \\nIDXMAX:\\n\", df.idxmax())\n",
    "\n",
    "idxmax = df.idxmax()[0]\n",
    "print(f'\\n \\nWorst Sample Index is {idxmax}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = adds_erode_ls\n",
    "n_bins = 100\n",
    "failures_filter = 0.1\n",
    "fig, axs = plt.subplots(3, 1, sharey=True, tight_layout=True)\n",
    "axs[0].hist(adds_init_ls, bins=n_bins)\n",
    "\n",
    "adds_gt_f = [a for a in adds_gt_ls if a < failures_filter]\n",
    "adds_erode_f = [a for a in adds_erode_ls if a < failures_filter]\n",
    "\n",
    "axs[1].hist(adds_gt_f, bins=n_bins)\n",
    "axs[2].hist(adds_erode_f, bins=n_bins)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install cvxpnpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from cvxpnpl import pnp\n",
    "\n",
    "# fix seed to allow for reproducible results\n",
    "np.random.seed(0)\n",
    "np.random.seed(42)\n",
    "\n",
    "# instantiate a couple of points centered around the origin\n",
    "pts = 0.6 * (np.random.random((6, 3)) - 0.5)\n",
    "\n",
    "# Made up projective matrix\n",
    "K = np.array([[160, 0, 320], [0, 120, 240], [0, 0, 1]])\n",
    "\n",
    "# A pose\n",
    "R_gt = np.array(\n",
    "    [\n",
    "        [-0.48048015, 0.1391384, -0.86589799],\n",
    "        [-0.0333282, -0.98951829, -0.14050899],\n",
    "        [-0.8763721, -0.03865296, 0.48008113],\n",
    "    ]\n",
    ")\n",
    "t_gt = np.array([-0.10266772, 0.25450789, 1.70391109])\n",
    "\n",
    "# Project points to 2D\n",
    "pts_2d = (pts @ R_gt.T + t_gt) @ K.T\n",
    "pts_2d = (pts_2d / pts_2d[:, -1, None])[:, :-1]\n",
    "\n",
    "# Compute pose candidates. the problem is not minimal so only one\n",
    "# will be provided\n",
    "poses = pnp(pts_2d=pts_2d, pts_3d=pts, K=K)\n",
    "R, t = poses[0]\n",
    "\n",
    "print(\"Nr of possible poses:\", len(poses))\n",
    "print(\"R (ground truth):\", R_gt, \"R (estimate):\", R, sep=\"\\n\")\n",
    "print(\"t (ground truth):\", t_gt)\n",
    "print(\"t (estimate):\", t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_latest",
   "language": "python",
   "name": "track_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
